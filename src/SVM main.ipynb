{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca30963-1e0e-4d4b-b0d4-324f04586794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from scipy.stats import chi2\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "\n",
    "from utils_baseline import *\n",
    "from utils_experiments import *\n",
    "# from utils_results import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cfdff4-9767-4f21-a901-86873bf0f030",
   "metadata": {},
   "source": [
    "# MAMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e5d3f7-0638-40b5-b550-83b04b991e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for MAMI dataset paths\n",
    "# Note: Dataset files are not included in this repository due to privacy restrictions.\n",
    "# To obtain the dataset, please contact the original dataset creators.\n",
    "\n",
    "# Default paths (update these according to your local setup)\n",
    "mami_training_data = \"\" # Path to training data\n",
    "mami_dev_data =  \"\" # Path to development data  \n",
    "mami_test_data = \"\"  # Path to test data\n",
    "\n",
    "\n",
    "mami_training_df = pd.read_json(mami_training_data,orient='index')\n",
    "mami_dev_df = pd.read_json(mami_dev_data,orient='index')\n",
    "mami_test_df = pd.read_json(mami_test_data,orient='index')\n",
    "\n",
    "#MAMI: combine training and dev data as training data\n",
    "mami_training_df = pd.concat([mami_training_df, mami_dev_df]).sort_index()\n",
    "\n",
    "# Check available columns\n",
    "print(\"Training data columns:\", mami_training_df.columns)\n",
    "print(\"Test data columns:\", mami_test_df.columns)\n",
    "\n",
    "#gold labels \n",
    "gold_test_txt = \"\" # Path to MAMI text gold label (txt)\n",
    "gold_test_bin = \"\" # Path to MAMI binary classification gold labels (json) \n",
    "gold_test_ml =  \"\" # Path to MAMI multi-label classification gold labels (json)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccc32e7-c14c-4d72-ae75-0419229acd5b",
   "metadata": {},
   "source": [
    "## Binary Classification: Misogynous vs. non-misogynous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4a0eb4-1f9f-4cc2-894b-d77b4bbbbc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\"Custom JSON encoder for numpy arrays and other types\"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        return super().default(obj)\n",
    "        \n",
    "def run_baseline_with_save(dataset_name, training_df, test_df, binary_label, label_names):\n",
    "    \"\"\"\n",
    "    Run baseline model and save results in the same format as ablation experiments.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset_name : str\n",
    "        Dataset name ('MAMI' or 'EXIST2024')\n",
    "    training_df : DataFrame\n",
    "        Training data\n",
    "    test_df : DataFrame\n",
    "        Test data\n",
    "    binary_label : str\n",
    "        Binary label column name\n",
    "    label_names : list\n",
    "        List of label names\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Baseline results dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ RUNNING BASELINE FOR {dataset_name}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Data preparation\n",
    "    X_train = training_df['svm representation'].tolist()\n",
    "    y_train = training_df[binary_label].tolist()\n",
    "    X_test = test_df['svm representation'].tolist()\n",
    "    y_test = test_df[binary_label].tolist()\n",
    "    \n",
    "    # Set model name\n",
    "    model_name = \"svm_baseline_bow\"\n",
    "    \n",
    "    # Create classifier\n",
    "    svm_model, vec = build_bin_classifier(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on test data\n",
    "    y_pred_svm = classify_data(X_test, svm_model, vec)\n",
    "    \n",
    "    # Set evaluation parameters\n",
    "    evaluation_type = 'binary'\n",
    "    \n",
    "    # Set gold standard file paths\n",
    "    if dataset_name == \"MAMI\":\n",
    "        gold_test_bin = \"\" # Path to MAMI binary classification gold labels (json)\n",
    "        gold_test_txt = \"\" # Path to MAMI text gold labels (txt)\n",
    "    else:  # EXIST2024\n",
    "        gold_test_bin = \"\" # Path to EXIST2024 binary classification gold labels (json)\n",
    "        gold_test_txt = \"\" # Path to EXIST2024 text gold labels (txt)\n",
    "    \n",
    "    # Save prediction results\n",
    "    test_pred_json, test_pred_txt = save_evaluation(\n",
    "        test_df, \"evaluation/predictions\", dataset_name, \"test\", \n",
    "        evaluation_type, model_name, y_pred_svm, binary_label, []\n",
    "    )\n",
    "    \n",
    "    # Calculate all metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred_svm)\n",
    "    precision_macro = precision_score(y_test, y_pred_svm, average='macro')\n",
    "    recall_macro = recall_score(y_test, y_pred_svm, average='macro')\n",
    "    f1_macro = f1_score(y_test, y_pred_svm, average='macro')\n",
    "    \n",
    "    # Get detailed classification report\n",
    "    class_report_dict = classification_report(\n",
    "        y_test, y_pred_svm, \n",
    "        target_names=label_names,\n",
    "        zero_division=0,\n",
    "        digits=3,\n",
    "        output_dict=True\n",
    "    )\n",
    "    \n",
    "    # Calculate binary F1 score \n",
    "    binary_f1 = evaluate_f1_scores(gold_test_txt, test_pred_txt, 2)\n",
    "    \n",
    "    # Save baseline results to JSON file WITH PREDICTIONS\n",
    "    baseline_results = {\n",
    "        'binary_f1': binary_f1,\n",
    "        'macro_f1': f1_macro,\n",
    "        'accuracy': accuracy,\n",
    "        'precision_macro': precision_macro,\n",
    "        'recall_macro': recall_macro,\n",
    "        'per_label_metrics': class_report_dict,\n",
    "        'predictions': y_pred_svm.tolist(), \n",
    "        'true_labels': y_test,  \n",
    "        'prediction_files': {\n",
    "            'json': test_pred_json,\n",
    "            'txt': test_pred_txt\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create results directory\n",
    "    os.makedirs(\"evaluation/results/binary/SVM\", exist_ok=True)\n",
    "    \n",
    "    # Save baseline results with updated filename format\n",
    "    baseline_results_file = f\"evaluation/results/binary/SVM/{model_name}_{dataset_name}_bin_baseline_results.json\"\n",
    "    \n",
    "    with open(baseline_results_file, 'w') as f:\n",
    "        json.dump(baseline_results, f, indent=2, cls=NumpyEncoder)\n",
    "    \n",
    "    print(f\"âœ… Enhanced baseline results saved to: {baseline_results_file}\")\n",
    "    \n",
    "    # Display evaluation results\n",
    "    evaluate_binary_classification(\n",
    "        gold_test_bin, test_pred_json,\n",
    "        y_test, y_pred_svm,\n",
    "        gold_test_txt, test_pred_txt,\n",
    "        label_names,\n",
    "        model_name=\"SVM baseline\"\n",
    "    )\n",
    "    \n",
    "    return baseline_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec17f8e-50a5-4da8-b796-0671fc5e12ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAMI Dataset\n",
    "print(\"ðŸš€ Running MAMI baseline with save...\")\n",
    "mami_baseline_results = run_baseline_with_save(\n",
    "    \"MAMI\", mami_training_df, mami_test_df, \n",
    "    \"misogynous\", [\"non-misogynous\", \"misogynous\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e84d5b-5959-4ed8-9695-ba4d9a21cfc9",
   "metadata": {},
   "source": [
    "## Multi-label Classification: Misogyny Categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2256c2cd-fcd6-4a71-b342-fc4e9b52896e",
   "metadata": {},
   "source": [
    "### Hierarchical classification: only fine-grained categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273a7a29-79a3-4065-96ac-b3c3f38a563a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multilabel_baseline_with_predictions(dataset_name, training_df, test_df, binary_label, fine_grained_labels):\n",
    "    \"\"\"\n",
    "    Run multilabel baseline using hierarchical approach and save predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ RUNNING MULTILABEL BASELINE FOR {dataset_name} (WITH PREDICTIONS)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Prepare data for hierarchical approach\n",
    "    X_train = training_df[\"svm representation\"].tolist()\n",
    "    y_train_binary = training_df[binary_label].tolist()\n",
    "    train_df_bin_positive = training_df.loc[training_df[binary_label] == 1]\n",
    "    X_train_bin_positive = train_df_bin_positive[\"svm representation\"].tolist()\n",
    "    y_train_categories = train_df_bin_positive[fine_grained_labels]\n",
    "    \n",
    "    X_test = test_df[\"svm representation\"].tolist()\n",
    "    \n",
    "    # Prepare complete label list (binary + fine-grained)\n",
    "    all_labels = [binary_label] + fine_grained_labels\n",
    "    y_test_all = test_df[all_labels]\n",
    "    \n",
    "    # Train models for hierarchical classification and get predictions\n",
    "    test_pred_df, bin_clf, bin_vec, ml_model, ml_vec = build_hierarchical_multilabel_classifier(\n",
    "        X_train, y_train_binary, X_train_bin_positive, y_train_categories, \n",
    "        X_test, binary_label, fine_grained_labels\n",
    "    )\n",
    "    \n",
    "    # Set model name and evaluation type\n",
    "    model_name = \"svm_baseline_bow_hierarchy\"\n",
    "    evaluation_type = \"hierarchical\"\n",
    "    \n",
    "    # Set gold file paths\n",
    "    if dataset_name == \"MAMI\":\n",
    "        gold_test_ml = \"\" # Path to gold label of MAMI for multi-label classification (json file) \n",
    "        gold_test_txt = \"\" # Path to gold label of MAMI (txt file)\n",
    "    else:  # EXIST2024\n",
    "        gold_test_ml = \"\" # Path to gold label of EXIST2024 for multi-label classification (json file) \n",
    "        gold_test_txt = \"\" # Path to gold label of EXIST2024 (txt file)\n",
    "    \n",
    "    # Create file with predictions\n",
    "    test_pred_json_ml, test_pred_txt_ml = save_evaluation(\n",
    "        test_df, \"evaluation/predictions\", dataset_name, \"test\", evaluation_type, \n",
    "        model_name, test_pred_df, binary_label, all_labels\n",
    "    )\n",
    "    \n",
    "    # Get evaluation metrics\n",
    "    baseline_metrics = evaluate_multilabel_classification(\n",
    "        gold_test_ml, test_pred_json_ml,\n",
    "        y_test_all, test_pred_df.to_numpy(),\n",
    "        gold_test_txt, test_pred_txt_ml,\n",
    "        all_labels, hierarchy=True\n",
    "    )\n",
    "    \n",
    "    \n",
    "    baseline_metrics['predictions'] = test_pred_df.to_numpy().tolist()  \n",
    "    baseline_metrics['true_labels'] = y_test_all.values.tolist()  \n",
    "    baseline_metrics['prediction_files'] = {\n",
    "        'json': test_pred_json_ml,\n",
    "        'txt': test_pred_txt_ml\n",
    "    }\n",
    "    baseline_metrics['all_labels'] = all_labels\n",
    "    \n",
    "    # Save baseline results with predictions\n",
    "    os.makedirs(\"evaluation/results/multi-label/SVM/fine-grained\", exist_ok=True)\n",
    "    baseline_results_file = f\"evaluation/results/multi-label/SVM/fine-grained/{model_name}_{dataset_name}_results.json\"\n",
    "    \n",
    "    with open(baseline_results_file, 'w') as f:\n",
    "        json.dump(baseline_metrics, f, indent=2, cls=NumpyEncoder)\n",
    "    \n",
    "    print(f\"âœ… Multilabel baseline results with predictions saved to: {baseline_results_file}\")\n",
    "    \n",
    "    return baseline_metrics\n",
    "    \n",
    "\n",
    "print(\"ðŸš€ Running MAMI multilabel baseline with predictions...\")\n",
    "mami_multilabel_baseline = run_multilabel_baseline_with_predictions(\n",
    "    \"MAMI\", mami_training_df, mami_test_df, \"misogynous\", \n",
    "    [\"shaming\", \"stereotype\", \"objectification\", \"violence\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ff78e8-6003-4e19-b23c-d6e0468267d4",
   "metadata": {},
   "source": [
    "# EXIST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208d813d-cda8-4cd9-899f-779ae5dc92f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file paths\n",
    "\n",
    "\n",
    "exist_training_data = \"\" # Path to training data\n",
    "exist_dev_data =  # Path to development data  \n",
    "exist_test_data = \"\"  # Path to test data\n",
    "\n",
    "exist_training_df = pd.read_json(exist_training_data,orient='index')\n",
    "exist_dev_df = pd.read_json(exist_dev_data,orient='index')\n",
    "exist_test_df = pd.read_json(exist_test_data,orient='index')\n",
    "\n",
    "exist_training_df = pd.concat([exist_training_df, exist_dev_df]).sort_index()\n",
    "\n",
    "# Check available columns\n",
    "print(\"Training data columns:\", exist_training_df.columns)\n",
    "print(\"Test data columns:\", exist_test_df.columns)\n",
    "\n",
    "\n",
    "#gold labels \n",
    "gold_test_txt = \"\" # Path to EXIST2024 text gold label (txt)\n",
    "gold_test_bin = \"\" # Path to EXIST2024 binary classification gold labels (json) \n",
    "gold_test_ml =  \"\" # Path to EXIST2024 multi-label classification gold labels (json)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb17da0-063c-43c0-a5cd-9939e353ad0f",
   "metadata": {},
   "source": [
    "## Binary Classification: Sexist vs. non-sexist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7a4dff-2332-44db-a524-da2d64e910f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXIST Dataset  \n",
    "print(\"ðŸš€ Running EXIST baseline with save...\")\n",
    "exist_baseline_results = run_baseline_with_save(\n",
    "    \"EXIST2024\", exist_training_df, exist_test_df,\n",
    "    \"sexist\", [\"non-sexist\", \"sexist\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d45e5bc-a13f-48da-97b1-cf63a986e3dd",
   "metadata": {},
   "source": [
    "## Multi-label Classification: Sexism Categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39b60bd-6492-42e5-a380-7fdf19b8c76b",
   "metadata": {},
   "source": [
    "### Hierarchical classification: only fine-grained categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46005b69-9278-487c-a740-7e17888935b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"ðŸš€ Running EXIST2024 multilabel baseline with predictions...\")\n",
    "exist_multilabel_baseline = run_multilabel_baseline_with_predictions(\n",
    "    \"EXIST2024\", exist_training_df, exist_test_df, \"sexist\",\n",
    "    [\"ideological-inequality\", \"stereotyping-dominance\", \"objectification\", \n",
    "     \"sexual-violence\", \"misogyny-non-sexual-violence\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baee3f1a-63f0-40a3-bf96-4a96dd4baa1e",
   "metadata": {},
   "source": [
    "# Ablation Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a1cce3-7491-4848-9c73-d908d0264050",
   "metadata": {},
   "source": [
    "### Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faa11f1-0dcb-4000-81bd-ab4f2150ed23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_random_including_results(y_test, y_pred_random_including, dataset_name, feature_type, ablation_method):\n",
    "    \"\"\"\n",
    "    Save random including ablation results to JSON file.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_test : list\n",
    "        True test labels\n",
    "    y_pred_random_including : array\n",
    "        Predictions from random including ablation model\n",
    "    dataset_name : str\n",
    "        Dataset name ('MAMI' or 'EXIST2024')\n",
    "    feature_type : str\n",
    "        Feature type being ablated (e.g., 'sentiment_pos', 'sentiment_neg', 'hate', 'function')\n",
    "    ablation_method : str\n",
    "        Ablation method ('mask' or 'remove')\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred_random_including)\n",
    "    precision_macro = precision_score(y_test, y_pred_random_including, average='macro')\n",
    "    recall_macro = recall_score(y_test, y_pred_random_including, average='macro')\n",
    "    f1_macro = f1_score(y_test, y_pred_random_including, average='macro')\n",
    "    \n",
    "    # Set label names based on dataset\n",
    "    if dataset_name == \"MAMI\":\n",
    "        label_names = [\"non-misogynous\", \"misogynous\"]\n",
    "    else:  # EXIST2024\n",
    "        label_names = [\"non-sexist\", \"sexist\"]\n",
    "    \n",
    "    # Generate detailed classification report\n",
    "    class_report_dict = classification_report(y_test, y_pred_random_including, \n",
    "                                            target_names=label_names, \n",
    "                                            zero_division=0, digits=3, \n",
    "                                            output_dict=True)\n",
    "    \n",
    "    # Create results dictionary matching the format used for feature ablation\n",
    "    random_results = {\n",
    "        'binary_f1': f1_macro,  # Using macro F1 as binary F1 metric\n",
    "        'macro_f1': f1_macro,\n",
    "        'accuracy': accuracy,\n",
    "        'precision_macro': precision_macro,\n",
    "        'recall_macro': recall_macro,\n",
    "        'per_label_metrics': class_report_dict\n",
    "    }\n",
    "    \n",
    "    # Generate filename following the same pattern as feature ablation results\n",
    "    model_name = f\"random_including_{feature_type}_{ablation_method}_{dataset_name.lower()}\"\n",
    "    os.makedirs(\"evaluation/results/binary/SVM\", exist_ok=True)\n",
    "    results_file = f\"evaluation/results/binary/SVM/{model_name}_bin_results.json\"\n",
    "    \n",
    "    # Save results to JSON file using custom encoder for numpy arrays\n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(random_results, f, indent=2, cls=NumpyEncoder)\n",
    "    \n",
    "    print(f\"âœ… Random including results saved to: {results_file}\")\n",
    "\n",
    "\n",
    "def run_coarse_binary_ablation_experiment(dataset_df_train, dataset_df_test, feature_type, \n",
    "                                        ablation_method, dataset_name, binary_label='misogynous'):\n",
    "    \"\"\"\n",
    "    Run a coarse-grained ablation experiment with three comparison conditions:\n",
    "    1. Feature ablation (remove/mask specific feature category)\n",
    "    2. Random ablation excluding feature words (remove/mask same number of random words, but not the feature words)\n",
    "    3. Random ablation including feature words (remove/mask same number of random words, potentially including feature words)\n",
    "    \n",
    "    For function words, comparison is to random words.\n",
    "    For sentiment/hate speech terms, comparison is to random content words (non-function words).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset_df_train : DataFrame\n",
    "        Training dataset\n",
    "    dataset_df_test : DataFrame  \n",
    "        Test dataset\n",
    "    feature_type : str\n",
    "        Type of feature:\n",
    "        - 'sentiment_pos': only positive sentiment words\n",
    "        - 'sentiment_neg': only negative sentiment words\n",
    "        - 'hate': all hate speech terms as a group\n",
    "        - 'function': all function words as a group\n",
    "    ablation_method : str\n",
    "        'mask' or 'remove'\n",
    "    dataset_name : str\n",
    "        Name of dataset (e.g., 'MAMI')\n",
    "    binary_label : str\n",
    "        Column for binary classification\n",
    "    \"\"\"\n",
    "    # Load necessary lexicons\n",
    "    # Note: Lexicon files are not included in this repository. \n",
    "    # Please download them from their respective sources before running this code.\n",
    "    nrc_lexicon = load_nrc_lexicon(\"NRC-Emotion-Lexicon/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\")\n",
    "    function_words_dict = load_function_words()\n",
    "    hurtlex_dict = load_hurtlex(\"hurtlex-master/lexica/EN/1.2/hurtlex_EN.tsv\") \n",
    "    \n",
    "    # Get original texts\n",
    "    X_train_orig = dataset_df_train[\"svm representation\"].tolist()\n",
    "    X_test_orig = dataset_df_test[\"svm representation\"].tolist()\n",
    "    y_train = dataset_df_train[binary_label].tolist()\n",
    "    y_test = dataset_df_test[binary_label].tolist()\n",
    "    \n",
    "    # Prepare containers for different ablation conditions\n",
    "    X_train_feature_ablated = []            # Feature words ablated\n",
    "    X_train_random_excluding_ablated = []   # Random words ablated (excluding feature words)\n",
    "    X_train_random_including_ablated = []   # Random words ablated (including feature words)\n",
    "    \n",
    "    X_test_feature_ablated = []\n",
    "    X_test_random_excluding_ablated = []\n",
    "    X_test_random_including_ablated = []\n",
    "    \n",
    "    print(f\"Running coarse-grained ablation experiment for {feature_type}...\")\n",
    "    print(\"1. Feature ablation\")\n",
    "    if feature_type in ['sentiment_pos', 'sentiment_neg', 'hate']:\n",
    "        print(\"2. Random content word ablation excluding feature words\")\n",
    "        print(\"3. Random content word ablation including feature words\")\n",
    "        random_comparison_type = \"content words\"\n",
    "    else:\n",
    "        print(\"2. Random word ablation excluding feature words\")\n",
    "        print(\"3. Random word ablation including feature words\")\n",
    "        random_comparison_type = \"words\"\n",
    "    \n",
    "    # Training set statistics\n",
    "    train_total_feature_words_processed = 0\n",
    "    train_documents_with_features = 0\n",
    "    train_feature_words_per_doc = []\n",
    "    \n",
    "    # Test set statistics  \n",
    "    test_total_feature_words_processed = 0\n",
    "    test_documents_with_features = 0\n",
    "    test_feature_words_per_doc = []\n",
    "    \n",
    "    # Get all function words (needed for sentiment/hate comparisons)\n",
    "    all_function_words = set()\n",
    "    for category, words in function_words_dict.items():\n",
    "        all_function_words.update(words)\n",
    "    \n",
    "    # Process training data\n",
    "    for text in X_train_orig:\n",
    "        # Extract feature words based on feature type\n",
    "        feature_words = []\n",
    "        if feature_type == 'sentiment_pos':\n",
    "            # Extract only positive sentiment words\n",
    "            feature_words = extract_sentiment_words(text, nrc_lexicon, 'positive')\n",
    "            \n",
    "        elif feature_type == 'sentiment_neg':\n",
    "            # Extract only negative sentiment words\n",
    "            feature_words = extract_sentiment_words(text, nrc_lexicon, 'negative')\n",
    "            \n",
    "        elif feature_type == 'hate':\n",
    "            # Extract all hate speech terms (any category)\n",
    "            hate_words = []\n",
    "            for word in text.lower().split():\n",
    "                if word in hurtlex_dict:\n",
    "                    hate_words.append(word)\n",
    "            feature_words = list(set(hate_words))\n",
    "            \n",
    "        elif feature_type == 'function':\n",
    "            # Extract all function words from all categories\n",
    "            function_words = []\n",
    "            for category in function_words_dict.keys():\n",
    "                category_words = extract_function_words(text, function_words_dict, category)\n",
    "                function_words.extend(category_words)\n",
    "            feature_words = list(set(function_words))\n",
    "        \n",
    "        # Convert to set for faster lookup\n",
    "        feature_words_set = set(feature_words)\n",
    "        \n",
    "        # Count feature words in this text\n",
    "        num_feature_words = len([word for word in text.lower().split() if word in feature_words_set])\n",
    "        train_feature_words_per_doc.append(num_feature_words)\n",
    "        \n",
    "        if num_feature_words > 0:\n",
    "            train_documents_with_features += 1\n",
    "        \n",
    "        # Determine random word sampling strategy based on feature type\n",
    "        if feature_type in ['sentiment_pos', 'sentiment_neg', 'hate']:\n",
    "            # For sentiment and hate speech, compare to random content words (excluding function words)\n",
    "            # Get all non-function words in the text\n",
    "            all_words = text.lower().split()\n",
    "            content_words_indices = [i for i, word in enumerate(all_words) if word not in all_function_words]\n",
    "            \n",
    "            # Process with feature ablation\n",
    "            if ablation_method == 'mask':\n",
    "                feature_ablated_text, count = replace_mask_features(text, feature_words, mask_token='UNK')\n",
    "                \n",
    "                # 1. Random ablation EXCLUDING feature words\n",
    "                random_excluding_ablated_text = text\n",
    "                if content_words_indices and num_feature_words > 0:\n",
    "                    # Get indices of content words that are NOT feature words\n",
    "                    words_list = text.lower().split()\n",
    "                    feature_word_indices = [i for i, word in enumerate(words_list) if word in feature_words_set]\n",
    "                    non_feature_content_indices = [i for i in content_words_indices if i not in feature_word_indices]\n",
    "                    \n",
    "                    if non_feature_content_indices:\n",
    "                        # Adjust number to mask if fewer eligible words available\n",
    "                        num_to_mask = min(num_feature_words, len(non_feature_content_indices))\n",
    "                        rand_indices = random.sample(non_feature_content_indices, num_to_mask)\n",
    "                        \n",
    "                        # Convert to list for modification\n",
    "                        random_words = text.split()\n",
    "                        for idx in rand_indices:\n",
    "                            random_words[idx] = 'UNK'\n",
    "                        random_excluding_ablated_text = ' '.join(random_words)\n",
    "                \n",
    "                # 2. Random ablation INCLUDING feature words\n",
    "                random_including_ablated_text = text\n",
    "                if content_words_indices and num_feature_words > 0:\n",
    "                    # Adjust number to mask if fewer content words available\n",
    "                    num_to_mask = min(num_feature_words, len(content_words_indices))\n",
    "                    rand_indices = random.sample(content_words_indices, num_to_mask)\n",
    "                    \n",
    "                    # Convert to list for modification\n",
    "                    random_words = text.split()\n",
    "                    for idx in rand_indices:\n",
    "                        random_words[idx] = 'UNK'\n",
    "                    random_including_ablated_text = ' '.join(random_words)\n",
    "            else:  # remove\n",
    "                feature_ablated_text, count = remove_features(text, feature_words)\n",
    "                \n",
    "                # 1. Random ablation EXCLUDING feature words\n",
    "                random_excluding_ablated_text = text\n",
    "                if content_words_indices and num_feature_words > 0:\n",
    "                    # Get indices of content words that are NOT feature words\n",
    "                    words_list = text.lower().split()\n",
    "                    feature_word_indices = [i for i, word in enumerate(words_list) if word in feature_words_set]\n",
    "                    non_feature_content_indices = [i for i in content_words_indices if i not in feature_word_indices]\n",
    "                    \n",
    "                    if non_feature_content_indices:\n",
    "                        # Adjust number to remove if fewer eligible words available\n",
    "                        num_to_remove = min(num_feature_words, len(non_feature_content_indices))\n",
    "                        rand_indices = random.sample(non_feature_content_indices, num_to_remove)\n",
    "                        \n",
    "                        # Sort indices in descending order to avoid index shifting when removing\n",
    "                        rand_indices.sort(reverse=True)\n",
    "                        \n",
    "                        # Convert to list for modification\n",
    "                        random_words = text.split()\n",
    "                        for idx in rand_indices:\n",
    "                            del random_words[idx]\n",
    "                        random_excluding_ablated_text = ' '.join(random_words)\n",
    "                \n",
    "                # 2. Random ablation INCLUDING feature words\n",
    "                random_including_ablated_text = text\n",
    "                if content_words_indices and num_feature_words > 0:\n",
    "                    # Adjust number to remove if fewer content words available\n",
    "                    num_to_remove = min(num_feature_words, len(content_words_indices))\n",
    "                    rand_indices = random.sample(content_words_indices, num_to_remove)\n",
    "                    \n",
    "                    # Sort indices in descending order to avoid index shifting when removing\n",
    "                    rand_indices.sort(reverse=True)\n",
    "                    \n",
    "                    # Convert to list for modification\n",
    "                    random_words = text.split()\n",
    "                    for idx in rand_indices:\n",
    "                        del random_words[idx]\n",
    "                    random_including_ablated_text = ' '.join(random_words)\n",
    "            \n",
    "        else:  # function words\n",
    "            # For function words, compare to random words from entire vocabulary\n",
    "            if ablation_method == 'mask':\n",
    "                feature_ablated_text, count = replace_mask_features(text, feature_words, mask_token='UNK')\n",
    "                \n",
    "                # 1. Random ablation EXCLUDING function words\n",
    "                words_list = text.lower().split()\n",
    "                non_function_indices = [i for i, word in enumerate(words_list) if word not in feature_words_set]\n",
    "                \n",
    "                random_excluding_ablated_text = text\n",
    "                if non_function_indices and num_feature_words > 0:\n",
    "                    # Adjust number to mask if fewer eligible words available\n",
    "                    num_to_mask = min(num_feature_words, len(non_function_indices))\n",
    "                    rand_indices = random.sample(non_function_indices, num_to_mask)\n",
    "                    \n",
    "                    # Convert to list for modification\n",
    "                    random_words = text.split()\n",
    "                    for idx in rand_indices:\n",
    "                        random_words[idx] = 'UNK'\n",
    "                    random_excluding_ablated_text = ' '.join(random_words)\n",
    "                \n",
    "                # 2. Random ablation INCLUDING all words\n",
    "                random_including_ablated_text, _ = replace_mask_random_words(text, num_feature_words, mask_token='UNK')\n",
    "            else:  # remove\n",
    "                feature_ablated_text, count = remove_features(text, feature_words)\n",
    "                \n",
    "                # 1. Random ablation EXCLUDING function words\n",
    "                words_list = text.lower().split()\n",
    "                non_function_indices = [i for i, word in enumerate(words_list) if word not in feature_words_set]\n",
    "                \n",
    "                random_excluding_ablated_text = text\n",
    "                if non_function_indices and num_feature_words > 0:\n",
    "                    # Adjust number to remove if fewer eligible words available\n",
    "                    num_to_remove = min(num_feature_words, len(non_function_indices))\n",
    "                    rand_indices = random.sample(non_function_indices, num_to_remove)\n",
    "                    \n",
    "                    # Sort indices in descending order to avoid index shifting when removing\n",
    "                    rand_indices.sort(reverse=True)\n",
    "                    \n",
    "                    # Convert to list for modification\n",
    "                    random_words = text.split()\n",
    "                    for idx in rand_indices:\n",
    "                        del random_words[idx]\n",
    "                    random_excluding_ablated_text = ' '.join(random_words)\n",
    "                \n",
    "                # 2. Random ablation INCLUDING all words\n",
    "                random_including_ablated_text, _ = remove_random_words(text, num_feature_words)\n",
    "        \n",
    "        X_train_feature_ablated.append(feature_ablated_text)\n",
    "        X_train_random_excluding_ablated.append(random_excluding_ablated_text)\n",
    "        X_train_random_including_ablated.append(random_including_ablated_text)\n",
    "        train_total_feature_words_processed += count\n",
    "    \n",
    "    # Process test data similarly\n",
    "    for text in X_test_orig:\n",
    "        # Extract feature words based on feature type\n",
    "        feature_words = []\n",
    "        if feature_type == 'sentiment_pos':\n",
    "            # Extract only positive sentiment words\n",
    "            feature_words = extract_sentiment_words(text, nrc_lexicon, 'positive')\n",
    "            \n",
    "        elif feature_type == 'sentiment_neg':\n",
    "            # Extract only negative sentiment words\n",
    "            feature_words = extract_sentiment_words(text, nrc_lexicon, 'negative')\n",
    "            \n",
    "        elif feature_type == 'hate':\n",
    "            # Extract all hate speech terms (any category)\n",
    "            hate_words = []\n",
    "            for word in text.lower().split():\n",
    "                if word in hurtlex_dict:\n",
    "                    hate_words.append(word)\n",
    "            feature_words = list(set(hate_words))\n",
    "            \n",
    "        elif feature_type == 'function':\n",
    "            # Extract all function words from all categories\n",
    "            function_words = []\n",
    "            for category in function_words_dict.keys():\n",
    "                category_words = extract_function_words(text, function_words_dict, category)\n",
    "                function_words.extend(category_words)\n",
    "            feature_words = list(set(function_words))\n",
    "        \n",
    "        # Convert to set for faster lookup\n",
    "        feature_words_set = set(feature_words)\n",
    "        \n",
    "        # Count feature words in this text\n",
    "        num_feature_words = len([word for word in text.lower().split() if word in feature_words_set])\n",
    "        test_feature_words_per_doc.append(num_feature_words)\n",
    "        \n",
    "        if num_feature_words > 0:\n",
    "            test_documents_with_features += 1\n",
    "        \n",
    "        # Determine random word sampling strategy based on feature type\n",
    "        if feature_type in ['sentiment_pos', 'sentiment_neg', 'hate']:\n",
    "            # For sentiment and hate speech, compare to random content words (excluding function words)\n",
    "            # Get all non-function words in the text\n",
    "            all_words = text.lower().split()\n",
    "            content_words_indices = [i for i, word in enumerate(all_words) if word not in all_function_words]\n",
    "            \n",
    "            # Process with feature ablation\n",
    "            if ablation_method == 'mask':\n",
    "                feature_ablated_text, count = replace_mask_features(text, feature_words, mask_token='UNK')\n",
    "                \n",
    "                # 1. Random ablation EXCLUDING feature words\n",
    "                random_excluding_ablated_text = text\n",
    "                if content_words_indices and num_feature_words > 0:\n",
    "                    # Get indices of content words that are NOT feature words\n",
    "                    words_list = text.lower().split()\n",
    "                    feature_word_indices = [i for i, word in enumerate(words_list) if word in feature_words_set]\n",
    "                    non_feature_content_indices = [i for i in content_words_indices if i not in feature_word_indices]\n",
    "                    \n",
    "                    if non_feature_content_indices:\n",
    "                        # Adjust number to mask if fewer eligible words available\n",
    "                        num_to_mask = min(num_feature_words, len(non_feature_content_indices))\n",
    "                        rand_indices = random.sample(non_feature_content_indices, num_to_mask)\n",
    "                        \n",
    "                        # Convert to list for modification\n",
    "                        random_words = text.split()\n",
    "                        for idx in rand_indices:\n",
    "                            random_words[idx] = 'UNK'\n",
    "                        random_excluding_ablated_text = ' '.join(random_words)\n",
    "                \n",
    "                # 2. Random ablation INCLUDING feature words\n",
    "                random_including_ablated_text = text\n",
    "                if content_words_indices and num_feature_words > 0:\n",
    "                    # Adjust number to mask if fewer content words available\n",
    "                    num_to_mask = min(num_feature_words, len(content_words_indices))\n",
    "                    rand_indices = random.sample(content_words_indices, num_to_mask)\n",
    "                    \n",
    "                    # Convert to list for modification\n",
    "                    random_words = text.split()\n",
    "                    for idx in rand_indices:\n",
    "                        random_words[idx] = 'UNK'\n",
    "                    random_including_ablated_text = ' '.join(random_words)\n",
    "            else:  # remove\n",
    "                feature_ablated_text, count = remove_features(text, feature_words)\n",
    "                \n",
    "                # 1. Random ablation EXCLUDING feature words\n",
    "                random_excluding_ablated_text = text\n",
    "                if content_words_indices and num_feature_words > 0:\n",
    "                    # Get indices of content words that are NOT feature words\n",
    "                    words_list = text.lower().split()\n",
    "                    feature_word_indices = [i for i, word in enumerate(words_list) if word in feature_words_set]\n",
    "                    non_feature_content_indices = [i for i in content_words_indices if i not in feature_word_indices]\n",
    "                    \n",
    "                    if non_feature_content_indices:\n",
    "                        # Adjust number to remove if fewer eligible words available\n",
    "                        num_to_remove = min(num_feature_words, len(non_feature_content_indices))\n",
    "                        rand_indices = random.sample(non_feature_content_indices, num_to_remove)\n",
    "                        \n",
    "                        # Sort indices in descending order to avoid index shifting when removing\n",
    "                        rand_indices.sort(reverse=True)\n",
    "                        \n",
    "                        # Convert to list for modification\n",
    "                        random_words = text.split()\n",
    "                        for idx in rand_indices:\n",
    "                            del random_words[idx]\n",
    "                        random_excluding_ablated_text = ' '.join(random_words)\n",
    "                \n",
    "                # 2. Random ablation INCLUDING feature words\n",
    "                random_including_ablated_text = text\n",
    "                if content_words_indices and num_feature_words > 0:\n",
    "                    # Adjust number to remove if fewer content words available\n",
    "                    num_to_remove = min(num_feature_words, len(content_words_indices))\n",
    "                    rand_indices = random.sample(content_words_indices, num_to_remove)\n",
    "                    \n",
    "                    # Sort indices in descending order to avoid index shifting when removing\n",
    "                    rand_indices.sort(reverse=True)\n",
    "                    \n",
    "                    # Convert to list for modification\n",
    "                    random_words = text.split()\n",
    "                    for idx in rand_indices:\n",
    "                        del random_words[idx]\n",
    "                    random_including_ablated_text = ' '.join(random_words)\n",
    "            \n",
    "        else:  # function words\n",
    "            # For function words, compare to random words from entire vocabulary\n",
    "            if ablation_method == 'mask':\n",
    "                feature_ablated_text, count = replace_mask_features(text, feature_words, mask_token='UNK')\n",
    "                \n",
    "                # 1. Random ablation EXCLUDING function words\n",
    "                words_list = text.lower().split()\n",
    "                non_function_indices = [i for i, word in enumerate(words_list) if word not in feature_words_set]\n",
    "                \n",
    "                random_excluding_ablated_text = text\n",
    "                if non_function_indices and num_feature_words > 0:\n",
    "                    # Adjust number to mask if fewer eligible words available\n",
    "                    num_to_mask = min(num_feature_words, len(non_function_indices))\n",
    "                    rand_indices = random.sample(non_function_indices, num_to_mask)\n",
    "                    \n",
    "                    # Convert to list for modification\n",
    "                    random_words = text.split()\n",
    "                    for idx in rand_indices:\n",
    "                        random_words[idx] = 'UNK'\n",
    "                    random_excluding_ablated_text = ' '.join(random_words)\n",
    "                \n",
    "                # 2. Random ablation INCLUDING all words\n",
    "                random_including_ablated_text, _ = replace_mask_random_words(text, num_feature_words, mask_token='UNK')\n",
    "            else:  # remove\n",
    "                feature_ablated_text, count = remove_features(text, feature_words)\n",
    "                \n",
    "                # 1. Random ablation EXCLUDING function words\n",
    "                words_list = text.lower().split()\n",
    "                non_function_indices = [i for i, word in enumerate(words_list) if word not in feature_words_set]\n",
    "                \n",
    "                random_excluding_ablated_text = text\n",
    "                if non_function_indices and num_feature_words > 0:\n",
    "                    # Adjust number to remove if fewer eligible words available\n",
    "                    num_to_remove = min(num_feature_words, len(non_function_indices))\n",
    "                    rand_indices = random.sample(non_function_indices, num_to_remove)\n",
    "                    \n",
    "                    # Sort indices in descending order to avoid index shifting when removing\n",
    "                    rand_indices.sort(reverse=True)\n",
    "                    \n",
    "                    # Convert to list for modification\n",
    "                    random_words = text.split()\n",
    "                    for idx in rand_indices:\n",
    "                        del random_words[idx]\n",
    "                    random_excluding_ablated_text = ' '.join(random_words)\n",
    "                \n",
    "                # 2. Random ablation INCLUDING all words\n",
    "                random_including_ablated_text, _ = remove_random_words(text, num_feature_words)\n",
    "        \n",
    "        X_test_feature_ablated.append(feature_ablated_text)\n",
    "        X_test_random_including_ablated.append(random_including_ablated_text)\n",
    "        test_total_feature_words_processed += count\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\n{'=' * 50}\")\n",
    "    print(f\"Processing Summary:\")\n",
    "    print(f\"Training set:\")\n",
    "    print(f\"  Total documents: {len(X_train_feature_ablated)}\")\n",
    "    print(f\"  Documents with features: {train_documents_with_features}\")\n",
    "    print(f\"  Total feature words processed: {train_total_feature_words_processed}\")\n",
    "    print(f\"  Average feature words per document: {np.mean(train_feature_words_per_doc):.2f}\")\n",
    "    \n",
    "    print(f\"Test set:\")\n",
    "    print(f\"  Total documents: {len(X_test_feature_ablated)}\")\n",
    "    print(f\"  Documents with features: {test_documents_with_features}\")\n",
    "    print(f\"  Total feature words processed: {test_total_feature_words_processed}\")\n",
    "    print(f\"  Average feature words per document: {np.mean(test_feature_words_per_doc):.2f}\")\n",
    "\n",
    "    print(f\"{'=' * 50}\")\n",
    "    \n",
    "    # Set up model names\n",
    "    feature_model_name = f\"{feature_type}_{ablation_method}_{dataset_name.lower()}\"\n",
    "    random_including_model_name = f\"random_including_{feature_type}_{ablation_method}_{dataset_name.lower()}\"\n",
    "\n",
    "    # Set up gold standard file paths based on dataset\n",
    "    if dataset_name == \"MAMI\":\n",
    "        gold_test_bin = \"\" # Path to MAMI binary classification gold labels (json)\n",
    "        gold_test_txt = \"\" # Path to MAMI text gold labels (txt)\n",
    "        label_names = [\"non-misogynous\", \"misogynous\"]\n",
    "    else:  # EXIST2024\n",
    "        gold_test_bin = \"\" # Path to EXIST2024 binary classification gold labels (json)\n",
    "        gold_test_txt = \"\" # Path to EXIST2024 text gold labels (txt)\n",
    "        label_names = [\"non-sexist\", \"sexist\"]\n",
    "    \n",
    "\n",
    "    \n",
    "    # Train model on feature-ablated data\n",
    "    print(\"Training feature ablation model...\")\n",
    "    feature_model, feature_vec = build_bin_classifier(X_train_feature_ablated, y_train)\n",
    "    y_pred_feature = classify_data(X_test_feature_ablated, feature_model, feature_vec)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Train model on random-ablated data (including feature words)\n",
    "    print(\"Training random ablation model (including feature words)...\")\n",
    "    random_including_model, random_including_vec = build_bin_classifier(X_train_random_including_ablated, y_train)\n",
    "    y_pred_random_including = classify_data(X_test_random_including_ablated, random_including_model, random_including_vec)\n",
    "    \n",
    "    # Create files with predictions for all three models\n",
    "    # 1. Feature ablation\n",
    "    print(f\"\\n{'='*20} Feature Ablation Results {'='*20}\")\n",
    "    test_pred_json_feature, test_pred_txt_feature = save_evaluation(\n",
    "        dataset_df_test, \"evaluation/predictions\", dataset_name, \"test\", \"binary\", \n",
    "        feature_model_name, y_pred_feature, binary_label, []\n",
    "    )\n",
    "    feature_metrics = evaluate_binary_classification(\n",
    "        gold_test_bin, test_pred_json_feature, y_test, y_pred_feature, \n",
    "        gold_test_txt, test_pred_txt_feature, label_names, model_name=feature_model_name\n",
    "    )\n",
    "    \n",
    "    # 2. Random ablation - Excluding feature words\n",
    "    print(f\"\\n{'='*20} Random Ablation - EXCLUDING feature words ({random_comparison_type}) Results {'='*20}\")\n",
    "    test_pred_json_random_excluding, test_pred_txt_random_excluding = save_evaluation(\n",
    "        dataset_df_test, \"evaluation/predictions\", dataset_name, \"test\", \"binary\", \n",
    "        random_excluding_model_name, y_pred_random_excluding, binary_label, []\n",
    "    )\n",
    "    random_excluding_metrics = evaluate_binary_classification(\n",
    "        gold_test_bin, test_pred_json_random_excluding, y_test, y_pred_random_excluding, \n",
    "        gold_test_txt, test_pred_txt_random_excluding, label_names, model_name=random_excluding_model_name\n",
    "    )\n",
    "    \n",
    "    # 3. Random ablation - Including feature words\n",
    "    print(f\"\\n{'='*20} Random Ablation - INCLUDING feature words ({random_comparison_type}) Results {'='*20}\")\n",
    "    test_pred_json_random_including, test_pred_txt_random_including = save_evaluation(\n",
    "        dataset_df_test, \"evaluation/predictions\", dataset_name, \"test\", \"binary\", \n",
    "        random_including_model_name, y_pred_random_including, binary_label, []\n",
    "    )\n",
    "    random_including_metrics = evaluate_binary_classification(\n",
    "        gold_test_bin, test_pred_json_random_including, y_test, y_pred_random_including, \n",
    "        gold_test_txt, test_pred_txt_random_including, label_names, model_name=random_including_model_name\n",
    "    )\n",
    "    \n",
    "    # Calculate metrics for feature ablation model\n",
    "    accuracy = accuracy_score(y_test, y_pred_feature)\n",
    "    precision_macro = precision_score(y_test, y_pred_feature, average='macro')\n",
    "    recall_macro = recall_score(y_test, y_pred_feature, average='macro')\n",
    "    f1_macro = f1_score(y_test, y_pred_feature, average='macro')\n",
    "    \n",
    "    # Get classification report as dictionary\n",
    "    class_report_dict = classification_report(y_test, y_pred_feature, \n",
    "                                            target_names=label_names, \n",
    "                                            zero_division=0, digits=3, \n",
    "                                            output_dict=True)\n",
    "    \n",
    "    # Calculate binary F1 score (MAMI evaluation metric)\n",
    "    binary_f1 = evaluate_f1_scores(gold_test_txt, test_pred_txt_feature, 2)\n",
    "    \n",
    "    # Create structured results dictionary - ONLY for feature ablation\n",
    "    results = {\n",
    "        'feature_type': feature_type,\n",
    "        'ablation_method': ablation_method,\n",
    "        'dataset_name': dataset_name,\n",
    "        'binary_label': binary_label,\n",
    "        'feature_ablation': {\n",
    "            'model_name': feature_model_name,\n",
    "            'binary_f1': binary_f1,\n",
    "            'macro_f1': f1_macro,\n",
    "            'accuracy': accuracy,\n",
    "            'precision_macro': precision_macro,\n",
    "            'recall_macro': recall_macro,\n",
    "            'per_label_metrics': class_report_dict,\n",
    "            'prediction_files': {\n",
    "                'json': test_pred_json_feature,\n",
    "                'txt': test_pred_txt_feature\n",
    "            }\n",
    "        },\n",
    "        'train_statistics': {\n",
    "            'total_documents': len(X_train_feature_ablated),\n",
    "            'documents_with_features': train_documents_with_features,\n",
    "            'total_feature_words_processed': train_total_feature_words_processed,\n",
    "            'avg_feature_words_per_doc': np.mean(train_feature_words_per_doc),\n",
    "            'max_feature_words_per_doc': max(train_feature_words_per_doc) if train_feature_words_per_doc else 0,\n",
    "            'feature_coverage_percentage': train_documents_with_features/len(X_train_feature_ablated)*100\n",
    "        },\n",
    "        'test_statistics': {\n",
    "            'total_documents': len(X_test_feature_ablated),\n",
    "            'documents_with_features': test_documents_with_features,\n",
    "            'total_feature_words_processed': test_total_feature_words_processed,\n",
    "            'avg_feature_words_per_doc': np.mean(test_feature_words_per_doc),\n",
    "            'max_feature_words_per_doc': max(test_feature_words_per_doc) if test_feature_words_per_doc else 0,\n",
    "            'feature_coverage_percentage': test_documents_with_features/len(X_test_feature_ablated)*100\n",
    "        }\n",
    "    }\n",
    "\n",
    "    \n",
    "    \n",
    "    # Save feature ablation results to JSON file for later analysis\n",
    "    os.makedirs(\"evaluation/results/binary/SVM\", exist_ok=True)\n",
    "    results_file = f\"evaluation/results/binary/SVM/{feature_model_name}_bin_results.json\"\n",
    "    \n",
    "    # Create the format expected by analyze_coarse_binary_ablation_drops\n",
    "    save_results = {\n",
    "        'binary_f1': binary_f1,\n",
    "        'macro_f1': f1_macro,\n",
    "        'accuracy': accuracy,\n",
    "        'precision_macro': precision_macro,\n",
    "        'recall_macro': recall_macro,\n",
    "        'per_label_metrics': class_report_dict,\n",
    "        'predictions': y_pred_feature.tolist(),  # Save actual predictions\n",
    "        'true_labels': y_test  # Save true labels for reference\n",
    "    }\n",
    "    \n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(save_results, f, indent=2, cls=NumpyEncoder)\n",
    "    \n",
    "    print(f\"âœ… Feature ablation results saved to: {results_file}\")\n",
    "\n",
    "\n",
    "    # Save random including results for comparison\n",
    "    print(f\"ðŸ’¾ Saving random including results...\")\n",
    "    save_random_including_results(y_test, y_pred_random_including, dataset_name, feature_type, ablation_method)\n",
    "    \n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_binary_coarse_grained_experiments_without_baseline():\n",
    "    \"\"\"\n",
    "    Run all coarse-grained binary ablation experiments for both datasets (MAMI and EXIST2024).\n",
    "    This focuses only on binary classification experiments without baseline comparison.\n",
    "    Uses the modified ablation experiment that includes three conditions:\n",
    "    1. Feature ablation\n",
    "    2. Random ablation excluding feature words\n",
    "    3. Random ablation including feature words\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"RUNNING COARSE-GRAINED BINARY ABLATION EXPERIMENTS (WITHOUT BASELINE)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Define feature types and ablation methods\n",
    "    feature_types = ['sentiment_pos', 'sentiment_neg', 'hate', 'function']\n",
    "    ablation_methods = ['mask', 'remove']\n",
    "    \n",
    "    # Storage for results\n",
    "    all_results = {\n",
    "        'MAMI': {},\n",
    "        'EXIST2024': {}\n",
    "    }\n",
    "    \n",
    "    # MAMI Dataset Experiments\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"MAMI DATASET EXPERIMENTS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # MAMI Binary Classification Experiments\n",
    "    for feature_type in feature_types:\n",
    "        all_results['MAMI'][feature_type] = {}\n",
    "        for ablation_method in ablation_methods:\n",
    "            print(f\"\\n{'='*20} MAMI Binary: {feature_type.capitalize()} with {ablation_method} {'='*20}\")\n",
    "            results = run_coarse_binary_ablation_experiment(\n",
    "                mami_training_df, mami_test_df, feature_type, ablation_method, \"MAMI\"\n",
    "            )\n",
    "            all_results['MAMI'][feature_type][ablation_method] = results\n",
    "    \n",
    "    # EXIST2024 Dataset Experiments\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"EXIST DATASET EXPERIMENTS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # EXIST2024 Binary Classification Experiments\n",
    "    for feature_type in feature_types:\n",
    "        all_results['EXIST2024'][feature_type] = {}\n",
    "        for ablation_method in ablation_methods:\n",
    "            print(f\"\\n{'='*20} EXIST2024 Binary: {feature_type.capitalize()} with {ablation_method} {'='*20}\")\n",
    "            results = run_coarse_binary_ablation_experiment(\n",
    "                exist_training_df, exist_test_df, feature_type, ablation_method, \n",
    "                \"EXIST2024\", binary_label='sexist'\n",
    "            )\n",
    "            all_results['EXIST2024'][feature_type][ablation_method] = results\n",
    "    \n",
    "\n",
    "\n",
    "# Call the function to run all experiments\n",
    "all_results = run_binary_coarse_grained_experiments_without_baseline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3e25c2-b856-407e-b4eb-24d4759008db",
   "metadata": {},
   "source": [
    "#### Binary ablation_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ccaae2-3915-4d05-8565-914fb2a7f43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_binary_category_features(text, category, nrc_lexicon, hurtlex_dict, function_words_dict):\n",
    "    \"\"\"Extract features for binary ablation categories from text.\"\"\"\n",
    "    \n",
    "    if category == \"sentiment_pos\":\n",
    "        return extract_sentiment_words(text, nrc_lexicon, 'positive')\n",
    "    elif category == \"sentiment_neg\":\n",
    "        return extract_sentiment_words(text, nrc_lexicon, 'negative')\n",
    "    elif category == \"hate\":\n",
    "        # Extract all hate speech terms (any category)\n",
    "        hate_words = []\n",
    "        for word in text.lower().split():\n",
    "            if word in hurtlex_dict:\n",
    "                hate_words.append(word)\n",
    "        return list(set(hate_words))\n",
    "    elif category == \"function\":\n",
    "        # Extract all function words from all categories\n",
    "        function_words = []\n",
    "        for func_category in function_words_dict.keys():\n",
    "            category_words = extract_function_words(text, function_words_dict, func_category)\n",
    "            function_words.extend(category_words)\n",
    "        return list(set(function_words))\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "def calculate_binary_ablation_statistics(dataset_df, feature_categories, dataset_name):\n",
    "    \"\"\"\n",
    "    Calculate statistics for binary ablation categories.\n",
    "    Counts features WITHOUT artificial deduplication.\n",
    "    Combined feature count should be the actual count from 'svm representation' text.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset_df : DataFrame\n",
    "        Dataset containing meme data\n",
    "    feature_categories : list\n",
    "        List of binary feature categories ['sentiment_pos', 'sentiment_neg', 'hate', 'function']\n",
    "    dataset_name : str\n",
    "        Name of dataset ('MAMI' or 'EXIST2024')\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Statistics for each binary feature category\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"ðŸ” CALCULATING BINARY ABLATION STATISTICS FOR {dataset_name}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Load linguistic resources\n",
    "    nrc_lexicon = load_nrc_lexicon(\"NRC-Emotion-Lexicon/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\")\n",
    "    hurtlex_dict = load_hurtlex(\"hurtlex-master/lexica/EN/1.2/hurtlex_EN.tsv\")\n",
    "    function_words_dict = load_function_words()\n",
    "    \n",
    "    results = {}\n",
    "\n",
    "    for category in feature_categories:\n",
    "        print(f\"\\nðŸ“Š ANALYZING: {category.upper()}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Initialize counters\n",
    "        stats = {\n",
    "            'total_documents': len(dataset_df),\n",
    "            'documents_with_features': 0,\n",
    "            'features_in_text_only': 0,\n",
    "            'features_in_caption_only': 0,\n",
    "            'features_in_both': 0,\n",
    "            'features_in_neither': 0,\n",
    "            'text_feature_count': 0,\n",
    "            'caption_feature_count': 0,\n",
    "            'combined_feature_count': 0,\n",
    "            'overlapping_instances_count': 0,  # Documents where same word appears in both text and caption\n",
    "            'unique_feature_words': set(),\n",
    "            'coverage_percentage': 0.0,\n",
    "            'unique_feature_count': 0,\n",
    "            'total_overlapping_features': 0  # Total count of overlapping feature instances\n",
    "        }\n",
    "        \n",
    "        for idx, row in dataset_df.iterrows():\n",
    "            # Get text components\n",
    "            meme_text = str(row.get('meme text', '')).lower()\n",
    "            meme_caption = str(row.get('meme caption', '')).lower()\n",
    "            combined_text = str(row.get('svm representation', '')).lower()\n",
    "\n",
    "            # Extract features from each component\n",
    "            text_features = extract_binary_category_features(meme_text, category, nrc_lexicon, hurtlex_dict, function_words_dict)\n",
    "            caption_features = extract_binary_category_features(meme_caption, category, nrc_lexicon, hurtlex_dict, function_words_dict)\n",
    "            combined_features = extract_binary_category_features(combined_text, category, nrc_lexicon, hurtlex_dict, function_words_dict)\n",
    "            \n",
    "            # Update unique words (from all sources)\n",
    "            stats['unique_feature_words'].update(text_features)\n",
    "            stats['unique_feature_words'].update(caption_features)\n",
    "            stats['unique_feature_words'].update(combined_features)\n",
    "            \n",
    "            # Count occurrences - these are RAW COUNTS, no deduplication\n",
    "            text_count = len(text_features)\n",
    "            caption_count = len(caption_features)\n",
    "            combined_count = len(combined_features)  # This is the ACTUAL count from svm representation\n",
    "            \n",
    "            stats['text_feature_count'] += text_count\n",
    "            stats['caption_feature_count'] += caption_count\n",
    "            stats['combined_feature_count'] += combined_count\n",
    "            \n",
    "            # Check for overlapping features (for analysis purposes only)\n",
    "            text_set = set(text_features)\n",
    "            caption_set = set(caption_features)\n",
    "            overlapping_words = text_set.intersection(caption_set)\n",
    "            \n",
    "            if overlapping_words:\n",
    "                stats['overlapping_instances_count'] += 1\n",
    "                # Count how many times overlapping words appear\n",
    "                overlap_count = 0\n",
    "                for word in overlapping_words:\n",
    "                    overlap_count += text_features.count(word)\n",
    "                    overlap_count += caption_features.count(word)\n",
    "                stats['total_overlapping_features'] += overlap_count\n",
    "\n",
    "            # Categorize documents based on where features appear\n",
    "            has_text_features = text_count > 0\n",
    "            has_caption_features = caption_count > 0\n",
    "            \n",
    "            if has_text_features or has_caption_features:\n",
    "                stats['documents_with_features'] += 1\n",
    "                \n",
    "            if has_text_features and has_caption_features:\n",
    "                stats['features_in_both'] += 1\n",
    "            elif has_text_features and not has_caption_features:\n",
    "                stats['features_in_text_only'] += 1\n",
    "            elif not has_text_features and has_caption_features:\n",
    "                stats['features_in_caption_only'] += 1\n",
    "            else:\n",
    "                stats['features_in_neither'] += 1\n",
    "        \n",
    "        # Calculate final statistics\n",
    "        stats['coverage_percentage'] = (stats['documents_with_features'] / stats['total_documents']) * 100\n",
    "        stats['unique_feature_count'] = len(stats['unique_feature_words'])\n",
    "        stats['unique_feature_words'] = list(stats['unique_feature_words'])  # Convert to list for JSON\n",
    "        \n",
    "        # Display results with proper explanation\n",
    "        print(f\"ðŸ“‹ DOCUMENT COVERAGE:\")\n",
    "        print(f\"   â€¢ Total documents: {stats['total_documents']}\")\n",
    "        print(f\"   â€¢ Documents with features: {stats['documents_with_features']} ({stats['coverage_percentage']:.1f}%)\")\n",
    "        print(f\"   â€¢ Features in text only: {stats['features_in_text_only']}\")\n",
    "        print(f\"   â€¢ Features in caption only: {stats['features_in_caption_only']}\")\n",
    "        print(f\"   â€¢ Features in both: {stats['features_in_both']}\")\n",
    "        print(f\"   â€¢ Features in neither: {stats['features_in_neither']}\")\n",
    "        \n",
    "        print(f\"\\nðŸ“ FEATURE WORD COUNTS (RAW COUNTS):\")\n",
    "        print(f\"   â€¢ Text feature occurrences: {stats['text_feature_count']}\")\n",
    "        print(f\"   â€¢ Caption feature occurrences: {stats['caption_feature_count']}\")\n",
    "        print(f\"   â€¢ Combined feature occurrences: {stats['combined_feature_count']}\")\n",
    "        print(f\"   â€¢ Unique feature words: {stats['unique_feature_count']}\")\n",
    "        \n",
    "        # Analyze the relationship between text+caption and combined\n",
    "        expected_sum = stats['text_feature_count'] + stats['caption_feature_count']\n",
    "        actual_difference = expected_sum - stats['combined_feature_count']\n",
    "        \n",
    "        print(f\"\\nðŸ“Š COUNTING ANALYSIS:\")\n",
    "        print(f\"   â€¢ Text + Caption sum: {stats['text_feature_count']} + {stats['caption_feature_count']} = {expected_sum}\")\n",
    "        print(f\"   â€¢ Actual combined count: {stats['combined_feature_count']}\")\n",
    "        \n",
    "        if actual_difference > 0:\n",
    "            print(f\"   â€¢ Difference: -{actual_difference} (combined has fewer)\")\n",
    "            print(f\"   â€¢ Reason: Likely overlapping words between text and caption\")\n",
    "            print(f\"   â€¢ Documents with overlaps: {stats['overlapping_instances_count']}\")\n",
    "        elif actual_difference < 0:\n",
    "            print(f\"   â€¢ Difference: +{abs(actual_difference)} (combined has more)\")\n",
    "            print(f\"   â€¢ Reason: 'svm representation' may have additional processing/words\")\n",
    "        else:\n",
    "            print(f\"   â€¢ Perfect match: Text + Caption = Combined\")\n",
    "            print(f\"   â€¢ This indicates no overlapping words between text and caption\")\n",
    "        \n",
    "        # Show sample words\n",
    "        if len(stats['unique_feature_words']) > 0:\n",
    "            sample_words = stats['unique_feature_words'][:10]\n",
    "            print(f\"   â€¢ Sample words: {', '.join(sample_words)}\")\n",
    "        \n",
    "        results[category] = stats\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def analyze_binary_text_caption_overlap(dataset_df, feature_categories, dataset_name, sample_size=3):\n",
    "    \"\"\"\n",
    "    Analyze and show examples of text-caption overlap for binary ablation features.\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸ” ANALYZING TEXT-CAPTION OVERLAP FOR {dataset_name} - BINARY CATEGORIES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load linguistic resources\n",
    "    nrc_lexicon = load_nrc_lexicon(\"NRC-Emotion-Lexicon/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\")\n",
    "    hurtlex_dict = load_hurtlex(\"hurtlex-master/lexica/EN/1.2/hurtlex_EN.tsv\")\n",
    "    function_words_dict = load_function_words()\n",
    "    \n",
    "    for category in feature_categories:\n",
    "        print(f\"\\nðŸ“Š {category.upper()} OVERLAP ANALYSIS:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        overlap_examples = []\n",
    "        total_overlaps = 0\n",
    "        \n",
    "        for idx, row in dataset_df.iterrows():\n",
    "            meme_text = str(row.get('meme text', '')).lower()\n",
    "            meme_caption = str(row.get('meme caption', '')).lower()\n",
    "            \n",
    "            text_features = extract_binary_category_features(meme_text, category, nrc_lexicon, hurtlex_dict, function_words_dict)\n",
    "            caption_features = extract_binary_category_features(meme_caption, category, nrc_lexicon, hurtlex_dict, function_words_dict)\n",
    "            \n",
    "            text_set = set(text_features)\n",
    "            caption_set = set(caption_features)\n",
    "            overlapping_words = text_set.intersection(caption_set)\n",
    "            \n",
    "            if overlapping_words:\n",
    "                total_overlaps += 1\n",
    "                \n",
    "                if len(overlap_examples) < sample_size:\n",
    "                    overlap_examples.append({\n",
    "                        'meme_id': row.get('meme id', idx),\n",
    "                        'text': meme_text[:100] + \"...\" if len(meme_text) > 100 else meme_text,\n",
    "                        'caption': meme_caption[:100] + \"...\" if len(meme_caption) > 100 else meme_caption,\n",
    "                        'overlapping_words': list(overlapping_words),\n",
    "                        'text_features': text_features,\n",
    "                        'caption_features': caption_features\n",
    "                    })\n",
    "        \n",
    "        print(f\"   â€¢ Total documents with overlapping features: {total_overlaps}\")\n",
    "        print(f\"   â€¢ Percentage of dataset: {(total_overlaps/len(dataset_df))*100:.1f}%\")\n",
    "        \n",
    "        if overlap_examples:\n",
    "            print(f\"\\n   ðŸ“ EXAMPLE OVERLAPS:\")\n",
    "            for i, example in enumerate(overlap_examples, 1):\n",
    "                print(f\"\\n   Example {i} (ID: {example['meme_id']}):\") \n",
    "                print(f\"     Text: \\\"{example['text']}\\\"\")\n",
    "                print(f\"     Caption: \\\"{example['caption']}\\\"\")\n",
    "                print(f\"     Overlapping words: {example['overlapping_words']}\")\n",
    "                print(f\"     Text features: {example['text_features']}\")\n",
    "                print(f\"     Caption features: {example['caption_features']}\")\n",
    "        else:\n",
    "            print(f\"   â€¢ No overlapping features found for {category}\")\n",
    "\n",
    "\n",
    "def combine_binary_train_test_stats(train_stats, test_stats, dataset_name):\n",
    "    \"\"\"Combine training and test statistics for binary ablation.\"\"\"\n",
    "    \n",
    "    print(f\"\\nðŸ”— COMBINING TRAIN AND TEST STATS FOR {dataset_name} - BINARY ABLATION\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    combined = {}\n",
    "    \n",
    "    for category in train_stats.keys():\n",
    "        if category in test_stats:\n",
    "            train = train_stats[category]\n",
    "            test = test_stats[category]\n",
    "            \n",
    "            # Combine counts\n",
    "            combined[category] = {\n",
    "                'total_documents': train['total_documents'] + test['total_documents'],\n",
    "                'documents_with_features': train['documents_with_features'] + test['documents_with_features'],\n",
    "                'features_in_text_only': train['features_in_text_only'] + test['features_in_text_only'],\n",
    "                'features_in_caption_only': train['features_in_caption_only'] + test['features_in_caption_only'],\n",
    "                'features_in_both': train['features_in_both'] + test['features_in_both'],\n",
    "                'features_in_neither': train['features_in_neither'] + test['features_in_neither'],\n",
    "                'text_feature_count': train['text_feature_count'] + test['text_feature_count'],\n",
    "                'caption_feature_count': train['caption_feature_count'] + test['caption_feature_count'],\n",
    "                'combined_feature_count': train['combined_feature_count'] + test['combined_feature_count'],\n",
    "                'overlapping_instances_count': train['overlapping_instances_count'] + test['overlapping_instances_count'],\n",
    "                'total_overlapping_features': train['total_overlapping_features'] + test['total_overlapping_features'],\n",
    "                \n",
    "                # Combine unique words\n",
    "                'unique_feature_words': list(set(train['unique_feature_words'] + test['unique_feature_words'])),\n",
    "            }\n",
    "            \n",
    "            # Recalculate percentages\n",
    "            total_docs = combined[category]['total_documents']\n",
    "            docs_with_features = combined[category]['documents_with_features']\n",
    "            combined[category]['coverage_percentage'] = (docs_with_features / total_docs) * 100\n",
    "            combined[category]['unique_feature_count'] = len(combined[category]['unique_feature_words'])\n",
    "            \n",
    "            print(f\"   {category}: {combined[category]['coverage_percentage']:.1f}% coverage ({docs_with_features}/{total_docs})\")\n",
    "    \n",
    "    return combined\n",
    "\n",
    "\n",
    "def generate_binary_comparison_report(mami_stats, exist_stats):\n",
    "    \"\"\"Generate comparison report for binary ablation statistics.\"\"\"\n",
    "    \n",
    "    print(f\"\\nðŸ“Š BINARY ABLATION COMPARATIVE ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    categories = ['sentiment_pos', 'sentiment_neg', 'hate', 'function']\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ MAMI BINARY FEATURES (Combined Train+Test):\")\n",
    "    for category in categories:\n",
    "        if category in mami_stats:\n",
    "            stats = mami_stats[category]\n",
    "            expected_sum = stats['text_feature_count'] + stats['caption_feature_count']\n",
    "            actual_combined = stats['combined_feature_count']\n",
    "            difference = expected_sum - actual_combined\n",
    "            \n",
    "            print(f\"   â€¢ {category}:\")\n",
    "            print(f\"     - Coverage: {stats['coverage_percentage']:.1f}% ({stats['documents_with_features']}/{stats['total_documents']})\")\n",
    "            print(f\"     - Text: {stats['text_feature_count']}, Caption: {stats['caption_feature_count']}, Combined: {actual_combined}\")\n",
    "            print(f\"     - Difference: {difference} ({'overlap' if difference > 0 else 'additional features' if difference < 0 else 'perfect match'})\")\n",
    "            print(f\"     - Overlapping instances: {stats.get('overlapping_instances_count', 0)} documents\")\n",
    "            print(f\"     - Unique words: {stats['unique_feature_count']}\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ EXIST2024 BINARY FEATURES (Combined Train+Test):\")\n",
    "    for category in categories:\n",
    "        if category in exist_stats:\n",
    "            stats = exist_stats[category]\n",
    "            expected_sum = stats['text_feature_count'] + stats['caption_feature_count']\n",
    "            actual_combined = stats['combined_feature_count']\n",
    "            difference = expected_sum - actual_combined\n",
    "            \n",
    "            print(f\"   â€¢ {category}:\")\n",
    "            print(f\"     - Coverage: {stats['coverage_percentage']:.1f}% ({stats['documents_with_features']}/{stats['total_documents']})\")\n",
    "            print(f\"     - Text: {stats['text_feature_count']}, Caption: {stats['caption_feature_count']}, Combined: {actual_combined}\")\n",
    "            print(f\"     - Difference: {difference} ({'overlap' if difference > 0 else 'additional features' if difference < 0 else 'perfect match'})\")\n",
    "            print(f\"     - Overlapping instances: {stats.get('overlapping_instances_count', 0)} documents\")\n",
    "            print(f\"     - Unique words: {stats['unique_feature_count']}\")\n",
    "    \n",
    "    # Cross-dataset comparison\n",
    "    print(f\"\\nðŸ” CROSS-DATASET COMPARISON:\")\n",
    "    print(f\"{'Category':<15} {'MAMI Coverage':<15} {'EXIST Coverage':<15} {'MAMI Words':<12} {'EXIST Words':<12}\")\n",
    "    print(\"-\" * 75)\n",
    "    \n",
    "    for category in categories:\n",
    "        mami_coverage = mami_stats[category]['coverage_percentage'] if category in mami_stats else 0\n",
    "        exist_coverage = exist_stats[category]['coverage_percentage'] if category in exist_stats else 0\n",
    "        mami_words = mami_stats[category]['unique_feature_count'] if category in mami_stats else 0\n",
    "        exist_words = exist_stats[category]['unique_feature_count'] if category in exist_stats else 0\n",
    "        \n",
    "        print(f\"{category:<15} {mami_coverage:<15.1f} {exist_coverage:<15.1f} {mami_words:<12} {exist_words:<12}\")\n",
    "    \n",
    "    print(f\"\\nðŸ” KEY INSIGHTS:\")\n",
    "    print(\"   â€¢ Combined counts reflect actual occurrences in 'svm representation' text\")\n",
    "    print(\"   â€¢ Differences indicate natural overlap between text and caption content\")\n",
    "    print(\"   â€¢ This analysis helps understand feature distribution patterns for binary ablation\")\n",
    "    print(\"   â€¢ No artificial deduplication applied - raw occurrence counts maintained\")\n",
    "    print(\"   â€¢ Coverage percentages show how widespread each feature type is in the datasets\")\n",
    "\n",
    "\n",
    "def run_binary_ablation_statistics_analysis():\n",
    "    \"\"\"\n",
    "    Run comprehensive binary ablation statistics analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ðŸš€ BINARY ABLATION STATISTICS ANALYSIS\")\n",
    "    print(\"=\" * 90)\n",
    "    print(\"Analyzing four main binary ablation categories for both datasets\")\n",
    "    print(\"Categories: sentiment_pos, sentiment_neg, hate, function\")\n",
    "    print()\n",
    "    \n",
    "    # Define binary ablation categories\n",
    "    binary_categories = ['sentiment_pos', 'sentiment_neg', 'hate', 'function']\n",
    "    \n",
    "    # Run analysis for MAMI\n",
    "    print(\"ðŸ” MAMI DATASET ANALYSIS\")\n",
    "    print(\"Training set:\")\n",
    "    mami_training_stats = calculate_binary_ablation_statistics(mami_training_df, binary_categories, \"MAMI\")\n",
    "    print(\"\\nTest set:\")\n",
    "    mami_test_stats = calculate_binary_ablation_statistics(mami_test_df, binary_categories, \"MAMI\")\n",
    "    \n",
    "    print(\"\\n\\nðŸ” EXIST2024 DATASET ANALYSIS\") \n",
    "    print(\"Training set:\")\n",
    "    exist_training_stats = calculate_binary_ablation_statistics(exist_training_df, binary_categories, \"EXIST2024\")\n",
    "    print(\"\\nTest set:\")\n",
    "    exist_test_stats = calculate_binary_ablation_statistics(exist_test_df, binary_categories, \"EXIST2024\")\n",
    "    \n",
    "    # Combine stats for overall analysis\n",
    "    mami_combined_stats = combine_binary_train_test_stats(mami_training_stats, mami_test_stats, \"MAMI\")\n",
    "    exist_combined_stats = combine_binary_train_test_stats(exist_training_stats, exist_test_stats, \"EXIST2024\")\n",
    "    \n",
    "    # Analyze text-caption overlap for both datasets\n",
    "    print(f\"\\nðŸ” TEXT-CAPTION OVERLAP ANALYSIS:\")\n",
    "    analyze_binary_text_caption_overlap(mami_test_df, binary_categories, \"MAMI\", sample_size=3)\n",
    "    analyze_binary_text_caption_overlap(exist_test_df, binary_categories, \"EXIST2024\", sample_size=3)\n",
    "    \n",
    "    # Save results\n",
    "    os.makedirs(\"evaluation/binary_ablation_analysis\", exist_ok=True)\n",
    "    \n",
    "    # Save detailed results\n",
    "    detailed_results = {\n",
    "        'MAMI': {\n",
    "            'training': mami_training_stats,\n",
    "            'test': mami_test_stats,\n",
    "            'combined': mami_combined_stats\n",
    "        },\n",
    "        'EXIST2024': {\n",
    "            'training': exist_training_stats,\n",
    "            'test': exist_test_stats,\n",
    "            'combined': exist_combined_stats\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(\"evaluation/binary_ablation_analysis/binary_ablation_stats.json\", 'w') as f:\n",
    "        json.dump(detailed_results, f, indent=2, cls=NumpyEncoder)\n",
    "    \n",
    "    print(f\"\\nâœ… Results saved to evaluation/binary_ablation_analysis/binary_ablation_stats.json\")\n",
    "    \n",
    "    # Generate comparison report\n",
    "    generate_binary_comparison_report(mami_combined_stats, exist_combined_stats)\n",
    "    \n",
    "    # Generate category-specific insights\n",
    "    print(f\"\\nðŸ“Š CATEGORY-SPECIFIC INSIGHTS:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for category in binary_categories:\n",
    "        print(f\"\\nðŸŽ¯ {category.upper()}:\")\n",
    "        \n",
    "        # Compare coverage across datasets\n",
    "        mami_coverage = mami_combined_stats[category]['coverage_percentage']\n",
    "        exist_coverage = exist_combined_stats[category]['coverage_percentage']\n",
    "        \n",
    "        print(f\"   â€¢ Coverage: MAMI {mami_coverage:.1f}% vs EXIST2024 {exist_coverage:.1f}%\")\n",
    "        \n",
    "        # Compare unique word counts\n",
    "        mami_words = mami_combined_stats[category]['unique_feature_count']\n",
    "        exist_words = exist_combined_stats[category]['unique_feature_count']\n",
    "        \n",
    "        print(f\"   â€¢ Unique words: MAMI {mami_words} vs EXIST2024 {exist_words}\")\n",
    "        \n",
    "        # Compare overlap patterns\n",
    "        mami_overlaps = mami_combined_stats[category]['overlapping_instances_count']\n",
    "        exist_overlaps = exist_combined_stats[category]['overlapping_instances_count']\n",
    "        mami_total = mami_combined_stats[category]['total_documents']\n",
    "        exist_total = exist_combined_stats[category]['total_documents']\n",
    "        \n",
    "        print(f\"   â€¢ Text-caption overlaps: MAMI {mami_overlaps}/{mami_total} ({mami_overlaps/mami_total*100:.1f}%) vs EXIST2024 {exist_overlaps}/{exist_total} ({exist_overlaps/exist_total*100:.1f}%)\")\n",
    "        \n",
    "        # Sample words comparison\n",
    "        mami_sample = mami_combined_stats[category]['unique_feature_words'][:5]\n",
    "        exist_sample = exist_combined_stats[category]['unique_feature_words'][:5]\n",
    "        \n",
    "        print(f\"   â€¢ Sample words MAMI: {', '.join(mami_sample)}\")\n",
    "        print(f\"   â€¢ Sample words EXIST: {', '.join(exist_sample)}\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ‰ BINARY ABLATION STATISTICS ANALYSIS COMPLETED!\")\n",
    "    print(\"ðŸ“ Results saved with proper feature counting (no artificial deduplication)\")\n",
    "    print(\"ðŸ” This analysis provides insights into feature distribution for binary ablation experiments\")\n",
    "    \n",
    "    return mami_combined_stats, exist_combined_stats\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        print(\"ðŸš€ Running binary ablation statistics analysis...\")\n",
    "        mami_results, exist_results = run_binary_ablation_statistics_analysis()\n",
    "        \n",
    "        print(f\"\\nðŸŽ‰ ANALYSIS COMPLETE!\")\n",
    "        print(\"ðŸ“ Results saved with comprehensive statistics for binary ablation categories\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print(\"Make sure all required functions and datasets are loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bed06a3-47ed-4275-a725-6c250b8fb563",
   "metadata": {},
   "source": [
    "## Multilabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1970ec5c-509a-4e12-a3aa-6bd0fd8adff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_coarse_grained_multilabel_ablation_experiments(dataset_df_train, dataset_df_test, feature_type, \n",
    "                                                       ablation_method, dataset_name, binary_label, fine_grained_labels):\n",
    "    \"\"\"\n",
    "    Run multilabel ablation experiment for coarse-grained feature categories.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset_df_train : DataFrame\n",
    "        Training dataset\n",
    "    dataset_df_test : DataFrame\n",
    "        Test dataset\n",
    "    feature_type : str\n",
    "        Type of feature to ablate:\n",
    "        - 'sentiment_pos': Positive sentiment words\n",
    "        - 'sentiment_neg': Negative sentiment words\n",
    "        - 'hate': All hate speech terms\n",
    "        - 'function': All function words\n",
    "    ablation_method : str\n",
    "        Method for ablation ('mask' or 'remove')\n",
    "    dataset_name : str\n",
    "        Name of the dataset ('MAMI' or 'EXIST2024')\n",
    "    binary_label : str\n",
    "        Name of the binary label column\n",
    "    fine_grained_labels : list\n",
    "        List of fine-grained category label names\n",
    "    \"\"\"\n",
    "    # Load necessary lexicons\n",
    "    nrc_lexicon = load_nrc_lexicon(\"NRC-Emotion-Lexicon/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\")\n",
    "    function_words_dict = load_function_words()\n",
    "    hurtlex_dict = load_hurtlex(\"hurtlex-master/lexica/EN/1.2/hurtlex_EN.tsv\")\n",
    "    \n",
    "    # Get original texts\n",
    "    X_train = dataset_df_train[\"svm representation\"].tolist()\n",
    "    y_train_binary = dataset_df_train[binary_label].tolist()\n",
    "    train_df_bin_positive = dataset_df_train.loc[dataset_df_train[binary_label] == 1]\n",
    "    X_train_bin_positive = train_df_bin_positive[\"svm representation\"].tolist()\n",
    "    y_train_categories = train_df_bin_positive[fine_grained_labels]\n",
    "    \n",
    "    X_test = dataset_df_test[\"svm representation\"].tolist()\n",
    "    \n",
    "    # Prepare complete label list (binary + fine-grained)\n",
    "    all_labels = [binary_label] + fine_grained_labels\n",
    "    y_test_all = dataset_df_test[all_labels]\n",
    "    \n",
    "    # Process texts based on feature type and ablation method\n",
    "    X_train_ablated = []\n",
    "    X_train_bin_pos_ablated = []\n",
    "    X_test_ablated = []\n",
    "    \n",
    "    # Create display name and model name for feature type\n",
    "    display_name = f\"Coarse-Grained: {feature_type.replace('_', ' ').title()}\"\n",
    "    feature_name_for_model = feature_type\n",
    "    \n",
    "    print(f\"Running multilabel coarse-grained ablation experiment for {dataset_name} - {display_name} with {ablation_method} method...\")\n",
    "    \n",
    "    # Statistics tracking\n",
    "    total_feature_words_processed = 0\n",
    "    documents_with_features = 0\n",
    "    feature_words_per_doc = []\n",
    "    \n",
    "    # Define function to extract coarse-grained features\n",
    "    def extract_coarse_grained_features(text):\n",
    "        \"\"\"Helper function to extract coarse-grained feature words\"\"\"\n",
    "        if feature_type == 'sentiment_pos':\n",
    "            # Extract positive sentiment words\n",
    "            return extract_sentiment_words(text, nrc_lexicon, 'positive')\n",
    "        elif feature_type == 'sentiment_neg':\n",
    "            # Extract negative sentiment words  \n",
    "            return extract_sentiment_words(text, nrc_lexicon, 'negative')\n",
    "        elif feature_type == 'hate':\n",
    "            # Extract all hate speech terms (any category)\n",
    "            hate_words = []\n",
    "            for word in text.lower().split():\n",
    "                if word in hurtlex_dict:\n",
    "                    hate_words.append(word)\n",
    "            return list(set(hate_words))\n",
    "        elif feature_type == 'function':\n",
    "            # Extract all function words from all categories\n",
    "            function_words = []\n",
    "            for category in function_words_dict.keys():\n",
    "                category_words = extract_function_words(text, function_words_dict, category)\n",
    "                function_words.extend(category_words)\n",
    "            return list(set(function_words))\n",
    "        else:\n",
    "            print(f\"Warning: Unknown coarse-grained feature type '{feature_type}'. No features will be ablated.\")\n",
    "            return []\n",
    "    \n",
    "    # Process all training data\n",
    "    for text in X_train:\n",
    "        feature_words = extract_coarse_grained_features(text)\n",
    "        \n",
    "        # Convert to set for faster lookup\n",
    "        feature_words_set = set(feature_words)\n",
    "        \n",
    "        # Count feature words in this text\n",
    "        num_feature_words = len([word for word in text.lower().split() if word in feature_words_set])\n",
    "        feature_words_per_doc.append(num_feature_words)\n",
    "        \n",
    "        if num_feature_words > 0:\n",
    "            documents_with_features += 1\n",
    "        \n",
    "        # Process with feature ablation\n",
    "        if ablation_method == 'mask':\n",
    "            ablated_text, count = replace_mask_features(text, feature_words, mask_token='UNK')\n",
    "        elif ablation_method == 'remove':\n",
    "            ablated_text, count = remove_features(text, feature_words)\n",
    "        \n",
    "        X_train_ablated.append(ablated_text)\n",
    "        total_feature_words_processed += count\n",
    "    \n",
    "    # Process positive-only training data\n",
    "    for text in X_train_bin_positive:\n",
    "        feature_words = extract_coarse_grained_features(text)\n",
    "        \n",
    "        # Process with feature ablation\n",
    "        if ablation_method == 'mask':\n",
    "            ablated_text, _ = replace_mask_features(text, feature_words, mask_token='UNK')\n",
    "        elif ablation_method == 'remove':\n",
    "            ablated_text, _ = remove_features(text, feature_words)\n",
    "        \n",
    "        X_train_bin_pos_ablated.append(ablated_text)\n",
    "    \n",
    "    # Process test data\n",
    "    for text in X_test:\n",
    "        feature_words = extract_coarse_grained_features(text)\n",
    "        \n",
    "        # Process with feature ablation\n",
    "        if ablation_method == 'mask':\n",
    "            ablated_text, _ = replace_mask_features(text, feature_words, mask_token='UNK')\n",
    "        elif ablation_method == 'remove':\n",
    "            ablated_text, _ = remove_features(text, feature_words)\n",
    "        \n",
    "        X_test_ablated.append(ablated_text)\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\n{'=' * 50}\")\n",
    "    print(f\"Processing Summary:\")\n",
    "    print(f\"Total documents: {len(X_train_ablated)}\")\n",
    "    print(f\"Documents with features: {documents_with_features} ({documents_with_features/len(X_train_ablated)*100:.2f}%)\")\n",
    "    print(f\"Total feature words processed: {total_feature_words_processed}\")\n",
    "    print(f\"Average feature words per document: {np.mean(feature_words_per_doc):.2f}\")\n",
    "    print(f\"Max words per document: {max(feature_words_per_doc) if feature_words_per_doc else 0}\")\n",
    "    print(f\"{'=' * 50}\")\n",
    "    \n",
    "    # Create model name\n",
    "    model_name = f\"{dataset_name}_svm_ablation_{feature_name_for_model}_{ablation_method}_hierarchy\"\n",
    "    \n",
    "    try:\n",
    "        # Train and evaluate model\n",
    "        print(f\"\\nTraining model with ablated {display_name}...\")\n",
    "        test_pred_df, bin_clf, bin_vec, ml_model, ml_vec = build_hierarchical_multilabel_classifier(\n",
    "            X_train_ablated,\n",
    "            y_train_binary,\n",
    "            X_train_bin_pos_ablated,\n",
    "            y_train_categories,\n",
    "            X_test_ablated,\n",
    "            binary_label,\n",
    "            fine_grained_labels\n",
    "        )\n",
    "        \n",
    "        # Get gold file paths based on dataset\n",
    "        dataset_path_name = \"EXIST2024\" if dataset_name == \"EXIST\" else dataset_name\n",
    "        gold_test_ml = f\"models/evaluation/golds/{dataset_path_name}/{dataset_path_name}_test_hierarchical.json\"\n",
    "        gold_test_txt = f'models/evaluation/golds/{dataset_path_name}/{dataset_path_name}_test_truth.txt'\n",
    "        evaluation_type = \"hierarchical\"\n",
    "        \n",
    "        # Create file with predictions\n",
    "        print(f\"\\nSaving evaluation results to: {model_name}\")\n",
    "        test_pred_json_ml, test_pred_txt_ml = save_evaluation(\n",
    "            dataset_df_test, \"evaluation/predictions\", dataset_name, \"test\", \n",
    "            evaluation_type, model_name, test_pred_df, binary_label, all_labels\n",
    "        )\n",
    "        \n",
    "        # Get evaluation metrics\n",
    "        print(f\"\\n{'='*20} Evaluation Results for {display_name} {'='*20}\")\n",
    "        metrics = evaluate_multilabel_classification(\n",
    "            gold_test_ml, test_pred_json_ml,\n",
    "            y_test_all, test_pred_df.to_numpy(),\n",
    "            gold_test_txt, test_pred_txt_ml,\n",
    "            all_labels, hierarchy=True\n",
    "        )\n",
    "\n",
    "        # âœ… ADD PREDICTIONS TO RESULTS\n",
    "        metrics['predictions'] = test_pred_df.to_numpy().tolist()  \n",
    "        metrics['true_labels'] = y_test_all.values.tolist()  \n",
    "        metrics['prediction_files'] = {\n",
    "            'json': test_pred_json_ml,\n",
    "            'txt': test_pred_txt_ml\n",
    "        }\n",
    "        metrics['all_labels'] = all_labels\n",
    "\n",
    "        os.makedirs(\"evaluation/results/multi-label/SVM/coarse-grained\", exist_ok=True)\n",
    "        results_file = f\"evaluation/results/multi-label/SVM/coarse-grained/{model_name}_results.json\"\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(metrics, f, indent=2, cls=NumpyEncoder)\n",
    "        print(f\"âœ… Results saved to: {results_file}\")\n",
    "        \n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'metrics': metrics,\n",
    "            'ablation_stats': {\n",
    "                'total_documents': len(X_train),\n",
    "                'documents_with_features': documents_with_features,\n",
    "                'percentage_with_features': documents_with_features/len(X_train)*100,\n",
    "                'total_feature_words': total_feature_words_processed,\n",
    "                'avg_feature_words': np.mean(feature_words_per_doc),\n",
    "                'max_feature_words': max(feature_words_per_doc) if feature_words_per_doc else 0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    except ValueError as e:\n",
    "        print(f\"Error in experiment: {e}\")\n",
    "        print(f\"Skipping multilabel coarse-grained ablation for {feature_type} with {ablation_method}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def run_fine_grained_multilabel_ablation_experiments(dataset_df_train, dataset_df_test, feature_type, feature_category, \n",
    "                                                     ablation_method, dataset_name, binary_label, fine_grained_labels):\n",
    "    \"\"\"\n",
    "    Run a complete multi-label ablation experiment for specific feature types.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset_df_train : DataFrame\n",
    "        Training dataset\n",
    "    dataset_df_test : DataFrame\n",
    "        Test dataset\n",
    "    feature_type : str\n",
    "        Type of feature to ablate:\n",
    "        - 'neg_emotion': Negative emotions (fear, anger, sadness, disgust)\n",
    "        - 'function': Function words\n",
    "        - 'hate': Hate speech terms\n",
    "    feature_category : str or list\n",
    "        Specific category within the feature type or list of categories.\n",
    "        For 'neg_emotion', can be individual emotion or None for all negative emotions.\n",
    "        For 'function', should be a specific function word category.\n",
    "        For 'hate', should be a specific hate speech category.\n",
    "    ablation_method : str\n",
    "        Method for ablation ('mask' or 'remove')\n",
    "    dataset_name : str\n",
    "        Name of the dataset ('MAMI' or 'EXIST2024')\n",
    "    binary_label : str\n",
    "        Name of the binary label column\n",
    "    fine_grained_labels : list\n",
    "        List of fine-grained category label names\n",
    "    \"\"\"\n",
    "    # Load necessary lexicons\n",
    "    nrc_lexicon = load_nrc_lexicon(\"NRC-Emotion-Lexicon/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\")\n",
    "    function_words_dict = load_function_words()\n",
    "    hurtlex_dict = load_hurtlex(\"hurtlex-master/lexica/EN/1.2/hurtlex_EN.tsv\")\n",
    "    \n",
    "    # Define negative emotions\n",
    "    negative_emotions = ['fear', 'anger', 'sadness', 'disgust']\n",
    "    \n",
    "    # Get original texts\n",
    "    X_train = dataset_df_train[\"svm representation\"].tolist()\n",
    "    y_train_binary = dataset_df_train[binary_label].tolist()\n",
    "    train_df_bin_positive = dataset_df_train.loc[dataset_df_train[binary_label] == 1]\n",
    "    X_train_bin_positive = train_df_bin_positive[\"svm representation\"].tolist()\n",
    "    y_train_categories = train_df_bin_positive[fine_grained_labels]\n",
    "    \n",
    "    X_test = dataset_df_test[\"svm representation\"].tolist()\n",
    "    \n",
    "    # Prepare complete label list (binary + fine-grained)\n",
    "    all_labels = [binary_label] + fine_grained_labels\n",
    "    y_test_all = dataset_df_test[all_labels]\n",
    "    \n",
    "    # Process texts based on feature type and ablation method\n",
    "    X_train_ablated = []\n",
    "    X_train_bin_pos_ablated = []\n",
    "    X_test_ablated = []\n",
    "    \n",
    "    # Create display name for feature category\n",
    "    if feature_type == 'neg_emotion' and feature_category is None:\n",
    "        display_name = \"All Negative Emotions (fear, anger, sadness, disgust)\"\n",
    "        feature_name_for_model = \"all_neg_emotions\"\n",
    "    elif feature_type == 'neg_emotion':\n",
    "        display_name = f\"Negative Emotion: {feature_category}\"\n",
    "        feature_name_for_model = f\"neg_{feature_category}\"\n",
    "    elif feature_type == 'function':\n",
    "        display_name = f\"Function Words: {feature_category}\"\n",
    "        feature_name_for_model = f\"func_{feature_category}\"\n",
    "    elif feature_type == 'hate':\n",
    "        display_name = f\"Hate Speech: {feature_category}\"\n",
    "        feature_name_for_model = f\"hate_{feature_category}\"\n",
    "    else:\n",
    "        display_name = f\"{feature_type}: {feature_category}\"\n",
    "        feature_name_for_model = f\"{feature_type}_{feature_category}\"\n",
    "    \n",
    "    print(f\"Running multi-label ablation experiment for {dataset_name} - {display_name} with {ablation_method} method...\")\n",
    "    \n",
    "    # Statistics tracking\n",
    "    total_feature_words_processed = 0\n",
    "    documents_with_features = 0\n",
    "    feature_words_per_doc = []\n",
    "    \n",
    "    # Define function to extract targeted features\n",
    "    def extract_targeted_features(text):\n",
    "        \"\"\"Helper function to extract the targeted feature words based on feature type\"\"\"\n",
    "        if feature_type == 'neg_emotion':\n",
    "            if feature_category is None:\n",
    "                # Extract all negative emotions\n",
    "                all_neg_words = []\n",
    "                for emotion in negative_emotions:\n",
    "                    emotion_words = extract_emotion_words(text, nrc_lexicon, emotion)\n",
    "                    all_neg_words.extend(emotion_words)\n",
    "                return list(set(all_neg_words))  # Remove duplicates\n",
    "            else:\n",
    "                # Extract specific negative emotion\n",
    "                return extract_emotion_words(text, nrc_lexicon, feature_category)\n",
    "        elif feature_type == 'function':\n",
    "            return extract_function_words(text, function_words_dict, feature_category)\n",
    "        elif feature_type == 'hate':\n",
    "            return extract_hate_speech_terms(text, hurtlex_dict, feature_category)\n",
    "        else:\n",
    "            print(f\"Warning: Unknown feature type '{feature_type}'. No features will be ablated.\")\n",
    "            return []\n",
    "    \n",
    "    # Process all training data\n",
    "    for text in X_train:\n",
    "        feature_words = extract_targeted_features(text)\n",
    "        \n",
    "        # Convert to set for faster lookup\n",
    "        feature_words_set = set(feature_words)\n",
    "        \n",
    "        # Count feature words in this text\n",
    "        num_feature_words = len([word for word in text.lower().split() if word in feature_words_set])\n",
    "        feature_words_per_doc.append(num_feature_words)\n",
    "        \n",
    "        if num_feature_words > 0:\n",
    "            documents_with_features += 1\n",
    "        \n",
    "        # Process with feature ablation\n",
    "        if ablation_method == 'mask':\n",
    "            ablated_text, count = replace_mask_features(text, feature_words, mask_token='UNK')\n",
    "        elif ablation_method == 'remove':\n",
    "            ablated_text, count = remove_features(text, feature_words)\n",
    "        \n",
    "        X_train_ablated.append(ablated_text)\n",
    "        total_feature_words_processed += count\n",
    "    \n",
    "    # Process positive-only training data\n",
    "    for text in X_train_bin_positive:\n",
    "        feature_words = extract_targeted_features(text)\n",
    "        \n",
    "        # Process with feature ablation\n",
    "        if ablation_method == 'mask':\n",
    "            ablated_text, _ = replace_mask_features(text, feature_words, mask_token='UNK')\n",
    "        elif ablation_method == 'remove':\n",
    "            ablated_text, _ = remove_features(text, feature_words)\n",
    "        \n",
    "        X_train_bin_pos_ablated.append(ablated_text)\n",
    "    \n",
    "    # Process test data\n",
    "    for text in X_test:\n",
    "        feature_words = extract_targeted_features(text)\n",
    "        \n",
    "        # Process with feature ablation\n",
    "        if ablation_method == 'mask':\n",
    "            ablated_text, _ = replace_mask_features(text, feature_words, mask_token='UNK')\n",
    "        elif ablation_method == 'remove':\n",
    "            ablated_text, _ = remove_features(text, feature_words)\n",
    "        \n",
    "        X_test_ablated.append(ablated_text)\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\n{'=' * 50}\")\n",
    "    print(f\"Processing Summary:\")\n",
    "    print(f\"Total documents: {len(X_train_ablated)}\")\n",
    "    print(f\"Documents with features: {documents_with_features} ({documents_with_features/len(X_train_ablated)*100:.2f}%)\")\n",
    "    print(f\"Total feature words processed: {total_feature_words_processed}\")\n",
    "    print(f\"Average feature words per document: {np.mean(feature_words_per_doc):.2f}\")\n",
    "    print(f\"Max words per document: {max(feature_words_per_doc) if feature_words_per_doc else 0}\")\n",
    "    print(f\"{'=' * 50}\")\n",
    "    \n",
    "    # Create model name\n",
    "    model_name = f\"{dataset_name}_svm_ablation_{feature_name_for_model}_{ablation_method}_hierarchy\"\n",
    "    \n",
    "    try:\n",
    "        # Train and evaluate model\n",
    "        print(f\"\\nTraining model with ablated {display_name}...\")\n",
    "        test_pred_df, bin_clf, bin_vec, ml_model, ml_vec = build_hierarchical_multilabel_classifier(\n",
    "            X_train_ablated,\n",
    "            y_train_binary,\n",
    "            X_train_bin_pos_ablated,\n",
    "            y_train_categories,\n",
    "            X_test_ablated,\n",
    "            binary_label,\n",
    "            fine_grained_labels\n",
    "        )\n",
    "        \n",
    "        # Get gold file paths based on dataset\n",
    "        gold_test_ml = f\"models/evaluation/golds/{dataset_path_name}/{dataset_path_name}_test_hierarchical.json\"\n",
    "        gold_test_txt = f'models/evaluation/golds/{dataset_path_name}/{dataset_path_name}_test_truth.txt'\n",
    "        evaluation_type = \"hierarchical\"\n",
    "        \n",
    "        # Create file with predictions\n",
    "        print(f\"\\nSaving evaluation results to: {model_name}\")\n",
    "        test_pred_json_ml, test_pred_txt_ml = save_evaluation(\n",
    "            dataset_df_test, \"evaluation/predictions\", dataset_name, \"test\", \n",
    "            evaluation_type, model_name, test_pred_df, binary_label, all_labels\n",
    "        )\n",
    "        \n",
    "        # Get evaluation metrics\n",
    "        print(f\"\\n{'='*20} Evaluation Results for {display_name} {'='*20}\")\n",
    "        metrics = evaluate_multilabel_classification(\n",
    "            gold_test_ml, test_pred_json_ml,\n",
    "            y_test_all, test_pred_df.to_numpy(),\n",
    "            gold_test_txt, test_pred_txt_ml,\n",
    "            all_labels, hierarchy=True\n",
    "        )\n",
    "\n",
    "        os.makedirs(\"evaluation/results/multi-label/SVM/fine-grained/\", exist_ok=True)\n",
    "        results_file = f\"evaluation/results/multi-label/SVM/fine-grained/{model_name}_results.json\"\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(metrics, f, indent=2, cls=NumpyEncoder)\n",
    "        print(f\"âœ… Results saved to: {results_file}\")\n",
    "        \n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'metrics': metrics,\n",
    "            'ablation_stats': {\n",
    "                'total_documents': len(X_train),\n",
    "                'documents_with_features': documents_with_features,\n",
    "                'percentage_with_features': documents_with_features/len(X_train)*100,\n",
    "                'total_feature_words': total_feature_words_processed,\n",
    "                'avg_feature_words': np.mean(feature_words_per_doc),\n",
    "                'max_feature_words': max(feature_words_per_doc) if feature_words_per_doc else 0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    except ValueError as e:\n",
    "        print(f\"Error in experiment: {e}\")\n",
    "        print(f\"Skipping multi-label ablation for {feature_type} - {feature_category} with {ablation_method}\")\n",
    "        return None\n",
    "\n",
    "    \n",
    "def run_multilabel_experiments(dataset_name=\"MAMI\", ablation_method=\"mask\"):\n",
    "    \"\"\"\n",
    "    Run multi-label ablation experiments including both:\n",
    "    1. Coarse-grained categories (sentiment_pos, sentiment_neg, hate, function)\n",
    "    2. Fine-grained categories (individual emotions, function word categories, hate speech categories)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset_name : str\n",
    "        Name of the dataset ('MAMI' or 'EXIST2024')\n",
    "    ablation_method : str\n",
    "        Method for ablation ('mask' or 'remove')\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"RUNNING MULTI-LABEL ABLATION EXPERIMENTS FOR {dataset_name}\")\n",
    "    print(\"Including both coarse-grained and fine-grained categories\")\n",
    "    print(\"Using ablation method:\", ablation_method)\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Set up dataset and labels based on dataset name\n",
    "    if dataset_name == \"MAMI\":\n",
    "        training_df = mami_training_df\n",
    "        test_df = mami_test_df\n",
    "        binary_label = \"misogynous\"\n",
    "        fine_grained_labels = [\"shaming\", \"stereotype\", \"objectification\", \"violence\"]\n",
    "    else:  # EXIST2024\n",
    "        training_df = exist_training_df\n",
    "        test_df = exist_test_df\n",
    "        binary_label = \"sexist\"\n",
    "        fine_grained_labels = [\"ideological-inequality\", \"stereotyping-dominance\", \"objectification\", \n",
    "                              \"sexual-violence\", \"misogyny-non-sexual-violence\"]\n",
    "    \n",
    "    # Store results\n",
    "    results = {\n",
    "        'coarse_grained': {},\n",
    "        'neg_emotion': {},\n",
    "        'function': {},\n",
    "        'hate': {}\n",
    "    }\n",
    "    \n",
    "    # 1. COARSE-GRAINED EXPERIMENTS (NEW!)\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"COARSE-GRAINED ABLATION EXPERIMENTS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    coarse_grained_categories = ['sentiment_pos', 'sentiment_neg', 'hate', 'function']\n",
    "    \n",
    "    for category in coarse_grained_categories:\n",
    "        print(f\"\\nðŸŽ¯ Running coarse-grained experiment: {category}\")\n",
    "        coarse_result = run_coarse_grained_multilabel_ablation_experiments(\n",
    "            training_df, test_df, category, \n",
    "            ablation_method, dataset_name, binary_label, fine_grained_labels\n",
    "        )\n",
    "        results['coarse_grained'][category] = coarse_result\n",
    "    \n",
    "    # 2. FINE-GRAINED EXPERIMENTS (EXISTING)\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"FINE-GRAINED NEGATIVE EMOTIONS ABLATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # First run combined negative emotions\n",
    "    neg_emotions_result = run_fine_grained_multilabel_ablation_experiments(\n",
    "        training_df, test_df, 'neg_emotion', None, \n",
    "        ablation_method, dataset_name, binary_label, fine_grained_labels\n",
    "    )\n",
    "    results['neg_emotion']['combined'] = neg_emotions_result\n",
    "    \n",
    "    # Then individual negative emotions\n",
    "    negative_emotions = ['fear', 'anger', 'sadness', 'disgust']\n",
    "    for emotion in negative_emotions:\n",
    "        emotion_result = run_fine_grained_multilabel_ablation_experiments(\n",
    "            training_df, test_df, 'neg_emotion', emotion, \n",
    "            ablation_method, dataset_name, binary_label, fine_grained_labels\n",
    "        )\n",
    "        results['neg_emotion'][emotion] = emotion_result\n",
    "    \n",
    "    # 3. Function Words Experiments\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"FINE-GRAINED FUNCTION WORDS ABLATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    function_words_dict = load_function_words()\n",
    "    function_categories = list(function_words_dict.keys())\n",
    "    \n",
    "    for category in function_categories:\n",
    "        function_result = run_fine_grained_multilabel_ablation_experiments(\n",
    "            training_df, test_df, 'function', category, \n",
    "            ablation_method, dataset_name, binary_label, fine_grained_labels\n",
    "        )\n",
    "        results['function'][category] = function_result\n",
    "    \n",
    "    # 4. Hate Speech Experiments\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"FINE-GRAINED HATE SPEECH LEXICON ABLATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Get HurtLex lexicon and use all categories\n",
    "    hurtlex_dict = load_hurtlex(\"hurtlex-master/lexica/EN/1.2/hurtlex_EN.tsv\")\n",
    "    category_counts = count_words_by_category(hurtlex_dict)\n",
    "    all_categories = list(category_counts.keys())  \n",
    "    \n",
    "    for category in all_categories:\n",
    "        hate_result = run_fine_grained_multilabel_ablation_experiments(\n",
    "            training_df, test_df, 'hate', category, \n",
    "            ablation_method, dataset_name, binary_label, fine_grained_labels\n",
    "        )\n",
    "        results['hate'][category] = hate_result\n",
    "    \n",
    "    # Save results\n",
    "    print(f\"\\nðŸ“Š SAVING ERESULTS...\")\n",
    "    os.makedirs(\"evaluation/multilabel_results\", exist_ok=True)\n",
    "    \n",
    "    results_file = f\"evaluation/multilabel_results/{dataset_name}_{ablation_method}_multilabel_results.json\"\n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(results, f, indent=2, cls=NumpyEncoder)\n",
    "    \n",
    "    print(f\"âœ…  multilabel results saved to: {results_file}\")\n",
    "    \n",
    "    # Summary report\n",
    "    print(f\"\\nðŸ“Š MULTILABEL EXPERIMENT SUMMARY FOR {dataset_name}:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"âœ… Coarse-grained categories: {len(results['coarse_grained'])} experiments\")\n",
    "    print(f\"âœ… Fine-grained emotions: {len(results['neg_emotion'])} experiments\")\n",
    "    print(f\"âœ… Fine-grained function words: {len(results['function'])} experiments\")\n",
    "    print(f\"âœ… Fine-grained hate speech: {len(results['hate'])} experiments\")\n",
    "    \n",
    "    total_experiments = (len(results['coarse_grained']) + \n",
    "                        len(results['neg_emotion']) + \n",
    "                        len(results['function']) + \n",
    "                        len(results['hate']))\n",
    "    print(f\"ðŸŽ¯ Total experiments completed: {total_experiments}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def run_all_multilabel_experiments():\n",
    "    \"\"\"\n",
    "    Run  multilabel experiments for both datasets and both methods.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ðŸš€ RUNNING ALL MULTILABEL EXPERIMENTS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"This includes coarse-grained categories: sentiment_pos, sentiment_neg, hate, function\")\n",
    "    print(\"Plus all existing fine-grained categories\")\n",
    "    print()\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    # MAMI experiments\n",
    "    print(\"ðŸŽ¯ MAMI DATASET EXPERIMENTS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"\\nðŸ“¦ MAMI - MASK method...\")\n",
    "    mami_mask = run_multilabel_experiments(\n",
    "        dataset_name=\"MAMI\", ablation_method=\"mask\"\n",
    "    )\n",
    "    all_results['MAMI_mask'] = mami_mask\n",
    "    \n",
    "    print(\"\\nðŸ“¦ MAMI - REMOVE method...\")\n",
    "    mami_remove = run_multilabel_experiments(\n",
    "        dataset_name=\"MAMI\", ablation_method=\"remove\"\n",
    "    )\n",
    "    all_results['MAMI_remove'] = mami_remove\n",
    "    \n",
    "    # EXIST2024 experiments\n",
    "    print(\"\\nðŸŽ¯ EXIST2024 DATASET EXPERIMENTS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"\\nðŸ“¦ EXIST2024 - MASK method...\")\n",
    "    exist_mask = run_multilabel_experiments(\n",
    "        dataset_name=\"EXIST2024\", ablation_method=\"mask\"\n",
    "    )\n",
    "    all_results['EXIST2024_mask'] = exist_mask\n",
    "    \n",
    "    print(\"\\nðŸ“¦ EXIST2024 - REMOVE method...\")\n",
    "    exist_remove = run_multilabel_experiments(\n",
    "        dataset_name=\"EXIST2024\", ablation_method=\"remove\"\n",
    "    )\n",
    "    all_results['EXIST2024_remove'] = exist_remove\n",
    "    \n",
    "    # Save combined results\n",
    "    print(f\"\\nðŸ’¾ SAVING COMBINED RESULTS...\")\n",
    "    os.makedirs(\"evaluation/multilabel_results\", exist_ok=True)\n",
    "    \n",
    "    combined_results_file = \"evaluation/multilabel_results/all_multilabel_experiments.json\"\n",
    "    with open(combined_results_file, 'w') as f:\n",
    "        json.dump(all_results, f, indent=2, cls=NumpyEncoder)\n",
    "    \n",
    "    print(f\"âœ… All multilabel results saved to: {combined_results_file}\")\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\nðŸŽ‰ ALL MULTILABEL EXPERIMENTS COMPLETED!\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for experiment_name, experiment_results in all_results.items():\n",
    "        dataset, method = experiment_name.split('_')\n",
    "        total_exp = (len(experiment_results.get('coarse_grained', {})) + \n",
    "                    len(experiment_results.get('neg_emotion', {})) + \n",
    "                    len(experiment_results.get('function', {})) + \n",
    "                    len(experiment_results.get('hate', {})))\n",
    "        print(f\"ðŸ“Š {dataset} ({method}): {total_exp} experiments completed\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "# Run the experiments\n",
    "if __name__ == \"__main__\":\n",
    "    # Run all multilabel experiments including coarse-grained categories\n",
    "    print(\"ðŸš€ Starting multilabel experiments with coarse-grained categories...\")\n",
    "    all_results = run_all_multilabel_experiments()\n",
    "    \n",
    "    print(\"\\nâœ… Multilabel experiments completed!\")\n",
    "    print(\"ðŸ“ Results include both coarse-grained and fine-grained ablation experiments\")\n",
    "    print(\"ðŸŽ¯ Coarse-grained categories: sentiment_pos, sentiment_neg, hate, function\")\n",
    "    print(\"ðŸ”¬ Fine-grained categories: individual emotions, function word types, hate speech categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eef5bcf-349a-4a08-a638-ba36841baf94",
   "metadata": {},
   "source": [
    "## POS Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a88963-e813-4e88-bb7d-b29cc7a0d247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, install and import required packages\n",
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"Installing spaCy English model...\")\n",
    "    import subprocess\n",
    "    subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11b57c2-ee52-4893-8d8e-71edd9ac94dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy model for POS tagging\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Universal POS tags based on Van Nooten et al. (2021)  -- open class words\n",
    "# https://universaldependencies.org/u/pos/\n",
    "\n",
    "UNIVERSAL_POS_TAGS = [\n",
    "    'ADJ',      # adjective\n",
    "    'ADV',      # adverb\n",
    "    'INTJ',     # interjection\n",
    "    'NOUN',     # noun\n",
    "    'PROPN',    # proper noun\n",
    "    'VERB'      # verb\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344b1376-faee-45ef-bc6c-9c38d5767bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_pos_ablation_experiment(dataset_df_train, dataset_df_test, pos_category, \n",
    "                                dataset_name, binary_label='misogynous',\n",
    "                                train_pos_tags=None, test_pos_tags=None):\n",
    "    \"\"\"\n",
    "    Run POS ablation experiment for a specific POS category (optimized version with predictions saving).\n",
    "    \n",
    "    Parameters:\n",
    "    - dataset_df_train: Training DataFrame\n",
    "    - dataset_df_test: Test DataFrame  \n",
    "    - pos_category: POS category to ablate\n",
    "    - dataset_name: Name of dataset ('MAMI' or 'EXIST2024')\n",
    "    - binary_label: Binary label column name\n",
    "    - train_pos_tags: Precomputed POS tags for training data\n",
    "    - test_pos_tags: Precomputed POS tags for test data\n",
    "    \n",
    "    Returns:\n",
    "    - dict: Results including metrics and statistics\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"POS Ablation Experiment: {pos_category}\")\n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Get original texts and labels\n",
    "    X_train_orig = dataset_df_train[\"svm representation\"].tolist()\n",
    "    X_test_orig = dataset_df_test[\"svm representation\"].tolist()\n",
    "    y_train = dataset_df_train[binary_label].tolist()\n",
    "    y_test = dataset_df_test[binary_label].tolist()\n",
    "    \n",
    "    # Create ablated datasets using precomputed POS tags\n",
    "    print(\"\\nCreating ablated training set...\")\n",
    "    X_train_ablated, train_stats = create_pos_ablated_dataset(\n",
    "        X_train_orig, pos_category, train_pos_tags)\n",
    "    \n",
    "    print(\"\\nCreating ablated test set...\")\n",
    "    X_test_ablated, test_stats = create_pos_ablated_dataset(\n",
    "        X_test_orig, pos_category, test_pos_tags)\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\n{'-'*50}\")\n",
    "    print(\"ABLATION STATISTICS:\")\n",
    "    print(f\"{'-'*50}\")\n",
    "    print(f\"Training set:\")\n",
    "    print(f\"  - Documents affected: {train_stats['documents_affected']}/{train_stats['total_documents']} ({train_stats['percentage_docs_affected']:.2f}%)\")\n",
    "    print(f\"  - Total words removed: {train_stats['total_words_removed']}\")\n",
    "    print(f\"  - Avg words removed per doc: {train_stats['avg_words_removed_per_doc']:.2f}\")\n",
    "    print(f\"  - Max words removed: {train_stats['max_words_removed']}\")\n",
    "    \n",
    "    print(f\"\\nTest set:\")\n",
    "    print(f\"  - Documents affected: {test_stats['documents_affected']}/{test_stats['total_documents']} ({test_stats['percentage_docs_affected']:.2f}%)\")\n",
    "    print(f\"  - Total words removed: {test_stats['total_words_removed']}\")\n",
    "    print(f\"  - Avg words removed per doc: {test_stats['avg_words_removed_per_doc']:.2f}\")\n",
    "    print(f\"  - Max words removed: {test_stats['max_words_removed']}\")\n",
    "    \n",
    "    # Train model on ablated data\n",
    "    print(f\"\\n{'-'*50}\")\n",
    "    print(\"TRAINING MODEL ON ABLATED DATA:\")\n",
    "    print(f\"{'-'*50}\")\n",
    "    \n",
    "    try:\n",
    "        model, vectorizer = build_bin_classifier(X_train_ablated, y_train)\n",
    "        y_pred = classify_data(X_test_ablated, model, vectorizer)\n",
    "        \n",
    "        # Set up evaluation parameters\n",
    "        model_name = f\"svm_pos_ablation_{pos_category.lower()}_{dataset_name.lower()}\"\n",
    "        evaluation_type = \"binary\"\n",
    "\n",
    "        # Set gold standard file paths\n",
    "        if dataset_name == \"MAMI\":\n",
    "            gold_test_bin = \"\" # Path to MAMI binary classification gold labels (json)\n",
    "            gold_test_txt = \"\" # Path to MAMI text gold labels (txt)\n",
    "            label_names = [\"non-misogynous\", \"misogynous\"]\n",
    "        else:  # EXIST2024\n",
    "            gold_test_bin = \"\" # Path to EXIST2024 binary classification gold labels (json)\n",
    "            gold_test_txt = \"\" # Path to EXIST2024 text gold labels (txt)\n",
    "            label_names = [\"non-sexist\", \"sexist\"]\n",
    "\n",
    "        \n",
    "        # Save predictions and evaluate\n",
    "        test_pred_json, test_pred_txt = save_evaluation(\n",
    "            dataset_df_test, \"evaluation/predictions\", dataset_name, \"test\", \n",
    "            evaluation_type, model_name, y_pred, binary_label, []\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n{'-'*50}\")\n",
    "        print(\"EVALUATION RESULTS:\")\n",
    "        print(f\"{'-'*50}\")\n",
    "        \n",
    "        # Use existing evaluation function\n",
    "        evaluate_binary_classification(\n",
    "            gold_test_bin, test_pred_json, y_test, y_pred,\n",
    "            gold_test_txt, test_pred_txt, label_names, \n",
    "            model_name=f\"SVM POS Ablation ({pos_category})\"\n",
    "        )\n",
    "        \n",
    "        # Calculate all required metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision_macro = precision_score(y_test, y_pred, average='macro')  \n",
    "        recall_macro = recall_score(y_test, y_pred, average='macro')        \n",
    "        f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "        \n",
    "        # Get classification report as dictionary\n",
    "        class_report_dict = classification_report(y_test, y_pred, \n",
    "                                                target_names=label_names, \n",
    "                                                zero_division=0, digits=3, \n",
    "                                                output_dict=True)\n",
    "        \n",
    "        # Calculate binary F1 score (MAMI evaluation metric)\n",
    "        binary_f1 = evaluate_f1_scores(gold_test_txt, test_pred_txt, 2)\n",
    "        \n",
    "        # âœ… FIXED: Create structured results dictionary WITH PREDICTIONS\n",
    "        pos_results = {\n",
    "            'binary_f1': binary_f1,\n",
    "            'macro_f1': f1_macro,\n",
    "            'accuracy': accuracy,\n",
    "            'precision_macro': precision_macro,  \n",
    "            'recall_macro': recall_macro,        \n",
    "            'per_label_metrics': class_report_dict,\n",
    "            'predictions': y_pred.tolist(),  \n",
    "            'true_labels': y_test,           \n",
    "            'confusion_matrix': confusion_matrix(y_test, y_pred).tolist(),\n",
    "            'label_names': label_names,\n",
    "            'prediction_files': {\n",
    "                'json': test_pred_json,\n",
    "                'txt': test_pred_txt\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save results to JSON file\n",
    "        os.makedirs(\"evaluation/results/POS/SVM\", exist_ok=True)\n",
    "        results_file = f\"evaluation/results/POS/SVM/{model_name}_results.json\"\n",
    "        \n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(pos_results, f, indent=2, cls=NumpyEncoder)\n",
    "        \n",
    "        print(f\"âœ… POS results saved to: {results_file}\")\n",
    "        \n",
    "        # Return structured results\n",
    "        results = {\n",
    "            'pos_category': pos_category,\n",
    "            'model_name': model_name,\n",
    "            'accuracy': accuracy,\n",
    "            'precision_macro': precision_macro,\n",
    "            'recall_macro': recall_macro,\n",
    "            'f1_macro': f1_macro,\n",
    "            'binary_f1': binary_f1,\n",
    "            'predictions': y_pred.tolist(),  \n",
    "            'true_labels': y_test,\n",
    "            'train_stats': train_stats,\n",
    "            'test_stats': test_stats,\n",
    "            'prediction_files': {\n",
    "                'json': test_pred_json,\n",
    "                'txt': test_pred_txt\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in POS ablation for {pos_category}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def run_complete_pos_ablation_study(dataset_df_train, dataset_df_test, dataset_name, \n",
    "                                   binary_label='misogynous', pos_tags=None):\n",
    "    \"\"\"\n",
    "    Run complete POS ablation study for all POS categories (optimized version with baseline predictions saving).\n",
    "    \"\"\"\n",
    "    if pos_tags is None:\n",
    "        pos_tags = UNIVERSAL_POS_TAGS.copy()\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"OPTIMIZED POS ABLATION STUDY FOR {dataset_name}\")\n",
    "    print(f\"Testing {len(pos_tags)} POS categories: {', '.join(pos_tags)}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Get original texts and labels\n",
    "    X_train = dataset_df_train[\"svm representation\"].tolist()\n",
    "    X_test = dataset_df_test[\"svm representation\"].tolist()\n",
    "    y_train = dataset_df_train[binary_label].tolist()\n",
    "    y_test = dataset_df_test[binary_label].tolist()\n",
    "    \n",
    "    # Precompute POS tags for all texts once\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"PRECOMPUTING POS TAGS (This will save time for multiple experiments)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    print(\"Computing POS tags for training data...\")\n",
    "    train_pos_tags = extract_pos_tags_batch(X_train, batch_size=1000)\n",
    "    \n",
    "    print(\"Computing POS tags for test data...\")\n",
    "    test_pos_tags = extract_pos_tags_batch(X_test, batch_size=1000)\n",
    "    \n",
    "    print(\"POS tag computation complete! Now running experiments...\")\n",
    "    \n",
    "    # First run baseline (no ablation)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"BASELINE EXPERIMENT (NO ABLATION)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    baseline_model, baseline_vec = build_bin_classifier(X_train, y_train)\n",
    "    baseline_pred = classify_data(X_test, baseline_model, baseline_vec)\n",
    "    \n",
    "    \n",
    "    baseline_accuracy = accuracy_score(y_test, baseline_pred)\n",
    "    baseline_f1 = f1_score(y_test, baseline_pred, average='macro')\n",
    "    baseline_precision = precision_score(y_test, baseline_pred, average='macro')\n",
    "    baseline_recall = recall_score(y_test, baseline_pred, average='macro')\n",
    "    \n",
    "    print(f\"Baseline Results:\")\n",
    "    print(f\"  - Accuracy: {baseline_accuracy:.3f}\")\n",
    "    print(f\"  - F1-macro: {baseline_f1:.3f}\")\n",
    "    print(f\"  - Precision-macro: {baseline_precision:.3f}\")\n",
    "    print(f\"  - Recall-macro: {baseline_recall:.3f}\")\n",
    "    \n",
    "    # Save baseline predictions for POS ablation\n",
    "    baseline_results = {\n",
    "        'accuracy': baseline_accuracy,\n",
    "        'f1_macro': baseline_f1,\n",
    "        'precision_macro': baseline_precision,\n",
    "        'recall_macro': baseline_recall,\n",
    "        'predictions': baseline_pred.tolist(), \n",
    "        'true_labels': y_test\n",
    "    }\n",
    "    \n",
    "    # Save baseline results\n",
    "    os.makedirs(\"evaluation/results/POS/SVM\", exist_ok=True)\n",
    "    baseline_file = f\"evaluation/results/POS/SVM/pos_baseline_{dataset_name.lower()}_results.json\"\n",
    "    with open(baseline_file, 'w') as f:\n",
    "        json.dump(baseline_results, f, indent=2, cls=NumpyEncoder)\n",
    "    \n",
    "    print(f\"âœ… Baseline predictions saved to: {baseline_file}\")\n",
    "    \n",
    "    # Store all results\n",
    "    all_results = {\n",
    "        'baseline': baseline_results,\n",
    "        'ablation_results': {}\n",
    "    }\n",
    "    \n",
    "    # Run ablation for each POS category using precomputed tags\n",
    "    for pos_category in pos_tags:\n",
    "        results = run_pos_ablation_experiment(\n",
    "            dataset_df_train, dataset_df_test, pos_category, dataset_name, binary_label,\n",
    "            train_pos_tags, test_pos_tags\n",
    "        )\n",
    "        \n",
    "        if results is not None:\n",
    "            all_results['ablation_results'][pos_category] = results\n",
    "    \n",
    "    return all_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14230fe8-52d7-41b4-b609-7f68877b73dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mami_pos_ablation():\n",
    "    \"\"\"Run POS ablation study on MAMI dataset.\"\"\"\n",
    "    print(\"Starting MAMI POS Ablation Study...\")\n",
    "    \n",
    "    results = run_complete_pos_ablation_study(\n",
    "        mami_training_df, mami_test_df, \"MAMI\", binary_label='misogynous'\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_exist2024_pos_ablation():\n",
    "    \"\"\"Run POS ablation study on EXIST2024 dataset.\"\"\"\n",
    "    print(\"Starting EXIST2024 POS Ablation Study...\")\n",
    "    \n",
    "    results = run_complete_pos_ablation_study(\n",
    "        exist_training_df, exist_test_df, \"EXIST2024\", binary_label='sexist'\n",
    "    )\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2a6b97-3658-4a29-9b86-011499ffae9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mami_results = run_mami_pos_ablation()\n",
    "exist_results = run_exist2024_pos_ablation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628f75b1-8751-410e-9a34-91aea20feadd",
   "metadata": {},
   "source": [
    "# Export Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a460b91-03c5-4f4d-9191-56c7ae22fc21",
   "metadata": {},
   "source": [
    "## coarse-grained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a027dd2f-a862-402e-9dc0-ab87e8ee29a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_results(file_path):\n",
    "    \"\"\"Load results from JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_feature_info_from_filename(filename):\n",
    "    \"\"\"Extract feature type and method from filename.\"\"\"\n",
    "    # Remove extension and directory path\n",
    "    basename = os.path.basename(filename).replace('_results.json', '').replace('_bin_results.json', '')\n",
    "    \n",
    "    # Feature type extraction\n",
    "    feature_type = None\n",
    "    if 'sentiment_pos' in basename:\n",
    "        feature_type = 'Pos_Sentiment'\n",
    "    elif 'sentiment_neg' in basename:\n",
    "        feature_type = 'Neg_Sentiment'\n",
    "    elif 'function' in basename:\n",
    "        feature_type = 'Function'\n",
    "    elif 'hate' in basename:\n",
    "        feature_type = 'Hate'\n",
    "    \n",
    "    # Method extraction\n",
    "    method = None\n",
    "    if 'mask' in basename:\n",
    "        method = 'Placeholder'\n",
    "    elif 'remove' in basename:\n",
    "        method = 'Remove'\n",
    "    \n",
    "    return feature_type, method\n",
    "\n",
    "def collect_binary_results(dataset_name):\n",
    "    \"\"\"Collect binary classification results.\"\"\"\n",
    "    results_dir = \"evaluation/results/binary/SVM\"\n",
    "    dataset_lower = dataset_name.lower()\n",
    "    \n",
    "    # Find all binary result files\n",
    "    patterns = [\n",
    "        f\"{results_dir}/*{dataset_lower}*bin_results.json\",\n",
    "        f\"{results_dir}/*{dataset_lower}*results.json\"\n",
    "    ]\n",
    "    \n",
    "    found_files = []\n",
    "    for pattern in patterns:\n",
    "        files = glob.glob(pattern)\n",
    "        found_files.extend(files)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    found_files = list(set(found_files))\n",
    "    \n",
    "    binary_results = []\n",
    "    \n",
    "    for file_path in found_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        \n",
    "        # Skip baseline files\n",
    "        if 'baseline' in filename.lower():\n",
    "            continue\n",
    "            \n",
    "        # Skip random files (we'll handle them separately)\n",
    "        if 'random' in filename.lower():\n",
    "            continue\n",
    "            \n",
    "        feature_type, method = extract_feature_info_from_filename(filename)\n",
    "        \n",
    "        if feature_type and method:\n",
    "            data = load_json_results(file_path)\n",
    "            if data:\n",
    "                # Extract metrics\n",
    "                precision_macro = data.get('precision_macro', 0)\n",
    "                recall_macro = data.get('recall_macro', 0)\n",
    "                f1_macro = data.get('macro_f1', 0)\n",
    "                \n",
    "                # Get positive class F1 from per_label_metrics\n",
    "                f1_positive = 0\n",
    "                if 'per_label_metrics' in data:\n",
    "                    if dataset_name == 'MAMI' and 'misogynous' in data['per_label_metrics']:\n",
    "                        f1_positive = data['per_label_metrics']['misogynous']['f1-score']\n",
    "                    elif dataset_name == 'EXIST2024' and 'sexist' in data['per_label_metrics']:\n",
    "                        f1_positive = data['per_label_metrics']['sexist']['f1-score']\n",
    "                \n",
    "                binary_results.append({\n",
    "                    'Dataset': dataset_name,\n",
    "                    'Experiment Category': 'Feature Ablation',\n",
    "                    'Feature Type': feature_type,\n",
    "                    'Ablation Method': method,\n",
    "                    'Precision Macro': precision_macro,\n",
    "                    'Recall Macro': recall_macro,\n",
    "                    'F1 Macro': f1_macro,\n",
    "                    'F1 Positive Class': f1_positive\n",
    "                })\n",
    "    \n",
    "    return binary_results\n",
    "\n",
    "\n",
    "def add_baseline_results(results_list, dataset_name, experiment_type):\n",
    "    \"\"\"Add baseline results to the results list.\"\"\"\n",
    "    if experiment_type == 'binary':\n",
    "        baseline_file = f\"evaluation/results/binary/SVM/svm_baseline_bow_{dataset_name}_bin_baseline_results.json\"\n",
    "    else:  # multilabel\n",
    "        baseline_file = f\"evaluation/results/multi-label/SVM/fine-grained/svm_baseline_bow_hierarchy_{dataset_name}_results.json\"\n",
    "    \n",
    "    if os.path.exists(baseline_file):\n",
    "        baseline_data = load_json_results(baseline_file)\n",
    "        if baseline_data:\n",
    "            if experiment_type == 'binary':\n",
    "                # Add binary baseline\n",
    "                precision_macro = baseline_data.get('precision_macro', 0)\n",
    "                recall_macro = baseline_data.get('recall_macro', 0)\n",
    "                f1_macro = baseline_data.get('macro_f1', 0)\n",
    "                \n",
    "                # Get positive class F1\n",
    "                f1_positive = 0\n",
    "                if 'per_label_metrics' in baseline_data:\n",
    "                    if dataset_name == 'MAMI' and 'misogynous' in baseline_data['per_label_metrics']:\n",
    "                        f1_positive = baseline_data['per_label_metrics']['misogynous']['f1-score']\n",
    "                    elif dataset_name == 'EXIST2024' and 'sexist' in baseline_data['per_label_metrics']:\n",
    "                        f1_positive = baseline_data['per_label_metrics']['sexist']['f1-score']\n",
    "                \n",
    "                baseline_row = {\n",
    "                    'Dataset': dataset_name,\n",
    "                    'Experiment Category': 'Baseline',\n",
    "                    'Feature Type': '',\n",
    "                    'Ablation Method': '',\n",
    "                    'Precision Macro': precision_macro,\n",
    "                    'Recall Macro': recall_macro,\n",
    "                    'F1 Macro': f1_macro,\n",
    "                    'F1 Positive Class': f1_positive\n",
    "                }\n",
    "                results_list.insert(0, baseline_row)\n",
    "            \n",
    "            else:  # multilabel\n",
    "                # Add multilabel baseline\n",
    "                per_label_metrics = baseline_data.get('per_label_metrics', {})\n",
    "                \n",
    "                baseline_row = {\n",
    "                    'Dataset': dataset_name,\n",
    "                    'Experiment Category': 'Baseline',\n",
    "                    'Feature Type': '',\n",
    "                    'Ablation Method': '',\n",
    "                }\n",
    "                \n",
    "                # Add metrics for each label\n",
    "                if dataset_name == 'MAMI':\n",
    "                    labels = ['non-misogynous', 'shaming', 'stereotype', 'objectification', 'violence']\n",
    "                    label_mapping = {\n",
    "                        'non-misogynous': 'Non-misogyny',\n",
    "                        'shaming': 'Shaming',\n",
    "                        'stereotype': 'Stereotype', \n",
    "                        'objectification': 'Objectification',\n",
    "                        'violence': 'Violence'\n",
    "                    }\n",
    "                else:  # EXIST2024\n",
    "                    labels = ['non-sexist', 'ideological-inequality', 'stereotyping-dominance', \n",
    "                             'objectification', 'sexual-violence', 'misogyny-non-sexual-violence']\n",
    "                    label_mapping = {\n",
    "                        'non-sexist': 'Non-sexist',\n",
    "                        'ideological-inequality': 'Ideological-inequality',\n",
    "                        'stereotyping-dominance': 'Stereotyping-dominance',\n",
    "                        'objectification': 'Objectification',\n",
    "                        'sexual-violence': 'Sexual-violence',\n",
    "                        'misogyny-non-sexual-violence': 'Misogyny-non-sexual-violence'\n",
    "                    }\n",
    "                \n",
    "                for label in labels:\n",
    "                    if label in per_label_metrics:\n",
    "                        metrics = per_label_metrics[label]\n",
    "                        mapped_label = label_mapping.get(label, label)\n",
    "                        baseline_row[f'{mapped_label} P'] = metrics.get('precision', 0)\n",
    "                        baseline_row[f'{mapped_label} R'] = metrics.get('recall', 0)\n",
    "                        baseline_row[f'{mapped_label} F1'] = metrics.get('f1-score', 0)\n",
    "                \n",
    "                # Add macro average\n",
    "                if 'macro avg' in per_label_metrics:\n",
    "                    macro_metrics = per_label_metrics['macro avg']\n",
    "                    baseline_row['Macro average P'] = macro_metrics.get('precision', 0)\n",
    "                    baseline_row['Macro average R'] = macro_metrics.get('recall', 0)\n",
    "                    baseline_row['Macro average F1'] = macro_metrics.get('f1-score', 0)\n",
    "                \n",
    "                results_list.insert(0, baseline_row)\n",
    "\n",
    "\n",
    "def collect_random_including_results(dataset_name):\n",
    "    \"\"\"Collect random including results, excluding function word random ablations.\"\"\"\n",
    "    results_dir = \"evaluation/results/binary/SVM\"\n",
    "    patterns = [f\"{results_dir}/random_including_*{dataset_name.lower()}*results.json\"]\n",
    "    \n",
    "    random_results = []\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        files = glob.glob(pattern)\n",
    "        for file_path in files:\n",
    "            filename = os.path.basename(file_path)\n",
    "            \n",
    "            if 'random_including' in filename:\n",
    "                # Extract base feature type\n",
    "                if 'sentiment_pos' in filename:\n",
    "                    base_feature = 'Pos_Sentiment'\n",
    "                elif 'sentiment_neg' in filename:\n",
    "                    base_feature = 'Neg_Sentiment'\n",
    "                elif 'function' in filename:\n",
    "                    # Skip function word random ablations\n",
    "                    print(f\"   â­ï¸ Skipping function word random ablation: {filename}\")\n",
    "                    continue\n",
    "                elif 'hate' in filename:\n",
    "                    base_feature = 'Hate'\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                data = load_json_results(file_path)\n",
    "                if data:\n",
    "                    precision_macro = data.get('precision_macro', 0)\n",
    "                    recall_macro = data.get('recall_macro', 0)\n",
    "                    f1_macro = data.get('macro_f1', 0)\n",
    " \n",
    "                    \n",
    "                    # Get positive class F1\n",
    "                    f1_positive = 0\n",
    "                    if 'per_label_metrics' in data:\n",
    "                        if dataset_name == 'MAMI' and 'misogynous' in data['per_label_metrics']:\n",
    "                            f1_positive = data['per_label_metrics']['misogynous']['f1-score']\n",
    "                        elif dataset_name == 'EXIST2024' and 'sexist' in data['per_label_metrics']:\n",
    "                            f1_positive = data['per_label_metrics']['sexist']['f1-score']\n",
    "                    \n",
    "                    method_name = 'Remove' if 'remove' in filename else 'Placeholder'\n",
    "                    \n",
    "                    random_row = {\n",
    "                        'Dataset': dataset_name,\n",
    "                        'Experiment Category': 'Random Words',\n",
    "                        'Feature Type': base_feature,\n",
    "                        'Ablation Method': method_name,\n",
    "                        'Precision Macro': precision_macro,\n",
    "                        'Recall Macro': recall_macro,\n",
    "                        'F1 Macro': f1_macro,\n",
    "                        'F1 Positive Class': f1_positive\n",
    "                    }\n",
    "                    random_results.append(random_row)\n",
    "    \n",
    "    return random_results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def collect_coarse_grained_multilabel_results(dataset_name):\n",
    "    \"\"\"\n",
    "    Collect coarse-grained multilabel classification results.\n",
    "    \n",
    "    Looks for results from run_coarse_grained_multilabel_ablation_experiments function\n",
    "    which processes sentiment_pos, sentiment_neg, hate, function categories.\n",
    "    \"\"\"\n",
    "    results_dir = \"evaluation/results/multi-label/SVM/coarse-grained\"\n",
    "    \n",
    "    # Search patterns for coarse-grained multilabel results\n",
    "    if dataset_name in [\"EXIST2024\", \"EXIST\"]:\n",
    "        patterns = [\n",
    "            f\"{results_dir}/*exist*_hierarchy_results.json\",\n",
    "            f\"{results_dir}/*EXIST2024*_hierarchy_results.json\"\n",
    "        ]\n",
    "    else:  # MAMI\n",
    "        patterns = [\n",
    "            f\"{results_dir}/*mami*_hierarchy_results.json\", \n",
    "            f\"{results_dir}/*MAMI*_hierarchy_results.json\"\n",
    "        ]\n",
    "    \n",
    "    found_files = []\n",
    "    for pattern in patterns:\n",
    "        files = glob.glob(pattern)\n",
    "        found_files.extend(files)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    found_files = list(set(found_files))\n",
    "    \n",
    "    coarse_multilabel_results = []\n",
    "    \n",
    "    for file_path in found_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        \n",
    "        # Skip baseline files\n",
    "        if 'baseline' in filename.lower():\n",
    "            continue\n",
    "        \n",
    "        # Extract feature info for coarse-grained categories\n",
    "        feature_type, method = extract_coarse_grained_feature_info(filename, dataset_name)\n",
    "        \n",
    "        if feature_type and method:\n",
    "            data = load_json_results(file_path)\n",
    "            if data:\n",
    "                # Get per-label metrics\n",
    "                per_label_metrics = data.get('per_label_metrics', {})\n",
    "                \n",
    "                # Initialize metrics\n",
    "                result_row = {\n",
    "                    'Dataset': dataset_name,\n",
    "                    'Experiment Category': 'Coarse-Grained Feature Ablation',\n",
    "                    'Feature Type': feature_type,\n",
    "                    'Ablation Method': method,\n",
    "                }\n",
    "                \n",
    "                # Extract metrics for each label\n",
    "                if dataset_name == 'MAMI':\n",
    "                    labels = ['non-misogynous', 'shaming', 'stereotype', 'objectification', 'violence']\n",
    "                    label_mapping = {\n",
    "                        'non-misogynous': 'Non-misogyny',\n",
    "                        'shaming': 'Shaming',\n",
    "                        'stereotype': 'Stereotype', \n",
    "                        'objectification': 'Objectification',\n",
    "                        'violence': 'Violence'\n",
    "                    }\n",
    "                else:  # EXIST2024\n",
    "                    labels = ['non-sexist', 'ideological-inequality', 'stereotyping-dominance', \n",
    "                             'objectification', 'sexual-violence', 'misogyny-non-sexual-violence']\n",
    "                    label_mapping = {\n",
    "                        'non-sexist': 'Non-sexist',\n",
    "                        'ideological-inequality': 'Ideological-inequality',\n",
    "                        'stereotyping-dominance': 'Stereotyping-dominance',\n",
    "                        'objectification': 'Objectification',\n",
    "                        'sexual-violence': 'Sexual-violence',\n",
    "                        'misogyny-non-sexual-violence': 'Misogyny-non-sexual-violence'\n",
    "                    }\n",
    "                \n",
    "                # Add precision, recall, F1 for each label\n",
    "                for label in labels:\n",
    "                    if label in per_label_metrics:\n",
    "                        metrics = per_label_metrics[label]\n",
    "                        mapped_label = label_mapping.get(label, label)\n",
    "                        result_row[f'{mapped_label} P'] = metrics.get('precision', 0)\n",
    "                        result_row[f'{mapped_label} R'] = metrics.get('recall', 0)\n",
    "                        result_row[f'{mapped_label} F1'] = metrics.get('f1-score', 0)\n",
    "                \n",
    "                # Add macro average\n",
    "                if 'macro avg' in per_label_metrics:\n",
    "                    macro_metrics = per_label_metrics['macro avg']\n",
    "                    result_row['Macro average P'] = macro_metrics.get('precision', 0)\n",
    "                    result_row['Macro average R'] = macro_metrics.get('recall', 0)\n",
    "                    result_row['Macro average F1'] = macro_metrics.get('f1-score', 0)\n",
    "                \n",
    "                coarse_multilabel_results.append(result_row)\n",
    "    \n",
    "    return coarse_multilabel_results\n",
    "\n",
    "\n",
    "def extract_coarse_grained_feature_info(filename, dataset_name):\n",
    "    \"\"\"\n",
    "    Extract feature type and method from coarse-grained multilabel filename.\n",
    "    \n",
    "    Focuses on the 4 coarse-grained categories: sentiment_pos, sentiment_neg, hate, function\n",
    "    \"\"\"\n",
    "    # Remove common parts and normalize\n",
    "    basename = filename.replace('_results.json', '').replace('_hierarchy_results.json', '')\n",
    "    basename = basename.replace(f'{dataset_name}_', '').replace('svm_ablation_', '')\n",
    "    basename = basename.replace('EXIST2024_', '').replace('MAMI_', '')  # Handle case variations\n",
    "    \n",
    "    feature_type = None\n",
    "    method = None\n",
    "    \n",
    "    # Extract coarse-grained feature types\n",
    "    if 'sentiment_pos' in basename:\n",
    "        feature_type = 'CG_Pos_Sentiment'\n",
    "    elif 'sentiment_neg' in basename:\n",
    "        feature_type = 'CG_Neg_Sentiment'\n",
    "    elif 'hate' in basename:\n",
    "        # Handle general hate category (not specific subcategories)\n",
    "        feature_type = 'CG_Hate'\n",
    "    elif 'function' in basename:\n",
    "        # Handle general function category (not specific subcategories)\n",
    "        feature_type = 'CG_Function'\n",
    "    \n",
    "    # Method extraction\n",
    "    if 'mask' in basename:\n",
    "        method = 'Placeholder'\n",
    "    elif 'remove' in basename:\n",
    "        method = 'Remove'\n",
    "    \n",
    "    return feature_type, method\n",
    "\n",
    "\n",
    "def export_coarse_grained_multilabel_results_separately():\n",
    "    \"\"\"\n",
    "    Export coarse-grained multilabel results as separate CSV files for MAMI and EXIST2024.\n",
    "    \n",
    "    This function specifically handles results from run_coarse_grained_multilabel_ablation_experiments\n",
    "    which processes the 4 coarse-grained categories: sentiment_pos, sentiment_neg, hate, function.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ðŸ”„ Collecting coarse-grained multilabel ablation results for CSV export...\")\n",
    "    \n",
    "    datasets = ['MAMI', 'EXIST2024']\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = \"evaluation/exported_results\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        print(f\"\\nðŸ“Š Processing coarse-grained multilabel results for {dataset}...\")\n",
    "        \n",
    "        dataset_results = []\n",
    "        \n",
    "        # Add baseline first\n",
    "        add_baseline_results(dataset_results, dataset, 'multilabel')\n",
    "        \n",
    "        # Add coarse-grained feature ablation results\n",
    "        coarse_results = collect_coarse_grained_multilabel_results(dataset)\n",
    "        dataset_results.extend(coarse_results)\n",
    "        \n",
    "        print(f\"   âœ… Added {len(coarse_results)} coarse-grained multilabel results for {dataset}\")\n",
    "        \n",
    "        if dataset_results:\n",
    "            # Create DataFrame\n",
    "            df = pd.DataFrame(dataset_results)\n",
    "            \n",
    "            # Sort: Feature Type first, then Method\n",
    "            df['Feature_Type_Sort'] = df['Feature Type'].fillna('')\n",
    "            df = df.sort_values(\n",
    "                ['Feature_Type_Sort', 'Ablation Method'], \n",
    "                na_position='first'\n",
    "            )\n",
    "            # Remove the helper column\n",
    "            df = df.drop('Feature_Type_Sort', axis=1)\n",
    "            \n",
    "            # Export to CSV\n",
    "            csv_path = f\"{output_dir}/coarse_grained_multilabel_results_{dataset}.csv\"\n",
    "            df.to_csv(csv_path, index=False, float_format='%.3f')\n",
    "            \n",
    "            print(f\"âœ… Coarse-grained multilabel results for {dataset} exported to: {csv_path}\")\n",
    "            print(f\"   ðŸ“Š Total rows: {len(dataset_results)}\")\n",
    "            print(f\"   ðŸ“Š Columns: {list(df.columns)}\")\n",
    "            \n",
    "            # Show feature types found\n",
    "            feature_types = df[df['Feature Type'].notna()]['Feature Type'].unique()\n",
    "            if len(feature_types) > 0:\n",
    "                print(f\"   ðŸŽ¯ Feature types: {', '.join(feature_types)}\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ No coarse-grained multilabel results found for {dataset}\")\n",
    "\n",
    "    \n",
    "    print(f\"ðŸ“ Results saved in: {output_dir}/\")\n",
    "    print(\"ðŸ“Š Separate CSV files created for MAMI and EXIST2024\")\n",
    "    print(\"ðŸŽ¯ Categories: CG_Pos_Sentiment, CG_Neg_Sentiment, CG_Hate, CG_Function\")\n",
    "\n",
    "\n",
    "\n",
    "export_coarse_grained_multilabel_results_separately()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb61396-c0e3-4eff-aa47-de0194e690bb",
   "metadata": {},
   "source": [
    "## fine-grained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe4995e-e555-4dae-ad40-0a4d0918bbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_multilabel_results(dataset_name):\n",
    "    \"\"\"Collect multilabel classification results.\"\"\"\n",
    "    results_dir = \"evaluation/results/multi-label/SVM/fine-grained\"\n",
    "    \n",
    "    # Search patterns\n",
    "    if dataset_name in [\"EXIST2024\", \"EXIST\"]:\n",
    "        patterns = [\n",
    "            f\"{results_dir}/*exist*_results.json\",\n",
    "            f\"{results_dir}/*EXIST2024*_results.json\"\n",
    "        ]\n",
    "    else:  # MAMI\n",
    "        patterns = [\n",
    "            f\"{results_dir}/*mami*_results.json\", \n",
    "            f\"{results_dir}/*MAMI*_results.json\"\n",
    "        ]\n",
    "    \n",
    "    found_files = []\n",
    "    for pattern in patterns:\n",
    "        files = glob.glob(pattern)\n",
    "        found_files.extend(files)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    found_files = list(set(found_files))\n",
    "    \n",
    "    multilabel_results = []\n",
    "    \n",
    "    for file_path in found_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        \n",
    "        # Skip baseline files\n",
    "        if 'baseline' in filename.lower():\n",
    "            continue\n",
    "        \n",
    "        # Extract feature info from multilabel filename\n",
    "        feature_type, method = extract_multilabel_feature_info(filename, dataset_name)\n",
    "        \n",
    "        if feature_type and method:\n",
    "            data = load_json_results(file_path)\n",
    "            if data:\n",
    "                # Get per-label metrics\n",
    "                per_label_metrics = data.get('per_label_metrics', {})\n",
    "                \n",
    "                # Initialize metrics\n",
    "                result_row = {\n",
    "                    'Dataset': dataset_name,\n",
    "                    'Experiment Category': 'Feature Ablation',\n",
    "                    'Feature Type': feature_type,\n",
    "                    'Ablation Method': method,\n",
    "                }\n",
    "                \n",
    "                # Extract metrics for each label\n",
    "                if dataset_name == 'MAMI':\n",
    "                    labels = ['non-misogynous', 'shaming', 'stereotype', 'objectification', 'violence']\n",
    "                    label_mapping = {\n",
    "                        'non-misogynous': 'Non-misogyny',\n",
    "                        'shaming': 'Shaming',\n",
    "                        'stereotype': 'Stereotype', \n",
    "                        'objectification': 'Objectification',\n",
    "                        'violence': 'Violence'\n",
    "                    }\n",
    "                else:  # EXIST2024\n",
    "                    labels = ['non-sexist', 'ideological-inequality', 'stereotyping-dominance', \n",
    "                             'objectification', 'sexual-violence', 'misogyny-non-sexual-violence']\n",
    "                    label_mapping = {\n",
    "                        'non-sexist': 'Non-sexist',\n",
    "                        'ideological-inequality': 'Ideological-inequality',\n",
    "                        'stereotyping-dominance': 'Stereotyping-dominance',\n",
    "                        'objectification': 'Objectification',\n",
    "                        'sexual-violence': 'Sexual-violence',\n",
    "                        'misogyny-non-sexual-violence': 'Misogyny-non-sexual-violence'\n",
    "                    }\n",
    "                \n",
    "                # Add precision, recall, F1 for each label\n",
    "                for label in labels:\n",
    "                    if label in per_label_metrics:\n",
    "                        metrics = per_label_metrics[label]\n",
    "                        mapped_label = label_mapping.get(label, label)\n",
    "                        result_row[f'{mapped_label} P'] = metrics.get('precision', 0)\n",
    "                        result_row[f'{mapped_label} R'] = metrics.get('recall', 0)\n",
    "                        result_row[f'{mapped_label} F1'] = metrics.get('f1-score', 0)\n",
    "                \n",
    "                # Add macro average\n",
    "                if 'macro avg' in per_label_metrics:\n",
    "                    macro_metrics = per_label_metrics['macro avg']\n",
    "                    result_row['Macro average P'] = macro_metrics.get('precision', 0)\n",
    "                    result_row['Macro average R'] = macro_metrics.get('recall', 0)\n",
    "                    result_row['Macro average F1'] = macro_metrics.get('f1-score', 0)\n",
    "                \n",
    "                multilabel_results.append(result_row)\n",
    "    \n",
    "    return multilabel_results\n",
    "\n",
    "def extract_multilabel_feature_info(filename, dataset_name):\n",
    "    \"\"\"Extract feature type and method from multilabel filename.\"\"\"\n",
    "    # Remove common parts and normalize\n",
    "    basename = filename.replace('_results.json', '').replace('_hierarchy_results.json', '')\n",
    "    basename = basename.replace(f'{dataset_name}_', '').replace('svm_ablation_', '')\n",
    "    basename = basename.replace('EXIST2024_', '').replace('MAMI_', '')  # Handle case variations\n",
    "    \n",
    "    \n",
    "    feature_type = None\n",
    "    method = None\n",
    "    \n",
    "    # Feature type extraction for multilabel (more specific categories)\n",
    "    if 'all_neg_emotions' in basename:\n",
    "        feature_type = 'All_Neg_Emotions'\n",
    "    elif 'neg_sadness' in basename:\n",
    "        feature_type = 'Neg_Sadness'\n",
    "    elif 'neg_anger' in basename:\n",
    "        feature_type = 'Neg_Anger'\n",
    "    elif 'neg_fear' in basename:\n",
    "        feature_type = 'Neg_Fear'\n",
    "    elif 'neg_disgust' in basename:\n",
    "        feature_type = 'Neg_Disgust'\n",
    "    elif 'func_' in basename or 'auxiliary_verbs' in basename or 'conjunctions' in basename or 'determiners' in basename or 'enumerators' in basename or 'interjections' in basename or 'particles' in basename or 'prepositions' in basename or 'pronouns' in basename or 'qualifiers' in basename:\n",
    "        # Extract specific function word category\n",
    "        if 'auxiliary_verbs' in basename or 'auxiliary' in basename:\n",
    "            feature_type = 'Func_Auxiliary'\n",
    "        elif 'conjunctions' in basename:\n",
    "            feature_type = 'Func_Conjunctions'\n",
    "        elif 'determiners' in basename:\n",
    "            feature_type = 'Func_Determiners'\n",
    "        elif 'enumerators' in basename:\n",
    "            feature_type = 'Func_Enumerators'\n",
    "        elif 'interjections' in basename:\n",
    "            feature_type = 'Func_Interjections'\n",
    "        elif 'particles' in basename:\n",
    "            feature_type = 'Func_Particles'\n",
    "        elif 'prepositions' in basename:\n",
    "            feature_type = 'Func_Prepositions'\n",
    "        elif 'pronouns' in basename:\n",
    "            feature_type = 'Func_Pronouns'\n",
    "        elif 'qualifiers' in basename:\n",
    "            feature_type = 'Func_Qualifiers'\n",
    "        else:\n",
    "            # Fallback for func_ pattern\n",
    "            parts = basename.split('_')\n",
    "            for i, part in enumerate(parts):\n",
    "                if part == 'func' and i + 1 < len(parts):\n",
    "                    func_category = parts[i + 1]\n",
    "                    feature_type = f'Func_{func_category.title()}'\n",
    "                    break\n",
    "    elif 'hate_' in basename or any(hate_word in basename for hate_word in ['hate_an', 'hate_asf', 'hate_asm', 'hate_cds', 'hate_ddp', 'hate_ddf', 'hate_dmc', 'hate_is', 'hate_om', 'hate_or', 'hate_pa', 'hate_pr', 'hate_ps', 'hate_qas', 'hate_rci', 'hate_re', 'hate_svp']):\n",
    "        # Extract specific hate speech category\n",
    "        hate_categories = ['an', 'asf', 'asm', 'cds', 'ddp', 'ddf', 'dmc', 'is', 'om', 'or', 'pa', 'pr', 'ps', 'qas', 'rci', 're', 'svp']\n",
    "        for category in hate_categories:\n",
    "            if f'hate_{category}' in basename:\n",
    "                feature_type = f'Hate_{category.upper()}'\n",
    "                break\n",
    "        \n",
    "        # Fallback for hate_ pattern\n",
    "        if not feature_type:\n",
    "            parts = basename.split('_')\n",
    "            for i, part in enumerate(parts):\n",
    "                if part == 'hate' and i + 1 < len(parts):\n",
    "                    hate_category = parts[i + 1]\n",
    "                    feature_type = f'Hate_{hate_category.upper()}'\n",
    "                    break\n",
    "    \n",
    "    # Method extraction\n",
    "    if 'mask' in basename:\n",
    "        method = 'Placeholder'\n",
    "    elif 'remove' in basename:\n",
    "        method = 'Remove'\n",
    "    \n",
    "    \n",
    "    return feature_type, method\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a35312a-7bce-4e20-9ca4-f80e62e0865b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_ablation_results_to_csv():\n",
    "    \"\"\"Main function to export all ablation results to CSV.\"\"\"\n",
    "    \n",
    "    print(\"ðŸ”„ Collecting ablation results for CSV export...\")\n",
    "    \n",
    "    datasets = ['MAMI', 'EXIST2024']\n",
    "    \n",
    "    # Collect binary results (baseline + feature ablation only)\n",
    "    print(\"\\nðŸ“Š Collecting binary classification results (baseline + feature ablation)...\")\n",
    "    binary_results = []\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        print(f\"   Processing {dataset}...\")\n",
    "        \n",
    "        # Add baseline first\n",
    "        add_baseline_results(binary_results, dataset, 'binary')\n",
    "        \n",
    "        # Add feature ablation results\n",
    "        dataset_results = collect_binary_results(dataset)\n",
    "        binary_results.extend(dataset_results)\n",
    "        \n",
    "        print(f\"   âœ… Added {len(dataset_results)} feature ablation results for {dataset}\")\n",
    "    \n",
    "    # Collect binary results with random words (baseline + feature ablation + random)\n",
    "    print(\"\\nðŸ“Š Collecting binary classification results with random words (excluding function word random ablations)...\")\n",
    "    binary_results_with_random = binary_results.copy()  # Start with baseline + feature ablation\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        print(f\"   Processing random results for {dataset}...\")\n",
    "        \n",
    "        # Add random including results (excluding function word random ablations)\n",
    "        random_results = collect_random_including_results(dataset)\n",
    "        binary_results_with_random.extend(random_results)\n",
    "        \n",
    "        print(f\"   âœ… Added {len(random_results)} random word results for {dataset} (function word random ablations excluded)\")\n",
    "    \n",
    "    # Collect multilabel results \n",
    "    print(\"\\nðŸ“Š Collecting multilabel classification results...\")\n",
    "    multilabel_results_by_dataset = {}  \n",
    "    \n",
    "    for dataset in datasets:\n",
    "        print(f\"   Processing {dataset}...\")\n",
    "        \n",
    "        dataset_multilabel_results = []\n",
    "        \n",
    "        # Add baseline first\n",
    "        add_baseline_results(dataset_multilabel_results, dataset, 'multilabel')\n",
    "        \n",
    "        # Add feature ablation results\n",
    "        dataset_results = collect_multilabel_results(dataset)\n",
    "        dataset_multilabel_results.extend(dataset_results)\n",
    "        \n",
    "        multilabel_results_by_dataset[dataset] = dataset_multilabel_results\n",
    "        \n",
    "        print(f\"   âœ… Added {len(dataset_results)} feature ablation results for {dataset}\")\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = \"evaluation/exported_results\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Export binary results (baseline + feature ablation only)\n",
    "    if binary_results:\n",
    "        binary_df = pd.DataFrame(binary_results)\n",
    "        binary_df = binary_df.sort_values(['Dataset', 'Feature Type', 'Ablation Method'], \n",
    "                                         na_position='first')\n",
    "        \n",
    "        binary_csv_path = f\"{output_dir}/binary_ablation_results.csv\"\n",
    "        binary_df.to_csv(binary_csv_path, index=False, float_format='%.3f')\n",
    "\n",
    "        print(f\"\\nâœ… Binary results (baseline + feature ablation) exported to: {binary_csv_path}\")\n",
    "        print(f\"   ðŸ“Š Total rows: {len(binary_results)}\")\n",
    "        print(f\"   ðŸ“Š Columns: {list(binary_df.columns)}\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸ No binary results found to export\")\n",
    "    \n",
    "    # Export binary results with random words (baseline + feature ablation + random)\n",
    "    if binary_results_with_random:\n",
    "        binary_random_df = pd.DataFrame(binary_results_with_random)\n",
    "        \n",
    "        # Custom sorting: Dataset first, then by Feature Type within each dataset\n",
    "        # Handle empty Feature Type (baseline) by putting it first\n",
    "        binary_random_df['Feature_Type_Sort'] = binary_random_df['Feature Type'].fillna('')\n",
    "        binary_random_df = binary_random_df.sort_values(\n",
    "            ['Dataset', 'Feature_Type_Sort', 'Experiment Category', 'Ablation Method'], \n",
    "            na_position='first'\n",
    "        )\n",
    "        # Remove the helper column\n",
    "        binary_random_df = binary_random_df.drop('Feature_Type_Sort', axis=1)\n",
    "        \n",
    "        binary_random_csv_path = f\"{output_dir}/binary_ablation_results_random.csv\"\n",
    "        binary_random_df.to_csv(binary_random_csv_path, index=False, float_format='%.3f')\n",
    "\n",
    "        print(f\"\\nâœ… Binary results with random words exported to: {binary_random_csv_path}\")\n",
    "        print(f\"   ðŸ“Š Total rows: {len(binary_results_with_random)} (function word random ablations excluded)\")\n",
    "        print(f\"   ðŸ“Š Columns: {list(binary_random_df.columns)}\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸ No binary results with random words found to export\")\n",
    "    \n",
    "    # Export multilabel results\n",
    "    multilabel_results_all = [] \n",
    "    \n",
    "    for dataset, dataset_results in multilabel_results_by_dataset.items():\n",
    "        if dataset_results:\n",
    "            multilabel_df = pd.DataFrame(dataset_results)\n",
    "            \n",
    "            # Sort multilabel results: Feature Type first\n",
    "            multilabel_df['Feature_Type_Sort'] = multilabel_df['Feature Type'].fillna('')\n",
    "            multilabel_df = multilabel_df.sort_values(\n",
    "                ['Feature_Type_Sort', 'Ablation Method'], \n",
    "                na_position='first'\n",
    "            )\n",
    "            # Remove the helper column\n",
    "            multilabel_df = multilabel_df.drop('Feature_Type_Sort', axis=1)\n",
    "            \n",
    "\n",
    "            multilabel_csv_path = f\"{output_dir}/multilabel_ablation_results_{dataset}.csv\"\n",
    "            multilabel_df.to_csv(multilabel_csv_path, index=False, float_format='%.3f')\n",
    "            \n",
    "            print(f\"\\nâœ… Multilabel results for {dataset} exported to: {multilabel_csv_path}\")\n",
    "            print(f\"   ðŸ“Š Total rows: {len(dataset_results)}\")\n",
    "            print(f\"   ðŸ“Š Columns: {list(multilabel_df.columns)}\")\n",
    "            \n",
    "\n",
    "            multilabel_results_all.extend(dataset_results)\n",
    "        else:\n",
    "            print(f\"\\nâš ï¸ No multilabel results found for {dataset}\")\n",
    "    \n",
    "    # Optional\n",
    "    if multilabel_results_all:\n",
    "        combined_multilabel_df = pd.DataFrame(multilabel_results_all)\n",
    "        combined_multilabel_df['Feature_Type_Sort'] = combined_multilabel_df['Feature Type'].fillna('')\n",
    "        combined_multilabel_df = combined_multilabel_df.sort_values(\n",
    "            ['Dataset', 'Feature_Type_Sort', 'Ablation Method'], \n",
    "            na_position='first'\n",
    "        )\n",
    "        combined_multilabel_df = combined_multilabel_df.drop('Feature_Type_Sort', axis=1)\n",
    "        \n",
    "        combined_multilabel_csv_path = f\"{output_dir}/multilabel_ablation_results_combined.csv\"\n",
    "        combined_multilabel_df.to_csv(combined_multilabel_csv_path, index=False, float_format='%.3f')\n",
    "        \n",
    "        print(f\"\\nâœ… Combined multilabel results exported to: {combined_multilabel_csv_path}\")\n",
    "        print(f\"   ðŸ“Š Total rows: {len(multilabel_results_all)}\")\n",
    "    \n",
    "    # Create summary\n",
    "    print(f\"\\nðŸŽ‰ EXPORT COMPLETED!\")\n",
    "    print(f\"ðŸ“ Results saved in: {output_dir}/\")\n",
    "    print(f\"ðŸ“Š Binary results (baseline + feature ablation): {len(binary_results) if binary_results else 0} rows\")\n",
    "    print(f\"ðŸ“Š Binary results with random words: {len(binary_results_with_random) if binary_results_with_random else 0} rows (function word random ablations excluded)\")\n",
    "    \n",
    "\n",
    "    for dataset, dataset_results in multilabel_results_by_dataset.items():\n",
    "        print(f\"ðŸ“Š Multilabel results for {dataset}: {len(dataset_results) if dataset_results else 0} rows\")\n",
    "    \n",
    "    print(f\"ðŸ“Š Combined multilabel results: {len(multilabel_results_all) if multilabel_results_all else 0} rows\")\n",
    "    \n",
    "    return binary_results, binary_results_with_random, multilabel_results_all\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the export\n",
    "    binary_results, binary_results_with_random, multilabel_results = export_ablation_results_to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf5ebe7-b9d1-43f7-84a3-a2fca1c26a36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
