{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-nokIlNT8Uy3",
    "outputId": "e6ff785f-3691-482a-8c07-1c964dfe5649"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers torch pandas numpy scikit-learn matplotlib seaborn\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from google.colab import files\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZrKcswqi9EKn",
    "outputId": "230205b3-4a4a-4997-9b24-f7cc7ba64306"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive in Colab:\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N_dJHRuM9IwV"
   },
   "outputs": [],
   "source": [
    "# Path to your folder in Google Drive\n",
    "route = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O_Ifc2hF9NP8",
    "outputId": "37cfb99f-8d19-40ef-943c-ca45df33c418"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afR9cyv69RyM"
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hbhMRfKxqjYL"
   },
   "source": [
    "## **utils_basic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nj6Gs4DO9VB9",
    "outputId": "d792a5d7-2534-4b56-f248-667c843f3a26"
   },
   "outputs": [],
   "source": [
    "!pip install scikit-multilearn\n",
    "!pip install pyevall\n",
    "\n",
    "from typing import Dict, List, Any\n",
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from datetime import datetime\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, multilabel_confusion_matrix\n",
    "from skmultilearn.problem_transform import BinaryRelevance, LabelPowerset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# Import for PyEvALL\n",
    "\n",
    "from pyevall.evaluation import PyEvALLEvaluation\n",
    "from pyevall.utils.utils import PyEvALLUtils\n",
    "\n",
    "\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\"Custom JSON encoder to handle NumPy arrays\"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.bool_):\n",
    "            return bool(obj)\n",
    "        return super(NumpyEncoder, self).default(obj)\n",
    "        \n",
    "\n",
    "### Evaluation metrics\n",
    "def check_matrix(confusion_matrix, gold_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Check and adjust confusion matrix dimensions for proper evaluation.\n",
    "\n",
    "    Parameters:\n",
    "    - confusion_matrix (numpy.ndarray): The confusion matrix to check and potentially adjust\n",
    "    - gold_labels (list or array): The gold standard (true) labels\n",
    "    - predicted_labels (list or array): The predicted labels\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: Properly dimensioned confusion matrix\n",
    "    \"\"\"\n",
    "    if confusion_matrix.size == 1:\n",
    "        tmp = confusion_matrix[0][0]\n",
    "        confusion_matrix = np.zeros((2, 2))\n",
    "        if (predicted_labels[1] == 0):\n",
    "            # true negative\n",
    "            if gold_labels[1] == 0:\n",
    "                confusion_matrix[0][0] = tmp\n",
    "            # false negative\n",
    "            else:\n",
    "                confusion_matrix[1][0] = tmp\n",
    "        else:\n",
    "            # false positive\n",
    "            if gold_labels[1] == 0:\n",
    "                confusion_matrix[0][1] = tmp\n",
    "            # true positive\n",
    "            else:\n",
    "                confusion_matrix[1][1] = tmp\n",
    "    return confusion_matrix\n",
    "\n",
    "def compute_f1(predicted_values, gold_values):\n",
    "    \"\"\"\n",
    "    Compute F1 score based on predicted and gold values.\n",
    "\n",
    "    Parameters:\n",
    "    - predicted_values (list or array): Predicted label values\n",
    "    - gold_values (list or array): Gold standard (true) label values\n",
    "\n",
    "    Returns:\n",
    "    - float: Macro-averaged F1 score (average of positive and negative class F1 scores)\n",
    "    \"\"\"\n",
    "    matrix = metrics.confusion_matrix(gold_values, predicted_values)\n",
    "    matrix = check_matrix(matrix, gold_values, predicted_values)\n",
    "\n",
    "    # Calculate precision and recall for positive label\n",
    "    if matrix[0][0] == 0:\n",
    "        pos_precision = 0.0\n",
    "        pos_recall = 0.0\n",
    "    else:\n",
    "        pos_precision = matrix[0][0] / (matrix[0][0] + matrix[0][1])\n",
    "        pos_recall = matrix[0][0] / (matrix[0][0] + matrix[1][0])\n",
    "\n",
    "    # Calculate F1 for positive label\n",
    "    if (pos_precision + pos_recall) != 0:\n",
    "        pos_F1 = 2 * (pos_precision * pos_recall) / (pos_precision + pos_recall)\n",
    "    else:\n",
    "        pos_F1 = 0.0\n",
    "\n",
    "    # Calculate precision and recall for negative label\n",
    "    neg_matrix = [[matrix[1][1], matrix[1][0]], [matrix[0][1], matrix[0][0]]]\n",
    "\n",
    "    if neg_matrix[0][0] == 0:\n",
    "        neg_precision = 0.0\n",
    "        neg_recall = 0.0\n",
    "    else:\n",
    "        neg_precision = neg_matrix[0][0] / (neg_matrix[0][0] + neg_matrix[0][1])\n",
    "        neg_recall = neg_matrix[0][0] / (neg_matrix[0][0] + neg_matrix[1][0])\n",
    "\n",
    "    # Calculate F1 for negative label\n",
    "    if (neg_precision + neg_recall) != 0:\n",
    "        neg_F1 = 2 * (neg_precision * neg_recall) / (neg_precision + neg_recall)\n",
    "    else:\n",
    "        neg_F1 = 0.0\n",
    "\n",
    "    # Return macro-averaged F1 (average of positive and negative F1 scores)\n",
    "    f1 = (pos_F1 + neg_F1) / 2\n",
    "    return f1\n",
    "\n",
    "def retrieve_label_values(ground_truth, model_submission, field_index):\n",
    "    \"\"\"\n",
    "    Extract specific field values from ground truth and submission dictionaries.\n",
    "\n",
    "    Parameters:\n",
    "    - ground_truth (dict): Dictionary containing ground truth values\n",
    "    - model_submission (dict): Dictionary containing model submission values\n",
    "    - field_index (int): Index of the field to extract\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Lists of extracted ground truth and model submission values\n",
    "    \"\"\"\n",
    "    gold = []\n",
    "    pred = []\n",
    "\n",
    "    for k, v in ground_truth.items():\n",
    "        gold.append(v[field_index])\n",
    "        pred.append(model_submission[k][field_index])\n",
    "\n",
    "    return gold, pred\n",
    "\n",
    "def compute_binary_f1(ground_truth, model_submission):\n",
    "    \"\"\"\n",
    "    Compute F1 score for binary classification task.\n",
    "\n",
    "    Parameters:\n",
    "    - ground_truth (dict): Dictionary containing ground truth values\n",
    "    - model_submission (dict): Dictionary containing model submission values\n",
    "\n",
    "    Returns:\n",
    "    - float: F1 score for binary classification\n",
    "    \"\"\"\n",
    "    gold, pred = retrieve_label_values(ground_truth, model_submission, 0)\n",
    "    score = compute_f1(pred, gold)\n",
    "    return score\n",
    "\n",
    "def compute_multilabel_f1(truth_data, prediction_data, label_count):\n",
    "    \"\"\"\n",
    "    Compute weighted F1 score for multi-label classification task.\n",
    "\n",
    "    Parameters:\n",
    "    - truth_data (dict): Dictionary containing ground truth values\n",
    "    - prediction_data (dict): Dictionary containing model prediction values\n",
    "    - label_count (int): Total number of labels including binary classification label\n",
    "\n",
    "    Returns:\n",
    "    - float: Weighted F1 score for multi-label classification\n",
    "    \"\"\"\n",
    "    score_components = []\n",
    "    occurrence_sum = 0\n",
    "\n",
    "    # Skip first column (index 0) which contains binary classification labels\n",
    "    for label_idx in range(1, label_count):\n",
    "        true_values, predicted_values = retrieve_label_values(truth_data, prediction_data, label_idx)\n",
    "        class_f1 = compute_f1(predicted_values, true_values)\n",
    "        class_weight = true_values.count(True)\n",
    "        occurrence_sum += class_weight\n",
    "        score_components.append(class_f1 * class_weight)\n",
    "\n",
    "    # Return weighted average, handling zero division case\n",
    "    return sum(score_components) / occurrence_sum if occurrence_sum != 0 else 0.0\n",
    "\n",
    "def load_data(filepath):\n",
    "    \"\"\"\n",
    "    Load data from a tab-separated file and convert labels to boolean values.\n",
    "\n",
    "    Parameters:\n",
    "    - filepath (str): Path to the tab-separated data file\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary where keys are the first column values, and values are lists of boolean labels\n",
    "\n",
    "    Raises:\n",
    "    - ValueError: If file has inconsistent or incorrect format\n",
    "    \"\"\"\n",
    "    result_dict = {}\n",
    "    expected_columns = None\n",
    "\n",
    "    with open(filepath) as input_file:\n",
    "        csv_reader = csv.reader(input_file, delimiter='\\t')\n",
    "        line_num = 1\n",
    "\n",
    "        for entry in csv_reader:\n",
    "            if len(entry) < 2:  # ensure at least one label column is present\n",
    "                raise ValueError(f'Wrong number of columns in line {line_num}, expected at least 2.')\n",
    "\n",
    "            if expected_columns and len(entry) != expected_columns:\n",
    "                raise ValueError(f'Inconsistent number of columns in line {line_num}.')\n",
    "\n",
    "            expected_columns = len(entry)\n",
    "            result_dict[entry[0]] = [bool(float(val)) for val in entry[1:]]\n",
    "            line_num += 1\n",
    "\n",
    "    return result_dict\n",
    "\n",
    "def evaluate_f1_scores(gold_label_path, prediction_path, num_labels):\n",
    "    \"\"\"\n",
    "    Evaluate scores for binary and multi-label classification tasks.\n",
    "\n",
    "    Parameters:\n",
    "    - gold_label_path (str): Path to the file containing gold standard labels\n",
    "    - prediction_path (str): Path to the file containing model predictions\n",
    "    - num_labels (int): Total number of labels (2 for binary classification, >2 for multi-label)\n",
    "\n",
    "    Returns:\n",
    "    - float or tuple: Binary score if num_labels=2, otherwise (binary_score, multilabel_score) tuple\n",
    "\n",
    "    Raises:\n",
    "    - ValueError: If submission is missing required keys\n",
    "    \"\"\"\n",
    "\n",
    "    truth = load_data(gold_label_path)\n",
    "    submission = load_data(prediction_path)\n",
    "\n",
    "    # Ensure submission contains all necessary keys\n",
    "    for key in truth.keys():\n",
    "        if key not in submission:\n",
    "            raise ValueError(f'Missing element {key} in submission')\n",
    "\n",
    "    # Compute F1 metric for binary classification\n",
    "    if num_labels == 2:\n",
    "        binary_score = compute_binary_f1(truth, submission)\n",
    "        return binary_score\n",
    "\n",
    "    # Compute F1 for both binary classification and multi-label classification\n",
    "    if num_labels > 2:\n",
    "        binary_score = compute_binary_f1(truth, submission)\n",
    "        multilabel_score = compute_multilabel_f1(truth, submission, num_labels)\n",
    "        return binary_score, multilabel_score\n",
    "\n",
    "### Binary classification\n",
    "\n",
    "def build_bin_classifier(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Create and train an SVM classifier with TF-IDF features.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train (list): List of text documents for training\n",
    "    - y_train (list): Binary labels for training documents\n",
    "\n",
    "    Returns:\n",
    "    - tuple: (svm_model, vectorizer) - Trained model and fitted vectorizer\n",
    "    \"\"\"\n",
    "    # Create TF-IDF features\n",
    "    vectorizer = TfidfVectorizer() # min_df=5,\n",
    "    X_train_features = vectorizer.fit_transform(X_train)\n",
    "\n",
    "    # Train SVM model\n",
    "    svm_model = LinearSVC(max_iter=10000)\n",
    "    svm_model.fit(X_train_features, y_train)\n",
    "\n",
    "    return svm_model, vectorizer\n",
    "\n",
    "def classify_data(X_test, model, vectorizer):\n",
    "    \"\"\"\n",
    "    Classify test data using the trained model.\n",
    "\n",
    "    Parameters:\n",
    "    - X_test (list): List of text documents to classify\n",
    "    - model: Trained classifier model\n",
    "    - vectorizer: Fitted vectorizer for feature extraction\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: Predicted labels for test documents\n",
    "    \"\"\"\n",
    "    # Transform test data\n",
    "    X_test_features = vectorizer.transform(X_test)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_features)\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "### Binary evaluation\n",
    "\n",
    "def evaluate_binary_classification(gold_label_json, predictions_json,\n",
    "                                   y_true, y_pred,\n",
    "                                   gold_labels_txt, predictions_txt,\n",
    "                                   label_names,\n",
    "                                   model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Generate and print comprehensive evaluation metrics for binary classification.\n",
    "\n",
    "    Parameters:\n",
    "    - gold_label_json (str): Path to the file with gold labels in PyEvALL format\n",
    "    - predictions_json (str): Path to the file with predicted labels in PyEvALL format\n",
    "    - y_true (list or array): Gold labels\n",
    "    - y_pred (list or array): Predicted labels\n",
    "    - gold_labels_txt (str): Path to the file with gold labels in txt format for f1 metric\n",
    "    - predictions_txt (str): Path to the file with predictions in txt format for f1 metric\n",
    "    - label_names (list of str): The list of label names corresponding to the binary problem\n",
    "    - model_name (str): Name of the model (default: \"Model\")\n",
    "\n",
    "    Returns:\n",
    "    - None: Results are printed to console and displayed as plots\n",
    "    \"\"\"\n",
    "\n",
    "    y_pred = y_pred.tolist()\n",
    "\n",
    "    # Print classification report with precision, recall, f1, and support metrics\n",
    "    report = classification_report(y_true, y_pred, target_names=label_names, digits=3)\n",
    "    print(f\"{'-'*100}\\nClassification Report for {model_name}:\\n{report}\\n{'-'*100}\")\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    print(f\"{'-'*100}\\nConfusion matrix for {model_name}:\")\n",
    "\n",
    "    # # Print confusion matrix with 3 decimal places (normalized)\n",
    "    # cf_matrix_normalized = cf_matrix.astype('float') / cf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "    # print(\"Normalized Confusion Matrix (3 decimal places):\")\n",
    "    # for i, row in enumerate(cf_matrix_normalized):\n",
    "    #     formatted_row = [f\"{val:.3f}\" for val in row]\n",
    "    #     print(f\"{label_names[i]:>15}: {formatted_row}\")\n",
    "    # print()\n",
    "\n",
    "    # Print raw confusion matrix\n",
    "    print(\"Raw Confusion Matrix:\")\n",
    "    for i, row in enumerate(cf_matrix):\n",
    "        print(f\"{label_names[i]:>15}: {row.tolist()}\")\n",
    "    print()\n",
    "\n",
    "    # Display confusion matrix as a heatmap with custom formatting\n",
    "    disp = ConfusionMatrixDisplay(cf_matrix, display_labels=label_names)\n",
    "\n",
    "    # Create figure with custom formatting for 3 decimal places\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Plot with custom text formatting\n",
    "    disp.plot(cmap=plt.cm.Greens, ax=ax, values_format='.3f' if np.any(cf_matrix < 1) else 'd')\n",
    "\n",
    "    # If you want to show both raw counts and percentages with 3 decimals\n",
    "    # Calculate percentages\n",
    "    cf_matrix_percent = cf_matrix.astype('float') / cf_matrix.sum(axis=1)[:, np.newaxis] * 100\n",
    "\n",
    "\n",
    "    plt.title(f\"Confusion Matrix - {model_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(f\"{'-'*100}\")\n",
    "\n",
    "    # PyEvALL evaluation metrics\n",
    "    print(f\"{'-'*100}\\nPyEvaLL Metrics for {model_name}:\\n\")\n",
    "    evaluator = PyEvALLEvaluation()\n",
    "    evaluation_params = dict()\n",
    "    evaluation_params[PyEvALLUtils.PARAM_REPORT] = PyEvALLUtils.PARAM_OPTION_REPORT_DATAFRAME\n",
    "    metric_list = [\"ICM\", \"ICMNorm\", \"FMeasure\"]\n",
    "    evaluation_report = evaluator.evaluate(predictions_json, gold_label_json, metric_list, **evaluation_params)\n",
    "    evaluation_report.print_report()\n",
    "    print(f\"{'-'*100}\")\n",
    "\n",
    "    # MAMI F1 metric (macro-f1 for binary classification)\n",
    "    print(f\"{'-'*100}\\n F1 Metrics for {model_name}:\\n\")\n",
    "    n_labels = 2\n",
    "\n",
    "    # Compute binary classification macro-F1 score\n",
    "    score_bin = evaluate_f1_scores(gold_labels_txt, predictions_txt, n_labels)\n",
    "    print(f\"Binary classification macro-F1 score: {score_bin:.3f}\")\n",
    "    print(f\"{'-'*100}\")\n",
    "\n",
    "    # Get structured classification report\n",
    "    class_report_dict = classification_report(y_true, y_pred,\n",
    "                                            target_names=label_names,\n",
    "                                            zero_division=0,\n",
    "                                            digits=3,\n",
    "                                            output_dict=True)\n",
    "\n",
    "    # Extract confusion matrix\n",
    "    cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Calculate binary F1 score\n",
    "    score_bin = evaluate_f1_scores(gold_labels_txt, predictions_txt, n_labels)\n",
    "\n",
    "    # Return structured results\n",
    "    results = {\n",
    "        'binary_f1': score_bin,\n",
    "        'macro_f1': class_report_dict['macro avg']['f1-score'],\n",
    "        'per_label_metrics': class_report_dict,\n",
    "        'confusion_matrix': cf_matrix,\n",
    "        'label_names': label_names\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Multi-label classification\n",
    "\n",
    "def build_multilabel_classifier(X_train, y_train, transform_strategy):\n",
    "    \"\"\"\n",
    "    Create and train a multi-label text classification model using SVM with the specified strategy.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train (list of str): Training text data\n",
    "    - y_train (list): Multi-label training labels\n",
    "    - transform_strategy (skmultilearn model wrapper): Multi-label classification strategy (BinaryRelevance/LabelPowerset)\n",
    "\n",
    "    Returns:\n",
    "    - tuple: (ml_model, vectorizer) - Trained multi-label model and fitted vectorizer\n",
    "    \"\"\"\n",
    "\n",
    "    # Create TF-IDF vectorizer with NLTK tokenization\n",
    "    vectorizer = TfidfVectorizer() # min_df=5\n",
    "    X_train_features = vectorizer.fit_transform(X_train)\n",
    "\n",
    "    # Configure multi-label model with LinearSVC base classifier\n",
    "    ml_model = transform_strategy(LinearSVC(max_iter=10000))\n",
    "    # Train the model\n",
    "    ml_model.fit(X_train_features, y_train)\n",
    "\n",
    "    return ml_model, vectorizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_hierarchical_multilabel_classifier(all_training_texts, binary_labels,\n",
    "                                             positive_subset_texts,category_labels,\n",
    "                                             test_texts, binary_label_name,\n",
    "                                             category_label_names, strategy_class=BinaryRelevance):\n",
    "    \"\"\"\n",
    "    Train a hierarchical classification model with two stages: binary and multi-label classification.\n",
    "\n",
    "    This approach first classifies instances as positive/negative (e.g., misogynous/non-misogynous),\n",
    "    then applies fine-grained classification only to positive instances.\n",
    "\n",
    "    Parameters:\n",
    "    - all_training_texts (list): All training text instances\n",
    "    - binary_labels (list): Binary labels for all training instances\n",
    "    - positive_subset_texts (list): Text instances with positive binary labels only\n",
    "    - category_labels (list): Fine-grained category labels for positive instances\n",
    "    - test_texts (list): Texts for evaluation\n",
    "    - binary_label_name (str): Name of the binary label column\n",
    "    - category_label_names (list): Names of fine-grained category labels\n",
    "    - strategy_class: Multi-label classification strategy (default: BinaryRelevance)\n",
    "\n",
    "    Returns:\n",
    "    - tuple: (predictions_df, bin_model, bin_vec, ml_model, ml_vec)\n",
    "            where:\n",
    "            - predictions_df: DataFrame containing predictions for both binary and fine-grained labels\n",
    "            - bin_model: Trained binary classification model\n",
    "            - bin_vec: Text vectorizer for binary classification\n",
    "            - ml_model: Trained multi-label classification model (None if no positive instances)\n",
    "            - ml_vec: Text vectorizer for multi-label classification (None if no positive instances)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # First build binary model to predict positive instances (misogynous/sexist)\n",
    "    bin_model, bin_vec  = build_bin_classifier(all_training_texts, binary_labels)\n",
    "    binary_predictions = classify_data(test_texts, bin_model, bin_vec )\n",
    "\n",
    "    # Filter positive instances （misogynous/sexist） for fine-grained classification\n",
    "    positive_test_texts = pd.DataFrame(test_texts)[binary_predictions == 1][0].tolist()\n",
    "\n",
    "    # Initialize predictions DataFrame with binary labels\n",
    "    # Default all fine-grained labels to 0\n",
    "    pred_df = pd.DataFrame({binary_label_name: binary_predictions})\n",
    "    pred_df[category_label_names] = 0\n",
    "\n",
    "    # Build multi-label model for fine-grained classification if there are positive instances\n",
    "    if len(positive_test_texts) > 0:\n",
    "        ml_model, ml_vec = build_multilabel_classifier(\n",
    "            positive_subset_texts, category_labels, strategy_class)\n",
    "\n",
    "        # Apply fine-grained classification only to positive instances\n",
    "        multilabel_predictions = classify_data(\n",
    "            positive_test_texts, ml_model, ml_vec)\n",
    "\n",
    "        # Add fine-grained labels to positive instances in the predictions DataFrame\n",
    "        pred_df.loc[binary_predictions == 1, category_label_names] = multilabel_predictions.toarray()\n",
    "    else:\n",
    "        # If no positive instances, create empty models (to maintain return structure)\n",
    "        ml_model, ml_vec = None, None\n",
    "\n",
    "    return pred_df, bin_model, bin_vec , ml_model, ml_vec\n",
    "\n",
    "\n",
    "\n",
    "### Multi-label evaluation\n",
    "\n",
    "def evaluate_multilabel_classification(gold_label_json, predictions_json,\n",
    "                                       y_true, y_pred,\n",
    "                                       gold_labels_txt, predictions_txt,\n",
    "                                       label_names,\n",
    "                                       hierarchy=True):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of a multi-label classification model with comprehensive metrics.\n",
    "\n",
    "    Parameters:\n",
    "    - gold_label_json (str): Path to the file with gold labels in PyEvALL format\n",
    "    - predictions_json (str): Path to the file with predicted labels in PyEvALL format\n",
    "    - y_true (array-like): Gold binary labels (multi-label binary matrix) for the test set\n",
    "    - y_pred (array-like): Predicted binary labels (multi-label binary matrix) for the test set\n",
    "    - label_names (list of str): The list of label names corresponding to the multi-label problem\n",
    "    - gold_labels_txt (str): Path to the file with gold labels in txt format for MAMI f1 metric\n",
    "    - predictions_txt (str): Path to the file with predictions in txt format for MAMI f1 metric\n",
    "    - hierarchy (bool): Whether the evaluation considers hierarchical evaluation (first binary\n",
    "                       classification and then multi-label). Default is True.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Evaluation results including F1 scores and per-label metrics\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Convert inputs to numpy arrays if they aren't already\n",
    "    y_pred_array = y_pred.toarray() if not isinstance(y_pred, np.ndarray) else y_pred\n",
    "    y_true_array = np.array(y_true) if not isinstance(y_true, np.ndarray) else y_true\n",
    "\n",
    "    # Process binary classification data (first column)\n",
    "    binary_true = y_true_array[:, 0]\n",
    "    binary_pred = y_pred_array[:, 0]\n",
    "\n",
    "    # Create negative class representation matrices\n",
    "    negative_true = np.zeros((len(y_true_array), 1))\n",
    "    negative_pred = np.zeros((len(y_pred_array), 1))\n",
    "\n",
    "    # Set values for negative class (inverse of binary classification)\n",
    "    negative_true[:, 0] = (binary_true == 0) #~binary_true.astype(bool)\n",
    "    negative_pred[:, 0] = (binary_pred == 0) #~binary_pred.astype(bool)\n",
    "\n",
    "    # Get multi-label classification data (all columns except first)\n",
    "    multilabel_true = y_true_array[:, 1:]\n",
    "    multilabel_pred = y_pred_array[:, 1:]\n",
    "\n",
    "    # Create combined representation with negative class and multi-labels\n",
    "    combined_true = np.hstack((negative_true, multilabel_true))\n",
    "    combined_pred = np.hstack((negative_pred, multilabel_pred))\n",
    "\n",
    "    # Update label names to include negative class\n",
    "    updated_labels = [f\"non-{label_names[0]}\"] + label_names[1:]\n",
    "\n",
    "    total_labels = len(updated_labels)\n",
    "\n",
    "    # Print classification metrics with 3 decimal places\n",
    "    print(f\"{'-'*100}\\nClassification Report:\")\n",
    "    class_report = classification_report(combined_true, combined_pred,\n",
    "                                        target_names=updated_labels,\n",
    "                                        zero_division=0,\n",
    "                                        digits=3)\n",
    "    print(f\"{class_report}\\n{'-'*100}\")\n",
    "\n",
    "    # GET STRUCTURED DATA: classification report as dictionary\n",
    "    class_report_dict = classification_report(combined_true, combined_pred,\n",
    "                                            target_names=updated_labels,\n",
    "                                            zero_division=0,\n",
    "                                            digits=3,\n",
    "                                            output_dict=True)  # Return as dictionary\n",
    "\n",
    "    # Generate and display confusion matrices (text only)\n",
    "    print(f\"{'-'*100}\\nConfusion matrices:\")\n",
    "    confusion_matrices = multilabel_confusion_matrix(combined_true, combined_pred)\n",
    "\n",
    "\n",
    "\n",
    "    # Print each confusion matrix with 3 decimal places\n",
    "    for idx, matrix in enumerate(confusion_matrices):\n",
    "        # Print numeric values with 3 decimal places for normalized matrix\n",
    "        print(f\"Confusion Matrix for '{updated_labels[idx]}' label:\")\n",
    "        print(\"Raw Matrix:\")\n",
    "        print(matrix)\n",
    "\n",
    "        # Calculate and print normalized matrix with 3 decimal places\n",
    "        if matrix.sum() > 0:\n",
    "            matrix_normalized = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]\n",
    "            print(\"Normalized Matrix (3 decimal places):\")\n",
    "            for i, row in enumerate(matrix_normalized):\n",
    "                formatted_row = [f\"{val:.3f}\" for val in row]\n",
    "                print(f\"  {['False', 'True'][i]:>5}: {formatted_row}\")\n",
    "        print()\n",
    "\n",
    "    # Run PyEvALL evaluation\n",
    "    print(f\"{'-'*100}\\nPyEvaLL Metrics:\\n\")\n",
    "    evaluator = PyEvALLEvaluation()\n",
    "    evaluation_params = dict()\n",
    "\n",
    "    # Configure hierarchical evaluation if needed\n",
    "    if hierarchy:\n",
    "        label_hierarchy = {\"yes\": label_names, \"no\":[]}\n",
    "        evaluation_params[PyEvALLUtils.PARAM_HIERARCHY] = label_hierarchy\n",
    "        metric_list = [\"ICM\", \"ICMNorm\", \"FMeasure\"]\n",
    "\n",
    "    else:\n",
    "        metric_list = [\"FMeasure\"]\n",
    "\n",
    "\n",
    "    # Set report format\n",
    "    evaluation_params[PyEvALLUtils.PARAM_REPORT] = PyEvALLUtils.PARAM_OPTION_REPORT_DATAFRAME\n",
    "\n",
    "    # Run evaluation and print results\n",
    "    evaluation_report = evaluator.evaluate(gold_label_json, predictions_json, metric_list, **evaluation_params)\n",
    "    evaluation_report.print_report()\n",
    "    print(f\"{'-'*100}\")\n",
    "\n",
    "\n",
    "    # Calculate F1 metrics with 3 decimal places\n",
    "    bin_score, ml_score = evaluate_f1_scores(gold_labels_txt, predictions_txt, total_labels)\n",
    "\n",
    "    print(f\"Binary classification macro-F1 score: {bin_score:.3f}\")\n",
    "    print(f\"Multi-label classification weighted-F1 score: {ml_score:.3f}\")\n",
    "    print(f\"{'-'*100}\")\n",
    "\n",
    "    # RETURN STRUCTURED RESULTS\n",
    "    results = {\n",
    "        'binary_f1': bin_score,\n",
    "        'macro_f1': class_report_dict['macro avg']['f1-score'],\n",
    "        'multilabel_f1': ml_score,\n",
    "        'per_label_metrics': class_report_dict,\n",
    "        'confusion_matrices': confusion_matrices,\n",
    "        'label_names': updated_labels,\n",
    "        'original_label_names': label_names\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Convert predictions to PyEvALL format\n",
    "\n",
    "def format_pred_for_pyevall(df, binary_label, labels, test_case, eval_type, pred_label):\n",
    "    \"\"\"\n",
    "    Convert a DataFrame of labeled meme dataset into a format suitable for PyEvALLEvaluation.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): A DataFrame containing meme id and associated labels\n",
    "    - binary_label (str): The name of the binary label column in the DataFrame (sexist or misogynous)\n",
    "    - labels (list): A list of column names representing the labels for evaluation in the dataset\n",
    "    - test_case (str): The test case identifier to be added as a new column, e.g. \"MAMI\" or \"EXIST2024\"\n",
    "    - eval_type (str): The type of evaluation format to be used:\n",
    "        - \"binary\": For binary classification\n",
    "        - \"hierarchical\": For multi-label classification\n",
    "    - pred_label (np.ndarray): A NumPy array containing predicted labels\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of dictionaries formatted for PyEvALLEvaluation with structure:\n",
    "        - test_case: The name of the dataset\n",
    "        - id: The meme id\n",
    "        - value: Labels in format appropriate for the evaluation type\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert files to input required by PyEvALLEvaluation\n",
    "    pred_labels = df[[\"meme id\"]].copy()\n",
    "\n",
    "    # Add the test_case column as per the library requirements\n",
    "    pred_labels.insert(0, \"test_case\", [test_case] * (len(pred_labels)), True)\n",
    "\n",
    "    if eval_type == \"binary\":\n",
    "\n",
    "        # Format binary labels\n",
    "        pred_labels[\"value\"] = pred_label\n",
    "        binary_labels = pred_labels\n",
    "\n",
    "        # Convert values to \"yes\" and \"no\" as required by PyEvALL\n",
    "        binary_labels = pred_labels.replace({\"value\":0}, \"no\").replace({\"value\":1}, \"yes\")\n",
    "\n",
    "        # Rename the id column to match requirements\n",
    "        binary_labels.rename(columns={\"meme id\": \"id\"}, inplace=True)\n",
    "\n",
    "        # Convert \"id\" column to string values\n",
    "        binary_labels[\"id\"] = binary_labels[\"id\"].astype(str)\n",
    "        labels_df = binary_labels\n",
    "\n",
    "    elif eval_type == \"hierarchical\":\n",
    "\n",
    "        # Format hierarchical multi-label data\n",
    "        multilabel_labels = pred_labels[[\"test_case\", \"meme id\"]].reset_index(drop=True)\n",
    "\n",
    "        # Concatenate with dataset df along columns (axis=1)\n",
    "        multilabel_labels = pd.concat([multilabel_labels, pred_label], axis=1)\n",
    "\n",
    "        # Extract only fine-grained category columns\n",
    "        value_cols = multilabel_labels.columns[2:]\n",
    "\n",
    "        # Create value column with list of labels where value is 1\n",
    "        multilabel_labels[\"value\"] = multilabel_labels[value_cols].apply(\n",
    "            lambda row: [label for i, label in enumerate(labels[1:]) if row.iloc[i]] or [\"no\"], axis=1)\n",
    "        multilabel_labels = multilabel_labels[[\"test_case\", \"meme id\", \"value\"]]\n",
    "\n",
    "        # Rename id column to match PyEvALL requirements\n",
    "        multilabel_labels.rename(columns={\"meme id\": \"id\"}, inplace=True)\n",
    "\n",
    "        # Convert \"id\" column to string values\n",
    "        multilabel_labels[\"id\"] = multilabel_labels[\"id\"].astype(str)\n",
    "        labels_df = multilabel_labels\n",
    "\n",
    "    # Convert DataFrame to list of dictionaries as required by PyEvALL\n",
    "    labels_list = labels_df.to_dict(orient=\"records\")\n",
    "\n",
    "    return labels_list\n",
    "\n",
    "\n",
    "### Convert predictions to MAMI f1 evaluation format\n",
    "\n",
    "def format_pred_for_mami_f1(df, eval_type, pred_label):\n",
    "    \"\"\"\n",
    "    Transform a DataFrame containing meme data into the format required by MAMI evaluation framework.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): DataFrame with meme identifiers and attribute data\n",
    "    - eval_type (str): Specifies the evaluation approach:\n",
    "        - \"binary\": Used for simple positive/negative classification\n",
    "        - \"hierarchical\": Used for multi-level category classification\n",
    "    - pred_label (np.ndarray or pd.DataFrame): Classification results from model prediction\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: Properly formatted data structure with IDs and classification values\n",
    "    \"\"\"\n",
    "    # Extract just the identifier column to a new DataFrame\n",
    "    pred_labels = df[[\"meme id\"]].copy()\n",
    "\n",
    "    if eval_type == \"binary\":\n",
    "        # Binary case: simply attach prediction vector as a value column\n",
    "        pred_labels[\"value\"] = pred_label\n",
    "\n",
    "    else:\n",
    "        # Handle multi-label scenario by first ensuring array format\n",
    "        if not isinstance(pred_label, np.ndarray):\n",
    "            pred_label = pred_label.toarray()\n",
    "        # Transform array into structured DataFrame\n",
    "        pred_df = pd.DataFrame(pred_label)\n",
    "        # Normalize index sequencing for proper alignment\n",
    "        pred_labels = pred_labels.reset_index(drop=True)\n",
    "        # Merge identifier column with prediction matrix\n",
    "        pred_labels = pd.concat([pred_labels, pred_df], axis=1)\n",
    "\n",
    "    # Standardize identifier type to string format\n",
    "    pred_labels[\"meme id\"] = pred_labels[\"meme id\"].astype(str)\n",
    "\n",
    "    return pred_labels\n",
    "\n",
    "\n",
    "\n",
    "### Save predictions for evaluation\n",
    "\n",
    "def write_labels_to_json(label_list, output_file, dataset_name, split_name, eval_type):\n",
    "    \"\"\"\n",
    "    Write labels to JSON file for PyEvALL Evaluation.\n",
    "\n",
    "    Parameters:\n",
    "    - label_list (list): List of dictionaries containing test case, meme id and labels\n",
    "    - output_file (str): Path to the output JSON file\n",
    "    - dataset_name (str): Name of the dataset (e.g., MAMI, EXIST2024)\n",
    "    - split_name (str): Name of the data split (e.g., training, test)\n",
    "    - eval_type (str): Type of evaluation (binary, flat, hierarchical)\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(label_list, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Saved {dataset_name} {split_name} split {eval_type} evaluation to {output_file}\")\n",
    "\n",
    "\n",
    "def write_labels_to_txt(labels_df, output_path, dataset_name, split_name):\n",
    "    \"\"\"\n",
    "    Write labels to tab-separated text file for MAMI Evaluation.\n",
    "\n",
    "    Parameters:\n",
    "    - labels_df (pandas.DataFrame): DataFrame containing meme id and associated labels\n",
    "    - output_path (str): Path to the output text file\n",
    "    - dataset_name (str): Name of the dataset (e.g., MAMI, EXIST2024)\n",
    "    - split_name (str): Name of the data split (e.g., training, test)\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    labels_df.to_csv(output_path, index=False, sep='\\t', header=False)\n",
    "\n",
    "    print(f\"Saved {dataset_name} {split_name} split to {output_path}\")\n",
    "\n",
    "\n",
    "def save_evaluation(df, pred_dir, dataset_name, split_name, eval_type, model_name, predictions, binary_label, labels):\n",
    "    \"\"\"\n",
    "    Store model evaluation data in structured format files and return their paths.\n",
    "\n",
    "    Exports prediction results to both JSON (PyEvALL compatible) and TXT (MAMI F1 compatible) formats.\n",
    "\n",
    "    Parameter:\n",
    "    - df (pandas.DataFrame): Source dataset containing meme identifiers and ground truth labels\n",
    "    - pred_dir (str): Target location for storing output files\n",
    "    - dataset_name (str): Identifier for the evaluation corpus (e.g., \"MAMI\", \"EXIST2024\")\n",
    "    - split_name (str): Partition identifier (e.g., \"train\", \"dev\", \"test\")\n",
    "    - eval_type (str): Classification approach used (\"binary\", \"hierarchical\", \"flat\")\n",
    "    - model_name (str): Classifier identifier for file naming\n",
    "    - predictions (np.ndarray): Model output predictions matrix\n",
    "    - binary_label (str): Primary category field name in the dataset\n",
    "                         Used for yes/no categorization in certain evaluation types\n",
    "    - labels (list): Field identifiers for all classification dimensions\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_name = \"EXIST2024\" if dataset_name == \"EXIST\" else dataset_name\n",
    "\n",
    "    # Construct nested directory path for this specific dataset\n",
    "    output_directory = os.path.join(pred_dir, dataset_name)\n",
    "\n",
    "    # Ensure storage location exists\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    # Generate PyEvALL-compatible representation\n",
    "    prediction_records = format_pred_for_pyevall(\n",
    "        df, binary_label, labels, dataset_name, eval_type, predictions\n",
    "    )\n",
    "\n",
    "    # Construct JSON output path and persist data\n",
    "    json_filepath = os.path.join(\n",
    "        output_directory,\n",
    "        f\"{model_name}_{dataset_name}_{split_name}_{eval_type}.json\"\n",
    "    )\n",
    "    write_labels_to_json(\n",
    "        prediction_records, json_filepath, dataset_name, split_name, eval_type\n",
    "    )\n",
    "\n",
    "    # Handle DataFrame predictions by converting to numpy array\n",
    "    prediction_data = predictions\n",
    "    if isinstance(predictions, pd.DataFrame):\n",
    "        prediction_data = predictions.to_numpy()\n",
    "\n",
    "    # Generate MAMI F1 compatible representation\n",
    "    mami_format_data = format_pred_for_mami_f1(df, eval_type, prediction_data)\n",
    "\n",
    "    # Construct TXT output path and persist data\n",
    "    txt_filepath = os.path.join(\n",
    "        output_directory,\n",
    "        f\"{model_name}_{dataset_name}_{split_name}_answer.txt\"\n",
    "    )\n",
    "    write_labels_to_txt(mami_format_data, txt_filepath, dataset_name, split_name)\n",
    "\n",
    "    # Return both file paths for reference\n",
    "    return json_filepath, txt_filepath\n",
    "\n",
    "def save_predictions_csv(test_df, predictions, column_names, output_file):\n",
    "    \"\"\"\n",
    "    Write predictions to a CSV file by adding new columns per predicted label(s).\n",
    "\n",
    "    Parameters:\n",
    "    - test_df (pd.DataFrame): DataFrame containing the original test instances and gold labels.\n",
    "    - predictions (list or np.array): Model predictions.\n",
    "    - column_names (list): A list of column names representing the labels for evaluation in the dataset.\n",
    "    - output_file (str): Path to the output CSV file.\n",
    "    \"\"\"\n",
    "    #convert predicted labels array to df with labels as column names + _prediction\n",
    "    pred_df = pd.DataFrame(predictions, columns=[f\"{col}_prediction\" for col in column_names])\n",
    "\n",
    "    #drop index to merge with predictions df\n",
    "    test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "    #save predictions while keeping original labels\n",
    "    pred_df = pd.concat([test_df, pred_df], axis=1)\n",
    "\n",
    "    #save updated file\n",
    "    pred_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Predictions saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JB7Xg46Rqvjw"
   },
   "source": [
    "## **utils_experiments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A-gfGrz1VN-C"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "\n",
    "# Load NRC lexicon for emotion and sentiment analysis\n",
    "\n",
    "def load_nrc_lexicon(path_nrc):\n",
    "    \"\"\"\n",
    "    Load the NRC Emotion Lexicon from a file.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    path_nrc : str\n",
    "        Path to the NRC Emotion Lexicon file (NRC-Emotion-Lexicon-Wordlevel-v0.92.txt)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        A dictionary where:\n",
    "        - Keys are words from the lexicon\n",
    "        - Values are lists of emotions/sentiments associated with each word\n",
    "          (only those with a value of 1 in the lexicon)\n",
    "    \"\"\"\n",
    "\n",
    "    lexicon = {}\n",
    "    with open(path_nrc, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if len(line.strip().split('\\t')) == 3:\n",
    "                word, emotion, value = line.strip().split('\\t')\n",
    "                if int(value) == 1:\n",
    "                    if word not in lexicon:\n",
    "                        lexicon[word] = []\n",
    "                    lexicon[word].append(emotion)\n",
    "    return lexicon\n",
    "\n",
    "\n",
    "\n",
    "# Load HurtLex for hate speech terms\n",
    "\n",
    "def load_hurtlex(file_path):\n",
    "    \"\"\"\n",
    "    Load HurtLex lexicon from TSV file.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): Path to the HurtLex TSV file\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary mapping words to their hate speech categories\n",
    "    \"\"\"\n",
    "    hurtlex = {}\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            # Skip header\n",
    "            header = next(f)\n",
    "\n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t')\n",
    "                # Based on your table: id, pos, category, stereotype, lemma, level\n",
    "                if len(parts) >= 6:\n",
    "                    id_val = parts[0]\n",
    "                    pos = parts[1]\n",
    "                    category = parts[2]  # This is the actual category code (om, qas, etc.)\n",
    "                    stereotype = parts[3]\n",
    "                    lemma = parts[4]\n",
    "                    level = parts[5]\n",
    "\n",
    "                    if lemma not in hurtlex:\n",
    "                        hurtlex[lemma] = []\n",
    "\n",
    "                    # Add the category to the word's list of categories\n",
    "                    hurtlex[lemma].append(category)\n",
    "\n",
    "        return hurtlex\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Could not find HurtLex file at {file_path}\")\n",
    "        return {}\n",
    "\n",
    "# Load function words dictionary\n",
    "\n",
    "def load_function_words():\n",
    "    \"\"\"\n",
    "    Load a comprehensive dictionary of function words based on linguistic closed class categories.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        A dictionary where:\n",
    "        - Keys are closed class categories\n",
    "        - Values are lists of words belonging to each category\n",
    "    \"\"\"\n",
    "    function_words = {\n",
    "        'determiners': ['the', 'a', 'an', 'this', 'that', 'these', 'those', 'my', 'your', 'his', 'her', 'its',\n",
    "                        'our', 'their', 'any', 'each', 'every', 'some', 'all', 'both', 'either', 'neither',\n",
    "                        'few', 'many', 'much', 'several', 'more', 'most', 'less', 'no', 'enough', 'which', 'what', 'whose'],\n",
    "\n",
    "        'pronouns': ['i', 'me', 'my', 'mine', 'myself', 'you', 'your', 'yours', 'yourself', 'yourselves',\n",
    "                    'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n",
    "                    'we', 'us', 'our', 'ours', 'ourselves', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "                    'who', 'whom', 'whose', 'which', 'what', 'this', 'that', 'these', 'those',\n",
    "                    'anybody', 'somebody', 'nobody', 'everybody', 'anyone', 'someone', 'no one', 'everyone',\n",
    "                    'each', 'either', 'neither', 'one', 'all', 'some', 'many', 'few', 'any', 'none', 'both'],\n",
    "\n",
    "        'prepositions': ['of', 'at', 'in', 'on', 'without', 'between', 'under', 'over', 'beside', 'through',\n",
    "                        'during', 'among', 'across', 'against', 'towards', 'around', 'before', 'after',\n",
    "                        'along', 'behind', 'below', 'beyond', 'despite', 'except', 'from', 'inside', 'near',\n",
    "                        'onto', 'outside', 'past', 'since', 'till', 'until', 'upon', 'within', 'about', 'above',\n",
    "                        'beneath', 'beside', 'by', 'down', 'into', 'like', 'off', 'out', 'throughout', 'to',\n",
    "                        'toward', 'underneath', 'unto', 'up', 'with', 'without', 'regarding', 'round'],\n",
    "\n",
    "        'conjunctions': ['and', 'but', 'or', 'nor', 'for', 'yet', 'so', 'although', 'because', 'since',\n",
    "                        'unless', 'until', 'when', 'while', 'whereas', 'after', 'before', 'if', 'then',\n",
    "                        'even though', 'though', 'as long as', 'provided that', 'however', 'therefore',\n",
    "                        'thus', 'moreover', 'nevertheless'],\n",
    "\n",
    "        'auxiliary_verbs': ['am', 'is', 'are', 'was', 'were', 'be', 'being', 'been',\n",
    "                           'have', 'has', 'had', 'having',\n",
    "                           'do', 'does', 'did',\n",
    "                           'will', 'would', 'shall', 'should', 'can', 'could', 'may', 'might', 'must', 'ought'],\n",
    "\n",
    "        'enumerators': ['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten',\n",
    "                        'first', 'second', 'third', 'fourth', 'fifth', 'sixth', 'seventh', 'eighth', 'ninth', 'tenth',\n",
    "                        'once', 'twice', 'thrice'],\n",
    "\n",
    "        'particles': ['no', 'not', 'nor', 'as', 'to', 'up', 'out', 'off', 'down', 'about',\n",
    "                     'around', 'aside', 'away', 'back', 'apart'],\n",
    "\n",
    "        'qualifiers': ['very', 'really', 'quite', 'somewhat', 'rather', 'too', 'pretty', 'fairly', 'slightly',\n",
    "                       'almost', 'nearly', 'barely', 'hardly', 'scarcely', 'completely', 'absolutely', 'totally',\n",
    "                       'utterly', 'extremely', 'especially', 'particularly', 'specifically'],\n",
    "\n",
    "\n",
    "        'interjections': ['oh', 'ah', 'ugh', 'hey', 'oops', 'gadzooks', 'wow', 'ouch', 'eh', 'hmm']\n",
    "    }\n",
    "\n",
    "    return function_words\n",
    "\n",
    "\n",
    "def get_all_categories(hurtlex_dict):\n",
    "    \"\"\"Get all unique categories in the lexicon\"\"\"\n",
    "    categories = set()\n",
    "    for word, cats in hurtlex_dict.items():\n",
    "        for cat in cats:\n",
    "            categories.add(cat)\n",
    "    return sorted(list(categories))\n",
    "\n",
    "def get_words_by_category(hurtlex_dict, category):\n",
    "    \"\"\"Get all words that belong to a specific category\"\"\"\n",
    "    words = []\n",
    "    for word, cats in hurtlex_dict.items():\n",
    "        if category in cats:\n",
    "            words.append(word)\n",
    "    return words\n",
    "\n",
    "def count_words_by_category(hurtlex_dict):\n",
    "    \"\"\"Count how many words are in each category\"\"\"\n",
    "    categories = get_all_categories(hurtlex_dict)\n",
    "    counts = {}\n",
    "    for cat in categories:\n",
    "        counts[cat] = len(get_words_by_category(hurtlex_dict, cat))\n",
    "    return counts\n",
    "\n",
    "def clean_tokens(tokens):\n",
    "    return [token.strip(string.punctuation) for token in tokens]\n",
    "\n",
    "\n",
    "\n",
    "def extract_emotion_words(text, nrc_lexicon, emotion_category):\n",
    "    \"\"\"\n",
    "    Extract words in text belonging to a specific emotion category.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        The text to analyze\n",
    "    nrc_lexicon : dict\n",
    "        The NRC emotion lexicon\n",
    "    emotion_category : str\n",
    "        Emotion category to extract (e.g., 'anger', 'joy')\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        List of words in text that belong to the emotion category\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokens = clean_tokens(tokens)\n",
    "    words = [w for w in tokens if w not in string.punctuation]\n",
    "    emotion_words = []\n",
    "\n",
    "    for word in words:\n",
    "        if word in nrc_lexicon and emotion_category in nrc_lexicon[word]:\n",
    "            emotion_words.append(word)\n",
    "\n",
    "    return emotion_words\n",
    "\n",
    "def extract_sentiment_words(text, nrc_lexicon, sentiment_type):\n",
    "    \"\"\"\n",
    "    Extract positive or negative sentiment words from text.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        The text to analyze\n",
    "    nrc_lexicon : dict\n",
    "        The NRC emotion lexicon\n",
    "    sentiment_type : str\n",
    "        Either 'positive' or 'negative'\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        List of words in text that have the specified sentiment\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokens = clean_tokens(tokens)\n",
    "    words = [w for w in tokens if w not in string.punctuation]\n",
    "    sentiment_words = []\n",
    "\n",
    "    for word in words:\n",
    "        if word in nrc_lexicon and sentiment_type in nrc_lexicon[word]:\n",
    "            sentiment_words.append(word)\n",
    "\n",
    "    return sentiment_words\n",
    "\n",
    "def extract_function_words(text, function_words_dict, function_category):\n",
    "    \"\"\"\n",
    "    Extract function words from a specific category.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        The text to analyze\n",
    "    function_words_dict : dict\n",
    "        Dictionary of function word categories\n",
    "    function_category : str\n",
    "        Category of function words to extract\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        List of function words found in the text from the specified category\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokens = clean_tokens(tokens)\n",
    "    words = [w for w in tokens if w not in string.punctuation]\n",
    "    category_words = []\n",
    "\n",
    "    if function_category in function_words_dict:\n",
    "        for word in words:\n",
    "            if word in function_words_dict[function_category]:\n",
    "                category_words.append(word)\n",
    "\n",
    "    return category_words\n",
    "\n",
    "def extract_hate_speech_terms(text, hurtlex_dict, hurtlex_category):\n",
    "    \"\"\"\n",
    "    Extract hate speech terms from a specific HurtLex category.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        The text to analyze\n",
    "    hurtlex_dict : dict\n",
    "        The HurtLex dictionary\n",
    "    hurtlex_category : str\n",
    "        HurtLex category to extract (e.g., 'ps', 'om')\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        List of words in text that belong to the specified HurtLex category\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokens = clean_tokens(tokens)\n",
    "    words = [w for w in tokens if w not in string.punctuation]\n",
    "    hate_terms = []\n",
    "\n",
    "    for word in words:\n",
    "        if word in hurtlex_dict and hurtlex_category in hurtlex_dict[word]:\n",
    "            hate_terms.append(word)\n",
    "\n",
    "    return hate_terms\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def replace_mask_features(text, feature_words, mask_token=\"[MASK]\"):\n",
    "    \"\"\"\n",
    "    Replace specified features with a mask token.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        The text to process\n",
    "    feature_words : list\n",
    "        List of words to mask\n",
    "    mask_token : str\n",
    "        Token to replace feature words with\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Text with specified words masked\n",
    "    tuple\n",
    "        Count of words masked\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokens = clean_tokens(tokens)\n",
    "    words = [w for w in tokens if w not in string.punctuation]\n",
    "\n",
    "    feature_words_lower = set(word.lower() for word in feature_words)\n",
    "    masked_count = 0\n",
    "    masked_words = []\n",
    "\n",
    "    for word in words:\n",
    "        if word.lower() in feature_words_lower:\n",
    "            masked_words.append(mask_token)\n",
    "            masked_count += 1\n",
    "        else:\n",
    "            masked_words.append(word)\n",
    "\n",
    "    return ' '.join(masked_words), masked_count\n",
    "\n",
    "def remove_features(text, feature_words):\n",
    "    \"\"\"\n",
    "    Remove specified features from text.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        The text to process\n",
    "    feature_words : list\n",
    "        List of words to remove\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Text with specified words removed\n",
    "    int\n",
    "        Count of words removed\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokens = clean_tokens(tokens)\n",
    "    words = [w for w in tokens if w not in string.punctuation]\n",
    "    removed_count = 0\n",
    "    filtered_words = []\n",
    "\n",
    "    for word in words:\n",
    "        if word.lower() in feature_words:\n",
    "            removed_count += 1\n",
    "        else:\n",
    "            filtered_words.append(word)\n",
    "\n",
    "    return ' '.join(filtered_words), removed_count\n",
    "\n",
    "##########################################################################################################\n",
    "\n",
    "def replace_mask_random_words(text, num_words_to_mask, mask_token=\"[MASK]\"):\n",
    "    \"\"\"\n",
    "    Mask a specified number of random words in the text.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        The text to process\n",
    "    num_words_to_mask : int\n",
    "        Number of words to mask\n",
    "    mask_token : str\n",
    "        Token to replace words with\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Text with random words masked\n",
    "    int\n",
    "        Actual count of words masked (might be less than num_words_to_mask if text is shorter)\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokens = clean_tokens(tokens)\n",
    "    words = [w for w in tokens if w not in string.punctuation]\n",
    "\n",
    "    # Adjust number of words to mask if text is shorter\n",
    "    num_words_to_mask = min(num_words_to_mask, len(words))\n",
    "\n",
    "    # Select random indices to mask\n",
    "    indices_to_mask = random.sample(range(len(words)), num_words_to_mask)\n",
    "\n",
    "    # Apply masking\n",
    "    for idx in indices_to_mask:\n",
    "        words[idx] = mask_token\n",
    "\n",
    "    return ' '.join(words), num_words_to_mask\n",
    "\n",
    "def remove_random_words(text, num_words_to_remove):\n",
    "    \"\"\"\n",
    "    Remove a specified number of random words from the text.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        The text to process\n",
    "    num_words_to_remove : int\n",
    "        Number of words to remove\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Text with random words removed\n",
    "    int\n",
    "        Actual count of words removed (might be less than num_words_to_remove if text is shorter)\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokens = clean_tokens(tokens)\n",
    "    words = [w for w in tokens if w not in string.punctuation]\n",
    "\n",
    "    # Adjust number of words to remove if text is shorter\n",
    "    num_words_to_remove = min(num_words_to_remove, len(words))\n",
    "\n",
    "    # Select random indices to remove\n",
    "    indices_to_remove = random.sample(range(len(words)), num_words_to_remove)\n",
    "\n",
    "    # Create a new list without the removed words\n",
    "    filtered_words = [word for idx, word in enumerate(words) if idx not in indices_to_remove]\n",
    "\n",
    "    return ' '.join(filtered_words), num_words_to_remove\n",
    "\n",
    "def calculate_ablation_statistics(texts, feature_extraction_func, feature_data):\n",
    "    \"\"\"\n",
    "    Calculate statistics for feature word ablation across a corpus.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    texts : list\n",
    "        List of text documents\n",
    "    feature_extraction_func : function\n",
    "        Function that extracts feature words from a text\n",
    "    feature_data : dict/object\n",
    "        Data needed by the feature extraction function\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing:\n",
    "        - total_ablated_words: Total number of words ablated\n",
    "        - ablated_words_per_doc: List of counts for each document\n",
    "        - avg_ablated_words: Average number of ablated words per document\n",
    "        - most_common_ablated: Counter of most commonly ablated words\n",
    "    \"\"\"\n",
    "    ablated_words_per_doc = []\n",
    "    all_ablated_words = []\n",
    "\n",
    "    for text in texts:\n",
    "        # Extract feature words specific to this text\n",
    "        feature_words = feature_extraction_func(text, feature_data)\n",
    "\n",
    "        # Count how many words would be ablated\n",
    "        words = text.lower().split()\n",
    "        ablated_in_doc = [word for word in words if word in feature_words]\n",
    "\n",
    "        ablated_words_per_doc.append(len(ablated_in_doc))\n",
    "        all_ablated_words.extend(ablated_in_doc)\n",
    "\n",
    "    # Calculate statistics\n",
    "    total_ablated_words = sum(ablated_words_per_doc)\n",
    "    avg_ablated_words = np.mean(ablated_words_per_doc) if ablated_words_per_doc else 0\n",
    "    most_common_ablated = Counter(all_ablated_words).most_common(10)\n",
    "\n",
    "    return {\n",
    "        'total_ablated_words': total_ablated_words,\n",
    "        'ablated_words_per_doc': ablated_words_per_doc,\n",
    "        'avg_ablated_words': avg_ablated_words,\n",
    "        'most_common_ablated': most_common_ablated\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_features_from_texts(texts, nrc_lexicon, hurtlex_dict, function_words_dict):\n",
    "    \"\"\"\n",
    "    Extract various linguistic features from a list of texts.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    texts : list\n",
    "        List of texts to analyze\n",
    "    nrc_lexicon : dict\n",
    "        The NRC emotion lexicon\n",
    "    hurtlex_dict : dict\n",
    "        The HurtLex dictionary\n",
    "    function_words_dict : dict\n",
    "        Dictionary of function word categories\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with extracted features for each text\n",
    "    \"\"\"\n",
    "    features = {\n",
    "        'emotion': {\n",
    "            'anger': [], 'anticipation': [], 'disgust': [],\n",
    "            'fear': [], 'joy': [], 'sadness': [],\n",
    "            'surprise': [], 'trust': []\n",
    "        },\n",
    "        'sentiment': {'positive': [], 'negative': []},\n",
    "        'function_words': {cat: [] for cat in function_words_dict.keys()},\n",
    "        'hate_speech': {}\n",
    "    }\n",
    "\n",
    "    # Define hate speech categories to extract from HurtLex\n",
    "    hurtlex_categories = ['ps', 'pa', 'ddf', 'ddp', 'asf', 'asp', 'om', 'qas',\n",
    "                         'cds', 'rci', 'pr', 'pe', 'dmc', 'is', 'or', 'an']\n",
    "\n",
    "    for category in hurtlex_categories:\n",
    "        features['hate_speech'][category] = []\n",
    "\n",
    "    for text in texts:\n",
    "        # Extract emotion words\n",
    "        for emotion in features['emotion'].keys():\n",
    "            emotion_words = [word for word in text.lower().split()\n",
    "                            if word in nrc_lexicon and emotion in nrc_lexicon[word]]\n",
    "            features['emotion'][emotion].append(emotion_words)\n",
    "\n",
    "        # Extract sentiment words\n",
    "        for sentiment in features['sentiment'].keys():\n",
    "            sentiment_words = [word for word in text.lower().split()\n",
    "                              if word in nrc_lexicon and sentiment in nrc_lexicon[word]]\n",
    "            features['sentiment'][sentiment].append(sentiment_words)\n",
    "\n",
    "        # Extract function words\n",
    "        for category in features['function_words'].keys():\n",
    "            category_words = [word for word in text.lower().split()\n",
    "                             if word in function_words_dict[category]]\n",
    "            features['function_words'][category].append(category_words)\n",
    "\n",
    "        # Extract hate speech terms\n",
    "        for category in features['hate_speech'].keys():\n",
    "            hate_terms = [word for word in text.lower().split()\n",
    "                         if word in hurtlex_dict and category in hurtlex_dict[word]]\n",
    "            features['hate_speech'][category].append(hate_terms)\n",
    "\n",
    "    return features\n",
    "\n",
    "##########################################################################################################\n",
    "\n",
    "\n",
    "def replace_mask_random_words_excluding_features(text, num_words_to_mask, feature_words, mask_token=\"[MASK]\"):\n",
    "    \"\"\"\n",
    "    Mask a specified number of random words in the text, excluding feature words.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        The text to process\n",
    "    num_words_to_mask : int\n",
    "        Number of words to mask\n",
    "    feature_words : set\n",
    "        Set of words to exclude from masking (feature words)\n",
    "    mask_token : str\n",
    "        Token to replace words with\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Text with random words masked (excluding feature words)\n",
    "    int\n",
    "        Actual count of words masked (might be less than num_words_to_mask if text is shorter)\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokens = clean_tokens(tokens)\n",
    "    words = [w for w in tokens if w not in string.punctuation]\n",
    "\n",
    "    # Get indices of non-feature words\n",
    "    non_feature_indices = [i for i, word in enumerate(words) if word.lower() not in feature_words]\n",
    "\n",
    "    # Adjust number of words to mask if fewer non-feature words available\n",
    "    num_words_to_mask = min(num_words_to_mask, len(non_feature_indices))\n",
    "\n",
    "    if num_words_to_mask == 0:\n",
    "        return text, 0\n",
    "\n",
    "    # Select random indices from non-feature words\n",
    "    indices_to_mask = random.sample(non_feature_indices, num_words_to_mask)\n",
    "\n",
    "    # Apply masking\n",
    "    for idx in indices_to_mask:\n",
    "        words[idx] = mask_token\n",
    "\n",
    "    return ' '.join(words), num_words_to_mask\n",
    "\n",
    "def remove_random_words_excluding_features(text, num_words_to_remove, feature_words):\n",
    "    \"\"\"\n",
    "    Remove a specified number of random words from the text, excluding feature words.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        The text to process\n",
    "    num_words_to_remove : int\n",
    "        Number of words to remove\n",
    "    feature_words : set\n",
    "        Set of words to exclude from removal (feature words)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Text with random words removed (excluding feature words)\n",
    "    int\n",
    "        Actual count of words removed (might be less than num_words_to_remove if text is shorter)\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokens = clean_tokens(tokens)\n",
    "    words = [w for w in tokens if w not in string.punctuation]\n",
    "\n",
    "    # Get indices of non-feature words\n",
    "    non_feature_indices = [i for i, word in enumerate(words) if word.lower() not in feature_words]\n",
    "\n",
    "    # Adjust number of words to remove if fewer non-feature words available\n",
    "    num_words_to_remove = min(num_words_to_remove, len(non_feature_indices))\n",
    "\n",
    "    if num_words_to_remove == 0:\n",
    "        return text, 0\n",
    "\n",
    "    # Select random indices from non-feature words to remove\n",
    "    indices_to_remove = random.sample(non_feature_indices, num_words_to_remove)\n",
    "\n",
    "    # Create a new list without the removed words\n",
    "    filtered_words = [word for idx, word in enumerate(words) if idx not in indices_to_remove]\n",
    "\n",
    "    return ' '.join(filtered_words), num_words_to_remove\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################################################\n",
    "\n",
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"Installing spaCy English model...\")\n",
    "    import subprocess\n",
    "    subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def extract_pos_tags_batch(texts, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Extract POS tags for multiple texts using batch processing for speed.\n",
    "\n",
    "    Parameters:\n",
    "    - texts (list): List of input texts\n",
    "    - batch_size (int): Number of texts to process in each batch\n",
    "\n",
    "    Returns:\n",
    "    - list: List of lists containing (word, pos_tag) tuples for each text\n",
    "    \"\"\"\n",
    "    all_pos_tags = []\n",
    "\n",
    "    print(f\"Processing {len(texts)} texts in batches of {batch_size}...\")\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "\n",
    "        # Process batch with nlp.pipe for speed\n",
    "        docs = list(nlp.pipe(batch, batch_size=batch_size))\n",
    "\n",
    "        batch_pos_tags = []\n",
    "        for doc in docs:\n",
    "            pos_tags = [(token.text, token.pos_) for token in doc]\n",
    "            batch_pos_tags.append(pos_tags)\n",
    "\n",
    "        all_pos_tags.extend(batch_pos_tags)\n",
    "\n",
    "        if (i + batch_size) % 5000 == 0 or (i + batch_size) >= len(texts):\n",
    "            print(f\"Processed {min(i + batch_size, len(texts))}/{len(texts)} texts\")\n",
    "\n",
    "    return all_pos_tags\n",
    "\n",
    "def remove_pos_category(text, pos_category):\n",
    "    \"\"\"\n",
    "    Remove all words of a specific POS category from text.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): Input text\n",
    "    - pos_category (str): POS category to remove (e.g., 'NOUN', 'VERB')\n",
    "\n",
    "    Returns:\n",
    "    - tuple: (modified_text, count_removed)\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    filtered_words = []\n",
    "    count_removed = 0\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ == pos_category:\n",
    "            count_removed += 1\n",
    "        else:\n",
    "            filtered_words.append(token.text)\n",
    "\n",
    "    return ' '.join(filtered_words), count_removed\n",
    "\n",
    "def create_pos_ablated_dataset(texts, pos_category, precomputed_pos_tags=None):\n",
    "    \"\"\"\n",
    "    Create a dataset with a specific POS category removed from all texts.\n",
    "    Optimized version that can reuse precomputed POS tags.\n",
    "\n",
    "    Parameters:\n",
    "    - texts (list): List of text documents\n",
    "    - pos_category (str): POS category to remove\n",
    "    - precomputed_pos_tags (list): Precomputed POS tags to avoid re-tagging\n",
    "\n",
    "    Returns:\n",
    "    - tuple: (ablated_texts, removal_stats)\n",
    "    \"\"\"\n",
    "    ablated_texts = []\n",
    "    total_removed = 0\n",
    "    documents_affected = 0\n",
    "    removal_per_doc = []\n",
    "\n",
    "    print(f\"Removing POS category: {pos_category}\")\n",
    "\n",
    "    # Use precomputed tags if available, otherwise compute them\n",
    "    if precomputed_pos_tags is None:\n",
    "        print(\"Computing POS tags...\")\n",
    "        pos_tags_list = extract_pos_tags_batch(texts)\n",
    "    else:\n",
    "        pos_tags_list = precomputed_pos_tags\n",
    "\n",
    "    for i, (text, pos_tags) in enumerate(zip(texts, pos_tags_list)):\n",
    "        # Filter out words with the target POS category\n",
    "        filtered_words = []\n",
    "        count_removed = 0\n",
    "\n",
    "        for word, pos_tag in pos_tags:\n",
    "            if pos_tag == pos_category:\n",
    "                count_removed += 1\n",
    "            else:\n",
    "                filtered_words.append(word)\n",
    "\n",
    "        ablated_text = ' '.join(filtered_words)\n",
    "        ablated_texts.append(ablated_text)\n",
    "\n",
    "        total_removed += count_removed\n",
    "        removal_per_doc.append(count_removed)\n",
    "\n",
    "        if count_removed > 0:\n",
    "            documents_affected += 1\n",
    "\n",
    "        if (i + 1) % 2000 == 0:\n",
    "            print(f\"Processed {i + 1}/{len(texts)} documents\")\n",
    "\n",
    "    stats = {\n",
    "        'pos_category': pos_category,\n",
    "        'total_documents': len(texts),\n",
    "        'documents_affected': documents_affected,\n",
    "        'total_words_removed': total_removed,\n",
    "        'avg_words_removed_per_doc': np.mean(removal_per_doc),\n",
    "        'max_words_removed': max(removal_per_doc),\n",
    "        'percentage_docs_affected': (documents_affected / len(texts)) * 100\n",
    "    }\n",
    "\n",
    "    return ablated_texts, stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fm2DtDAm9WSF"
   },
   "source": [
    "# Binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rtqTF6Gk9bBM"
   },
   "outputs": [],
   "source": [
    "# Define a custom binary dataset class for BERT\n",
    "class MemeDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Define a BERT classifier model\n",
    "class BertClassifier(torch.nn.Module):\n",
    "    def __init__(self, n_classes, model_name=\"bert-base-uncased\"):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.drop = torch.nn.Dropout(0.3)\n",
    "        self.out = torch.nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        pooled_output = outputs.pooler_output\n",
    "        output = self.drop(pooled_output)\n",
    "        return self.out(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8YwWzTXQ9gpq"
   },
   "outputs": [],
   "source": [
    "# Training Function for binary classification\n",
    "def train_epoch_bin(model, data_loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss = torch.nn.CrossEntropyLoss()(outputs, labels)\n",
    "\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        total_predictions += len(labels)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    return correct_predictions.double() / total_predictions, np.mean(losses)\n",
    "\n",
    "# Evaluation Function for binary classification\n",
    "def eval_model_bin(model, data_loader, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            loss = torch.nn.CrossEntropyLoss()(outputs, labels)\n",
    "\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            total_predictions += len(labels)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    return correct_predictions.double() / total_predictions, np.mean(losses), all_preds, all_labels\n",
    "\n",
    "# Create a function to perform binary prediction\n",
    "def predict(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_z1S_RE-9jGr"
   },
   "source": [
    "# Multi-label classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LxLpNVF09kdr"
   },
   "outputs": [],
   "source": [
    "# Dataset for multi-label classification\n",
    "class MemeMultiLabelDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# Training function for multi-label classification\n",
    "def train_epoch_multilabel(model, data_loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "\n",
    "        loss = torch.nn.BCELoss()(outputs, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    return np.mean(losses)\n",
    "\n",
    "\n",
    "# Evaluation function for multi-label classification\n",
    "def eval_model_multilabel(model, data_loader, device, threshold=0.5):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "\n",
    "            loss = torch.nn.BCELoss()(outputs, labels)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # Convert probabilities to binary predictions\n",
    "            preds = (outputs > threshold).int()\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return np.mean(losses), all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z_9Unq829pVq"
   },
   "outputs": [],
   "source": [
    "# Create a multi-label BERT classifier\n",
    "class BertMultiLabelClassifier(torch.nn.Module):\n",
    "    def __init__(self, n_classes, model_name=\"bert-base-uncased\"):\n",
    "        super(BertMultiLabelClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.drop = torch.nn.Dropout(0.3)\n",
    "        self.out = torch.nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        pooled_output = outputs.pooler_output\n",
    "        output = self.drop(pooled_output)\n",
    "        return self.sigmoid(self.out(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jG00eb049wvm"
   },
   "outputs": [],
   "source": [
    "# Create a function to perform hierarchical prediction\n",
    "def predict_hierarchical(binary_model, ml_model, data_loader, ml_tokenizer, device,\n",
    "                        fine_grained_labels, test_texts=None, threshold=0.5, batch_size=16):\n",
    "    \"\"\"\n",
    "    Create a function to perform hierarchical prediction\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    binary_model : torch.nn.Module\n",
    "        Pre-trained binary classification model\n",
    "    ml_model : torch.nn.Module\n",
    "        Pre-trained multi-label classification model\n",
    "    data_loader : DataLoader\n",
    "        DataLoader for test data\n",
    "    ml_tokenizer : transformers.Tokenizer\n",
    "        Tokenizer for processing text\n",
    "    device : torch.device\n",
    "        Device to run models on\n",
    "    fine_grained_labels : list\n",
    "        List of fine-grained label names\n",
    "    test_texts : list, optional\n",
    "        List of test texts (if None, will extract from data_loader)\n",
    "    threshold : float, default=0.5\n",
    "        Threshold for multi-label predictions\n",
    "    batch_size : int, default=16\n",
    "        Batch size for multi-label predictions\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    np.ndarray : Full prediction matrix (binary + multi-label)\n",
    "    \"\"\"\n",
    "    # First, get binary predictions\n",
    "    binary_model.eval()\n",
    "    ml_model.eval()\n",
    "    binary_preds = []\n",
    "    all_ml_preds = np.zeros((len(data_loader.dataset), len(fine_grained_labels)))\n",
    "\n",
    "    # Extract test texts if not provided\n",
    "    if test_texts is None:\n",
    "        test_texts = []\n",
    "        for batch in data_loader:\n",
    "            test_texts.extend(batch['text'])\n",
    "        # Reset data_loader for predictions\n",
    "        data_loader = DataLoader(data_loader.dataset, batch_size=data_loader.batch_size)\n",
    "\n",
    "    # Get binary predictions\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "\n",
    "            outputs = binary_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            binary_preds.extend(preds.cpu().tolist())\n",
    "\n",
    "    # For positive binary predictions, get multi-label predictions\n",
    "    positive_indices = [i for i, pred in enumerate(binary_preds) if pred == 1]\n",
    "\n",
    "    if positive_indices:\n",
    "        # Get texts for positive samples\n",
    "        positive_texts = [test_texts[i] for i in positive_indices]\n",
    "\n",
    "        # Create a dataset for positive samples\n",
    "        positive_dataset = MemeDataset(\n",
    "            texts=positive_texts,\n",
    "            labels=[0] * len(positive_texts),  # Dummy labels\n",
    "            tokenizer=ml_tokenizer\n",
    "        )\n",
    "\n",
    "        positive_loader = DataLoader(\n",
    "            positive_dataset,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "\n",
    "        # Predict multi-labels\n",
    "        ml_preds = []\n",
    "        with torch.no_grad():\n",
    "            for batch in positive_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                token_type_ids = batch['token_type_ids'].to(device)\n",
    "\n",
    "                outputs = ml_model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids\n",
    "                )\n",
    "\n",
    "                # Convert probabilities to binary predictions\n",
    "                preds = (outputs > threshold).int()\n",
    "                ml_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "        # Assign multi-label predictions to positive samples\n",
    "        for i, idx in enumerate(positive_indices):\n",
    "            all_ml_preds[idx] = ml_preds[i]\n",
    "\n",
    "    # Create full prediction matrix (binary + multi-label)\n",
    "    full_preds = np.zeros((len(binary_preds), 1 + len(fine_grained_labels)))\n",
    "    full_preds[:, 0] = binary_preds  # Binary classification\n",
    "    full_preds[:, 1:] = all_ml_preds  # Multi-label classification\n",
    "\n",
    "    return full_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZonKH7r293-o"
   },
   "source": [
    "# MAMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i3vpBX2195Fo",
    "outputId": "3078d040-f44e-48da-9304-a7365b2a4df2"
   },
   "outputs": [],
   "source": [
    "# Load MAMI dataset\n",
    "print(\"Loading MAMI dataset...\")\n",
    "mami_training_data = route +  '' # Path to training data\n",
    "mami_dev_data = route + '' # Path to development data\n",
    "mami_test_data = route + '' # Path to test data\n",
    "\n",
    "mami_training_df = pd.read_json(mami_training_data, orient='index')\n",
    "mami_dev_df = pd.read_json(mami_dev_data, orient='index')\n",
    "mami_test_df = pd.read_json(mami_test_data, orient='index')\n",
    "\n",
    "# Combine training and validation sets\n",
    "mami_training_df = pd.concat([mami_training_df, mami_dev_df]).sort_index()\n",
    "\n",
    "# Check available columns\n",
    "print(\"Training data columns:\", mami_training_df.columns)\n",
    "print(\"Test data columns:\", mami_test_df.columns)\n",
    "\n",
    "# Set up PyEvaLL evaluation gold files\n",
    "gold_test_txt = route + '' # Path to MAMI text gold label (txt)\n",
    "gold_test_bin = route + '' # Path to MAMI binary classification gold labels (json) \n",
    "gold_test_ml = route + '' # Path to MAMI multi-label classification gold labels (json)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vmaf8s9i-B8i"
   },
   "source": [
    "### Binary Classification: Misogynous vs. non-misogynous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "KA8sFiYzKKCt",
    "outputId": "d2434571-5d7c-42e9-ad81-792d37540b8f"
   },
   "outputs": [],
   "source": [
    "def run_bert_baseline_bin(dataset_name, training_df, test_df, binary_label, label_names):\n",
    "    \"\"\"\n",
    "    Run BERT baseline model and save results in the same format as ablation experiments.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset_name : str\n",
    "        Dataset name ('MAMI' or 'EXIST')\n",
    "    training_df : DataFrame\n",
    "        Training data\n",
    "    test_df : DataFrame\n",
    "        Test data\n",
    "    binary_label : str\n",
    "        Binary label column name\n",
    "    label_names : list\n",
    "        List of label names (e.g., [\"non-misogynous\", \"misogynous\"])\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : BERT baseline results dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"\\n🤖 RUNNING BERT BASELINE FOR {dataset_name}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Data preparation\n",
    "    X_train = training_df[\"bert representation\"].tolist()\n",
    "    X_test = test_df[\"bert representation\"].tolist()\n",
    "    y_train = training_df[binary_label].tolist()\n",
    "    y_test = test_df[binary_label].tolist()\n",
    "\n",
    "    # Set model name\n",
    "    model_name = \"bert_baseline\"\n",
    "\n",
    "    # Initialize tokenizer and device\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = MemeDataset(\n",
    "        texts=X_train,\n",
    "        labels=y_train,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    test_dataset = MemeDataset(\n",
    "        texts=X_test,\n",
    "        labels=y_test,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    # Create data loaders\n",
    "    batch_size = 16\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Initialize model, optimizer, and scheduler\n",
    "    bert_model = BertClassifier(n_classes=2).to(device)\n",
    "    optimizer = AdamW(bert_model.parameters(), lr=2e-5)\n",
    "    total_steps = len(train_loader) * 3  # epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    print(\"\\n🚀 Training BERT binary classifier...\")\n",
    "    epochs = 3\n",
    "    best_accuracy = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch {epoch + 1}/{epochs}')\n",
    "\n",
    "        train_acc, train_loss = train_epoch_bin(\n",
    "            bert_model,\n",
    "            train_loader,\n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            device\n",
    "        )\n",
    "\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "\n",
    "        val_acc, val_loss, _, _ = eval_model_bin(\n",
    "            bert_model,\n",
    "            test_loader,\n",
    "            device\n",
    "        )\n",
    "\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "        if val_acc > best_accuracy:\n",
    "            best_accuracy = val_acc\n",
    "            torch.save(bert_model.state_dict(), f'best_model_{model_name}_{dataset_name.lower()}.pt')\n",
    "            print('✅ Best model saved!')\n",
    "\n",
    "    # Load the best model\n",
    "    bert_model.load_state_dict(torch.load(f'best_model_{model_name}_{dataset_name.lower()}.pt'))\n",
    "\n",
    "    # Make predictions on test set\n",
    "    print(\"\\n📊 Evaluating model on test set...\")\n",
    "    _, _, all_preds, all_labels = eval_model_bin(\n",
    "        bert_model,\n",
    "        test_loader,\n",
    "        device\n",
    "    )\n",
    "\n",
    "    # Set evaluation parameters\n",
    "    evaluation_type = 'binary'\n",
    "\n",
    "    # Set gold standard file paths\n",
    "    if dataset_name == \"MAMI\":\n",
    "        gold_test_bin = route + '' # Path to MAMI binary classification gold labels (json) \n",
    "        gold_test_txt = route + '' # Path to MAMI text gold label (txt)\n",
    "    else:  # EXIST\n",
    "        gold_test_bin = route + '' # Path to EXIST2024 binary classification gold labels (json) \n",
    "        gold_test_txt = route + ''# Path to EXIST2024 text gold label (txt)\n",
    "\n",
    "    # Save prediction results\n",
    "    test_pred_json, test_pred_txt = save_evaluation(\n",
    "        test_df, \"evaluation/predictions\", dataset_name, \"test\",\n",
    "        evaluation_type, model_name, np.array(all_preds), binary_label, []\n",
    "    )\n",
    "\n",
    "    # Calculate all metrics\n",
    "    accuracy = accuracy_score(y_test, all_preds)\n",
    "    precision_macro = precision_score(y_test, all_preds, average='macro')\n",
    "    recall_macro = recall_score(y_test, all_preds, average='macro')\n",
    "    f1_macro = f1_score(y_test, all_preds, average='macro')\n",
    "\n",
    "    # Get detailed classification report\n",
    "    class_report_dict = classification_report(\n",
    "        y_test, all_preds,\n",
    "        target_names=label_names,\n",
    "        zero_division=0,\n",
    "        digits=3,\n",
    "        output_dict=True\n",
    "    )\n",
    "\n",
    "    # Calculate binary F1 score (MAMI evaluation metric)\n",
    "    binary_f1 = evaluate_f1_scores(gold_test_txt, test_pred_txt, 2)\n",
    "\n",
    "    # Save baseline results to JSON file\n",
    "    baseline_results = {\n",
    "        'binary_f1': binary_f1,\n",
    "        'macro_f1': f1_macro,\n",
    "        'accuracy': accuracy,\n",
    "        'precision_macro': precision_macro,\n",
    "        'recall_macro': recall_macro,\n",
    "        'per_label_metrics': class_report_dict,\n",
    "        'model_config': {\n",
    "            'model_type': 'BERT',\n",
    "            'model_name': 'bert-base-uncased',\n",
    "            'epochs': epochs,\n",
    "            'batch_size': batch_size,\n",
    "            'learning_rate': 2e-5,\n",
    "            'max_length': 128\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Create results directory\n",
    "    os.makedirs(\"evaluation/results/binary/BERT\", exist_ok=True)\n",
    "\n",
    "    # Save baseline results with updated filename format\n",
    "    baseline_results_file = f\"evaluation/results/binary/BERT/{model_name}_{dataset_name}_bin_baseline_results.json\"\n",
    "\n",
    "    with open(baseline_results_file, 'w') as f:\n",
    "        json.dump(baseline_results, f, indent=2, cls=NumpyEncoder)\n",
    "\n",
    "    print(f\"✅ Baseline results saved to: {baseline_results_file}\")\n",
    "\n",
    "    # Display evaluation results (keep original functionality)\n",
    "    evaluate_binary_classification(\n",
    "        gold_test_bin, test_pred_json,\n",
    "        y_test, np.array(all_preds),\n",
    "        gold_test_txt, test_pred_txt,\n",
    "        label_names,\n",
    "        model_name=\"BERT baseline\"\n",
    "    )\n",
    "\n",
    "    # Print saved metrics (for verification)\n",
    "    print(f\"\\n📊 SAVED BERT BASELINE METRICS:\")\n",
    "    print(f\"   • Binary F1: {binary_f1:.3f}\")\n",
    "    print(f\"   • Macro F1: {f1_macro:.3f}\")\n",
    "    print(f\"   • Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"   • Precision (macro): {precision_macro:.3f}\")\n",
    "    print(f\"   • Recall (macro): {recall_macro:.3f}\")\n",
    "\n",
    "    # Save model state dict as well\n",
    "    model_save_dir = f\"models/bert_baseline/{dataset_name.lower()}\"\n",
    "    os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "    model_save_path = f\"{model_save_dir}/{model_name}_final.pt\"\n",
    "    torch.save(bert_model.state_dict(), model_save_path)\n",
    "    print(f\"✅ Model saved to: {model_save_path}\")\n",
    "\n",
    "    return baseline_results\n",
    "\n",
    "# Run MAMI BERT baseline\n",
    "mami_bert_results = run_bert_baseline_bin(\n",
    "    dataset_name=\"MAMI\",\n",
    "    training_df=mami_training_df,\n",
    "    test_df=mami_test_df,\n",
    "    binary_label=\"misogynous\",\n",
    "    label_names=[\"non-misogynous\", \"misogynous\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Edn2taz5-GFu"
   },
   "source": [
    "## Multi-label Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kPssXxGx-Jhk"
   },
   "outputs": [],
   "source": [
    "def run_bert_baseline_ml(dataset_name, training_df, test_df, binary_model,\n",
    "                                  binary_label, fine_grained_labels, tokenizer, device,\n",
    "                                  batch_size=16, route=\"\"):\n",
    "    \"\"\"\n",
    "    Run BERT multi-label (hierarchical) model and save results.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"\\n🏷️ RUNNING BERT MULTI-LABEL FOR {dataset_name}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Set evaluation parameters\n",
    "    evaluation_type = \"hierarchical\"\n",
    "    model_name = \"bert_baseline\"\n",
    "    label_names = [binary_label] + fine_grained_labels\n",
    "\n",
    "    print(f\"Fine-grained labels: {fine_grained_labels}\")\n",
    "    print(f\"All label names: {label_names}\")\n",
    "\n",
    "    # Filter data for positive binary instances (only train on positive examples for multi-label)\n",
    "    train_df_bin_positive = training_df.loc[training_df[binary_label] == 1]\n",
    "    X_train_bin_positive = train_df_bin_positive[\"bert representation\"].tolist()\n",
    "    y_train_categories = train_df_bin_positive[fine_grained_labels].values.tolist()\n",
    "\n",
    "    print(f\"Total training samples: {len(training_df)}\")\n",
    "    print(f\"Positive binary samples for multi-label training: {len(train_df_bin_positive)}\")\n",
    "\n",
    "    # Prepare test data (all samples)\n",
    "    X_test = test_df[\"bert representation\"].tolist()\n",
    "    y_test = test_df[binary_label].tolist()\n",
    "\n",
    "    # Create datasets for multi-label classification\n",
    "    ml_train_dataset = MemeMultiLabelDataset(\n",
    "        texts=X_train_bin_positive,\n",
    "        labels=y_train_categories,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    test_dataset = MemeDataset(\n",
    "        texts=X_test,\n",
    "        labels=y_test,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    # Create data loaders\n",
    "    ml_train_loader = DataLoader(\n",
    "        ml_train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Initialize multi-label model\n",
    "    ml_model = BertMultiLabelClassifier(n_classes=len(fine_grained_labels)).to(device)\n",
    "    ml_optimizer = AdamW(ml_model.parameters(), lr=2e-5)\n",
    "    ml_total_steps = len(ml_train_loader) * 3  # epochs\n",
    "    ml_scheduler = get_linear_schedule_with_warmup(\n",
    "        ml_optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=ml_total_steps\n",
    "    )\n",
    "\n",
    "    # Train multi-label model\n",
    "    print(\"\\n🚀 Training BERT multi-label classifier...\")\n",
    "    ml_epochs = 3\n",
    "\n",
    "    for epoch in range(ml_epochs):\n",
    "        print(f'Multi-label Epoch {epoch + 1}/{ml_epochs}')\n",
    "\n",
    "        train_loss = train_epoch_multilabel(\n",
    "            ml_model,\n",
    "            ml_train_loader,\n",
    "            ml_optimizer,\n",
    "            ml_scheduler,\n",
    "            device\n",
    "        )\n",
    "\n",
    "        print(f'Multi-label Train Loss: {train_loss:.4f}')\n",
    "\n",
    "    # Save multi-label model\n",
    "    torch.save(ml_model.state_dict(), f'best_model_bert_multilabel_{dataset_name.lower()}.pt')\n",
    "    print('✅ Multi-label model saved!')\n",
    "\n",
    "    # Make hierarchical predictions\n",
    "    print(\"\\n🔮 Making hierarchical predictions...\")\n",
    "    hierarchical_predictions = predict_hierarchical(\n",
    "        binary_model,\n",
    "        ml_model,\n",
    "        test_loader,\n",
    "        tokenizer,\n",
    "        device,\n",
    "        fine_grained_labels,\n",
    "        test_texts=X_test,\n",
    "        threshold=0.5,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Convert predictions to a DataFrame for evaluation\n",
    "    test_pred_df = pd.DataFrame(\n",
    "        hierarchical_predictions,\n",
    "        columns=[binary_label] + fine_grained_labels\n",
    "    )\n",
    "\n",
    "    print(f\"Hierarchical predictions shape: {hierarchical_predictions.shape}\")\n",
    "    print(f\"Test predictions DataFrame shape: {test_pred_df.shape}\")\n",
    "\n",
    "    # Set gold standard file paths\n",
    "    if dataset_name == \"MAMI\":\n",
    "        gold_test_ml = route + '' # Path to MAMI multi-label classification gold labels (json)  \n",
    "        gold_test_txt = route + '' # Path to MAMI text gold label (txt)\n",
    "    else:  # EXIST\n",
    "        gold_test_ml = route + '' # Path to EXIST2024 multi-label classification gold labels (json)  \n",
    "        gold_test_txt = route + ''# Path to EXIST2024 text gold label (txt)\n",
    "\n",
    "    # Save and evaluate hierarchical predictions\n",
    "    test_pred_json_ml, test_pred_txt_ml = save_evaluation(\n",
    "        test_df,\n",
    "        \"evaluation/predictions\",\n",
    "        dataset_name,\n",
    "        \"test\",\n",
    "        \"hierarchical\",\n",
    "        model_name,\n",
    "        test_pred_df,\n",
    "        binary_label,\n",
    "        label_names\n",
    "    )\n",
    "\n",
    "    # Calculate metrics for each label\n",
    "    y_true_df = test_df[label_names]\n",
    "\n",
    "    # Calculate macro F1 score for hierarchical evaluation\n",
    "    hierarchical_scores = evaluate_f1_scores(gold_test_txt, test_pred_txt_ml, len(label_names))\n",
    "    if isinstance(hierarchical_scores, tuple):\n",
    "        binary_f1, multilabel_f1 = hierarchical_scores\n",
    "    else:\n",
    "        binary_f1 = hierarchical_scores\n",
    "        multilabel_f1 = hierarchical_scores\n",
    "\n",
    "    # Calculate individual label metrics\n",
    "    binary_true = y_true_df[binary_label].values\n",
    "    binary_pred = hierarchical_predictions[:, 0]\n",
    "\n",
    "    # Create negative class representation\n",
    "    negative_true = np.zeros((len(hierarchical_predictions), 1))\n",
    "    negative_pred = np.zeros((len(hierarchical_predictions), 1))\n",
    "    negative_true[:, 0] = (binary_true == 0)\n",
    "    negative_pred[:, 0] = (binary_pred == 0)\n",
    "\n",
    "    # Get multi-label data\n",
    "    multilabel_true = y_true_df[fine_grained_labels].values\n",
    "    multilabel_pred = hierarchical_predictions[:, 1:]\n",
    "\n",
    "    # Create combined representation\n",
    "    combined_true = np.hstack((negative_true, multilabel_true))\n",
    "    combined_pred = np.hstack((negative_pred, multilabel_pred))\n",
    "\n",
    "    # 更新标签名称\n",
    "    updated_label_names = [f\"non-{binary_label}\"] + fine_grained_labels\n",
    "\n",
    "    # 重新计算 individual metrics\n",
    "    individual_metrics = {}\n",
    "    for i, label in enumerate(updated_label_names):\n",
    "        y_true_label = combined_true[:, i]\n",
    "        y_pred_label = combined_pred[:, i]\n",
    "\n",
    "        # Calculate metrics for this label\n",
    "        accuracy = accuracy_score(y_true_label, y_pred_label)\n",
    "        precision = precision_score(y_true_label, y_pred_label, zero_division=0)\n",
    "        recall = recall_score(y_true_label, y_pred_label, zero_division=0)\n",
    "        f1 = f1_score(y_true_label, y_pred_label, zero_division=0)\n",
    "\n",
    "        individual_metrics[label] = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }\n",
    "\n",
    "    # Calculate overall macro metrics\n",
    "    all_f1_scores = [individual_metrics[label]['f1'] for label in updated_label_names]\n",
    "    all_precision_scores = [individual_metrics[label]['precision'] for label in updated_label_names]\n",
    "    all_recall_scores = [individual_metrics[label]['recall'] for label in updated_label_names]\n",
    "\n",
    "    macro_f1 = np.mean(all_f1_scores)\n",
    "    macro_precision = np.mean(all_precision_scores)\n",
    "    macro_recall = np.mean(all_recall_scores)\n",
    "\n",
    "    # Save hierarchical results to JSON file\n",
    "    hierarchical_results = {\n",
    "        'binary_f1': binary_f1,\n",
    "        'hierarchical_f1': multilabel_f1,\n",
    "        'macro_f1': macro_f1,\n",
    "        'macro_precision': macro_precision,\n",
    "        'macro_recall': macro_recall,\n",
    "        'individual_label_metrics': individual_metrics,\n",
    "        'fine_grained_labels': fine_grained_labels,\n",
    "        'all_labels': label_names,\n",
    "        'model_config': {\n",
    "            'model_type': 'BERT',\n",
    "            'model_name': 'bert-base-uncased',\n",
    "            'epochs': ml_epochs,\n",
    "            'batch_size': batch_size,\n",
    "            'learning_rate': 2e-5,\n",
    "            'max_length': 128,\n",
    "            'num_classes': len(fine_grained_labels),\n",
    "            'hierarchical': True\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Create results directory\n",
    "    os.makedirs(\"evaluation/results/multi-label/BERT\", exist_ok=True)\n",
    "\n",
    "    # Save hierarchical results\n",
    "    hierarchical_results_file = f\"evaluation/results/multi-label/BERT/{model_name}_{dataset_name}_hierarchical_results.json\"\n",
    "\n",
    "    with open(hierarchical_results_file, 'w') as f:\n",
    "        json.dump(hierarchical_results, f, indent=2, cls=NumpyEncoder)\n",
    "\n",
    "    print(f\"✅ Hierarchical results saved to: {hierarchical_results_file}\")\n",
    "\n",
    "    # Display evaluation results (keep original functionality)\n",
    "    print(\"\\n📊 Multi-label classification evaluation:\")\n",
    "    evaluate_multilabel_classification(\n",
    "        gold_test_ml,\n",
    "        test_pred_json_ml,\n",
    "        y_true=test_df[label_names],\n",
    "        y_pred=hierarchical_predictions,\n",
    "        gold_labels_txt=gold_test_txt,\n",
    "        predictions_txt=test_pred_txt_ml,\n",
    "        label_names=label_names\n",
    "    )\n",
    "\n",
    "    # Print saved metrics (for verification)\n",
    "    print(f\"\\n📊 SAVED BERT HIERARCHICAL METRICS:\")\n",
    "    print(f\"   • Hierarchical F1: {multilabel_f1:.3f}\")\n",
    "    print(f\"   • Macro F1: {macro_f1:.3f}\")\n",
    "    print(f\"   • Macro Precision: {macro_precision:.3f}\")\n",
    "    print(f\"   • Macro Recall: {macro_recall:.3f}\")\n",
    "\n",
    "    # Print individual label metrics\n",
    "    print(f\"\\n📋 INDIVIDUAL LABEL METRICS:\")\n",
    "    for label, metrics in individual_metrics.items():\n",
    "        print(f\"   • {label}: F1={metrics['f1']:.3f}, P={metrics['precision']:.3f}, R={metrics['recall']:.3f}\")\n",
    "\n",
    "    # Save model state dict as well\n",
    "    model_save_dir = f\"models/bert_baseline/{dataset_name.lower()}\"\n",
    "    os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "    model_save_path = f\"{model_save_dir}/{model_name}_multilabel_final.pt\"\n",
    "    torch.save(ml_model.state_dict(), model_save_path)\n",
    "    print(f\"✅ Multi-label model saved to: {model_save_path}\")\n",
    "\n",
    "    return hierarchical_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eh_lW6VuFci4",
    "outputId": "4d501a55-8655-44c1-92d8-d2738c2836bb"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load binary model\n",
    "mami_bin_model = BertClassifier(n_classes=2).to(device)\n",
    "mami_bin_model.load_state_dict(torch.load('best_model_bert_baseline_mami.pt'))\n",
    "\n",
    "# Run multi-label classification\n",
    "mami_fine_grained_labels = [\"shaming\", \"stereotype\", \"objectification\", \"violence\"]\n",
    "\n",
    "mami_bert_multilabel_results = run_bert_baseline_ml(\n",
    "    dataset_name=\"MAMI\",\n",
    "    training_df=mami_training_df,\n",
    "    test_df=mami_test_df,\n",
    "    binary_model=mami_bin_model,\n",
    "    binary_label=\"misogynous\",\n",
    "    fine_grained_labels=mami_fine_grained_labels,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    batch_size=16,\n",
    "    route=route\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJv2n3Sv-clK"
   },
   "source": [
    "# EXIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "98XVRdA4-eIZ",
    "outputId": "70867604-8748-4c36-9640-05ffb009416a"
   },
   "outputs": [],
   "source": [
    "# Implement EXIST dataset classification\n",
    "print(\"\\n\\nLoading EXIST dataset...\")\n",
    "exist_training_data = route + '' # Path to training data\n",
    "exist_dev_data = route + '' # Path to development data\n",
    "exist_test_data = route + '' # Path to test data\n",
    "\n",
    "exist_training_df = pd.read_json(exist_training_data, orient='index')\n",
    "exist_dev_df = pd.read_json(exist_dev_data, orient='index')\n",
    "exist_test_df = pd.read_json(exist_test_data, orient='index')\n",
    "\n",
    "exist_training_df = pd.concat([exist_training_df, exist_dev_df]).sort_index()\n",
    "\n",
    "# Check available columns\n",
    "print(\"Training data columns:\", exist_training_df.columns)\n",
    "print(\"Test data columns:\", exist_test_df.columns)\n",
    "\n",
    "# Set up PyEvaLL evaluation gold files\n",
    "gold_test_bin = route + '' # Path to EXIST2024 binary classification gold labels (json) \n",
    "gold_test_txt = route + '' # Path to EXIST2024 text gold label (txt)\n",
    "gold_test_ml = route + '' # Path to EXIST2024 multi-label classification gold labels (json)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W5dQcrqf-mSq"
   },
   "source": [
    "## Binary Classification: Sexist vs. non-sexist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UIoGPY-1Kdhy",
    "outputId": "b90f484c-3075-4918-c94c-75cc701dcc68"
   },
   "outputs": [],
   "source": [
    "# Run EXIST BERT baseline\n",
    "exist_bert_results = run_bert_baseline_bin(\n",
    "    dataset_name=\"EXIST\",\n",
    "    training_df=exist_training_df,\n",
    "    test_df=exist_test_df,\n",
    "    binary_label=\"sexist\",\n",
    "    label_names=[\"non-sexist\", \"sexist\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ErEV-3_Z-ga-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ki4aqqc-tyZ"
   },
   "source": [
    "## Multi-label Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9qKkJhWy-0wk",
    "outputId": "7bd9cc74-53b9-43cd-a71c-fd90897555d5"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "exist_fine_grained_labels = [\"ideological-inequality\", \"stereotyping-dominance\",\n",
    "                            \"objectification\", \"sexual-violence\", \"misogyny-non-sexual-violence\"]\n",
    "\n",
    "# Load binary model\n",
    "exist_bin_model = BertClassifier(n_classes=2).to(device)\n",
    "exist_bin_model.load_state_dict(torch.load('best_model_bert_baseline_exist.pt'))\n",
    "\n",
    "# Run multi-label classification\n",
    "exist_bert_multilabel_results = run_bert_baseline_ml(\n",
    "    dataset_name=\"EXIST\",\n",
    "    training_df=exist_training_df,\n",
    "    test_df=exist_test_df,\n",
    "    binary_model=exist_bin_model,\n",
    "    binary_label=\"sexist\",\n",
    "    fine_grained_labels=exist_fine_grained_labels,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    batch_size=16,\n",
    "    route=route\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8Dn3zZYGLFk"
   },
   "source": [
    "# **Ablation Study**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQ6RTj9fyUmV"
   },
   "source": [
    "## Coarse-grained ablation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gW8k3rzjkO9s"
   },
   "source": [
    "### Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IjbeL8j8F9Qp"
   },
   "outputs": [],
   "source": [
    "# clean text for coarse-grained ablation and POS ablation\n",
    "def clean_text_sep_token(texts):\n",
    "    \"\"\"\n",
    "    Clean texts for consistent feature extraction across SVM and BERT experiments.\n",
    "    Removes BERT-specific tokens and standardizes text format.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    texts : list\n",
    "        List of texts to clean\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        List of cleaned texts\n",
    "    \"\"\"\n",
    "    cleaned = []\n",
    "    for text in texts:\n",
    "        # remove [SEP] token\n",
    "        clean_text = text.replace('[SEP]', ' ')\n",
    "        clean_text = ' '.join(clean_text.split())\n",
    "        cleaned.append(clean_text)\n",
    "    return cleaned\n",
    "\n",
    "def run_coarse_binary_bert_ablation_experiment(dataset_df_train, dataset_df_test, feature_type,\n",
    "                                               ablation_method, dataset_name, binary_label='misogynous'):\n",
    "    \"\"\"\n",
    "    Run a coarse-grained ablation experiment with BERT for binary classification.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset_df_train : DataFrame\n",
    "        Training dataset\n",
    "    dataset_df_test : DataFrame\n",
    "        Test dataset\n",
    "    feature_type : str\n",
    "        Type of feature:\n",
    "        - 'sentiment_pos': only positive sentiment words\n",
    "        - 'sentiment_neg': only negative sentiment words\n",
    "        - 'hate': all hate speech terms as a group\n",
    "        - 'function': all function words as a group\n",
    "    ablation_method : str\n",
    "        'mask' or 'remove'\n",
    "    dataset_name : str\n",
    "        Name of dataset (e.g., 'MAMI', 'EXIST2024')\n",
    "    binary_label : str\n",
    "        Column for binary classification ('misogynous' or 'sexist')\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict: Results of the ablation experiment\n",
    "    \"\"\"\n",
    "    # Download required NLTK data\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt_tab')\n",
    "    except LookupError:\n",
    "        print(\"Downloading required NLTK data...\")\n",
    "        nltk.download('punkt_tab')\n",
    "        nltk.download('punkt')\n",
    "\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt')\n",
    "\n",
    "    # Load necessary lexicons\n",
    "    # Note: Lexicon files are not included in this repository. \n",
    "    # Please download them from their respective sources before running this code.\n",
    "    print(f\"Loading lexicons for {feature_type} ablation...\")\n",
    "    nrc_lexicon = load_nrc_lexicon(route + \"NRC-Emotion-Lexicon/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\")\n",
    "    function_words_dict = load_function_words()\n",
    "    hurtlex_dict = load_hurtlex(route + \"hurtlex-master/lexica/EN/1.2/hurtlex_EN.tsv\")\n",
    "\n",
    "    # Get original texts and labels\n",
    "    X_train_orig_raw = dataset_df_train[\"bert representation\"].tolist()\n",
    "    X_test_orig_raw = dataset_df_test[\"bert representation\"].tolist()\n",
    "    y_train = dataset_df_train[binary_label].tolist()\n",
    "    y_test = dataset_df_test[binary_label].tolist()\n",
    "\n",
    "    # 🔥 KEY CHANGE: Clean texts for consistent feature extraction\n",
    "    print(\"🧹 Cleaning texts for feature extraction (removing BERT tokens)...\")\n",
    "    X_train_orig = clean_text_sep_token(X_train_orig_raw)\n",
    "    X_test_orig = clean_text_sep_token(X_test_orig_raw)\n",
    "\n",
    "    # Prepare containers for ablated data\n",
    "    X_train_feature_ablated = []\n",
    "    X_test_feature_ablated = []\n",
    "\n",
    "    print(f\"Running coarse-grained BERT ablation experiment for {feature_type}...\")\n",
    "\n",
    "    # Training set statistics\n",
    "    train_total_feature_words_processed = 0\n",
    "    train_documents_with_features = 0\n",
    "    train_feature_words_per_doc = []\n",
    "\n",
    "    # Test set statistics\n",
    "    test_total_feature_words_processed = 0\n",
    "    test_documents_with_features = 0\n",
    "    test_feature_words_per_doc = []\n",
    "\n",
    "    # Process training data\n",
    "    print(\"Processing training data...\")\n",
    "    for text in X_train_orig:\n",
    "        # Extract feature words based on feature type\n",
    "        feature_words = []\n",
    "        if feature_type == 'sentiment_pos':\n",
    "            feature_words = extract_sentiment_words(text, nrc_lexicon, 'positive')\n",
    "        elif feature_type == 'sentiment_neg':\n",
    "            feature_words = extract_sentiment_words(text, nrc_lexicon, 'negative')\n",
    "        elif feature_type == 'hate':\n",
    "            hate_words = []\n",
    "            for word in text.lower().split():\n",
    "                if word in hurtlex_dict:\n",
    "                    hate_words.append(word)\n",
    "            feature_words = list(set(hate_words))\n",
    "        elif feature_type == 'function':\n",
    "            function_words = []\n",
    "            for category in function_words_dict.keys():\n",
    "                category_words = extract_function_words(text, function_words_dict, category)\n",
    "                function_words.extend(category_words)\n",
    "            feature_words = list(set(function_words))\n",
    "\n",
    "        # Convert to set for faster lookup\n",
    "        feature_words_set = set(feature_words)\n",
    "\n",
    "        # Count feature words in this text\n",
    "        num_feature_words = len([word for word in text.lower().split() if word in feature_words_set])\n",
    "        train_feature_words_per_doc.append(num_feature_words)\n",
    "\n",
    "        if num_feature_words > 0:\n",
    "            train_documents_with_features += 1\n",
    "\n",
    "        # Apply feature ablation\n",
    "        if ablation_method == 'mask':\n",
    "            feature_ablated_text, count = replace_mask_features(text, feature_words, mask_token='[MASK]')\n",
    "        else:  # remove\n",
    "            feature_ablated_text, count = remove_features(text, feature_words)\n",
    "\n",
    "        X_train_feature_ablated.append(feature_ablated_text)\n",
    "        train_total_feature_words_processed += count\n",
    "\n",
    "    # Process test data\n",
    "    print(\"Processing test data...\")\n",
    "    for text in X_test_orig:\n",
    "        # Extract feature words based on feature type\n",
    "        feature_words = []\n",
    "        if feature_type == 'sentiment_pos':\n",
    "            feature_words = extract_sentiment_words(text, nrc_lexicon, 'positive')\n",
    "        elif feature_type == 'sentiment_neg':\n",
    "            feature_words = extract_sentiment_words(text, nrc_lexicon, 'negative')\n",
    "        elif feature_type == 'hate':\n",
    "            hate_words = []\n",
    "            for word in text.lower().split():\n",
    "                if word in hurtlex_dict:\n",
    "                    hate_words.append(word)\n",
    "            feature_words = list(set(hate_words))\n",
    "        elif feature_type == 'function':\n",
    "            function_words = []\n",
    "            for category in function_words_dict.keys():\n",
    "                category_words = extract_function_words(text, function_words_dict, category)\n",
    "                function_words.extend(category_words)\n",
    "            feature_words = list(set(function_words))\n",
    "\n",
    "        # Convert to set for faster lookup\n",
    "        feature_words_set = set(feature_words)\n",
    "\n",
    "        # Count feature words in this text\n",
    "        num_feature_words = len([word for word in text.lower().split() if word in feature_words_set])\n",
    "        test_feature_words_per_doc.append(num_feature_words)\n",
    "\n",
    "        if num_feature_words > 0:\n",
    "            test_documents_with_features += 1\n",
    "\n",
    "        # Apply feature ablation\n",
    "        if ablation_method == 'mask':\n",
    "            feature_ablated_text, count = replace_mask_features(text, feature_words, mask_token='[MASK]')\n",
    "        else:  # remove\n",
    "            feature_ablated_text, count = remove_features(text, feature_words)\n",
    "\n",
    "        X_test_feature_ablated.append(feature_ablated_text)\n",
    "        test_total_feature_words_processed += count\n",
    "\n",
    "    # Summary statistics\n",
    "    print(f\"\\n{'=' * 50}\")\n",
    "    print(f\"Processing Summary:\")\n",
    "    print(f\"Training set:\")\n",
    "    print(f\"  Total documents: {len(X_train_feature_ablated)}\")\n",
    "    print(f\"  Documents with features: {train_documents_with_features} ({train_documents_with_features/len(X_train_feature_ablated)*100:.2f}%)\")\n",
    "    print(f\"  Total feature words processed: {train_total_feature_words_processed}\")\n",
    "    print(f\"  Average feature words per document: {np.mean(train_feature_words_per_doc):.2f}\")\n",
    "    print(f\"Test set:\")\n",
    "    print(f\"  Total documents: {len(X_test_feature_ablated)}\")\n",
    "    print(f\"  Documents with features: {test_documents_with_features} ({test_documents_with_features/len(X_test_feature_ablated)*100:.2f}%)\")\n",
    "    print(f\"  Total feature words processed: {test_total_feature_words_processed}\")\n",
    "    print(f\"  Average feature words per document: {np.mean(test_feature_words_per_doc):.2f}\")\n",
    "    print(f\"{'=' * 50}\")\n",
    "\n",
    "    # Set up model name\n",
    "    model_name = f\"bert_{feature_type}_{ablation_method}_{dataset_name.lower()}\"\n",
    "\n",
    "    # Initialize BERT tokenizer and device\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Create datasets\n",
    "    print(\"Creating BERT datasets...\")\n",
    "    train_dataset = MemeDataset(\n",
    "        texts=X_train_feature_ablated,\n",
    "        labels=y_train,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=128\n",
    "    )\n",
    "\n",
    "    test_dataset = MemeDataset(\n",
    "        texts=X_test_feature_ablated,\n",
    "        labels=y_test,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=128\n",
    "    )\n",
    "\n",
    "    # Create data loaders\n",
    "    batch_size = 16\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Initialize BERT model\n",
    "    print(\"Initializing BERT model...\")\n",
    "    bert_model = BertClassifier(n_classes=2).to(device)\n",
    "    optimizer = AdamW(bert_model.parameters(), lr=2e-5)\n",
    "\n",
    "    epochs = 3\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    print(f\"Training BERT model for {epochs} epochs...\")\n",
    "    best_accuracy = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch {epoch + 1}/{epochs}')\n",
    "\n",
    "        train_acc, train_loss = train_epoch_bin(\n",
    "            bert_model,\n",
    "            train_loader,\n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            device\n",
    "        )\n",
    "\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "\n",
    "        val_acc, val_loss, _, _ = eval_model_bin(\n",
    "            bert_model,\n",
    "            test_loader,\n",
    "            device\n",
    "        )\n",
    "\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "        if val_acc > best_accuracy:\n",
    "            best_accuracy = val_acc\n",
    "            torch.save(bert_model.state_dict(), f'best_model_{model_name}.pt')\n",
    "            print('Best model saved!')\n",
    "\n",
    "    # Load the best model and make final predictions\n",
    "    print(\"Loading best model and making final predictions...\")\n",
    "    bert_model.load_state_dict(torch.load(f'best_model_{model_name}.pt'))\n",
    "    _, _, y_pred_feature, _ = eval_model_bin(bert_model, test_loader, device)\n",
    "\n",
    "    # Convert predictions to numpy array\n",
    "    y_pred_feature = np.array(y_pred_feature)\n",
    "\n",
    "    # Set up gold standard file paths based on dataset\n",
    "    if dataset_name == \"MAMI\":\n",
    "        gold_test_bin = route + '' # Path to MAMI binary classification gold labels (json) \n",
    "        gold_test_txt = route + '' # Path to MAMI text gold label (txt)\n",
    "        label_names = [\"non-misogynous\", \"misogynous\"]\n",
    "    else:  # EXIST2024\n",
    "        gold_test_bin = route + '' # Path to EXIST2024 binary classification gold labels (json) \n",
    "        gold_test_txt = route + '' # Path to EXIST2024 text gold label (txt)\n",
    "        label_names = [\"non-sexist\", \"sexist\"]\n",
    "\n",
    "    # Create files with predictions\n",
    "    print(f\"\\n{'='*20} BERT Feature Ablation Results {'='*20}\")\n",
    "    test_pred_json_feature, test_pred_txt_feature = save_evaluation(\n",
    "        dataset_df_test, \"evaluation/predictions\", dataset_name, \"test\", \"binary\",\n",
    "        model_name, y_pred_feature, binary_label, []\n",
    "    )\n",
    "\n",
    "    # Evaluate the model\n",
    "    feature_metrics = evaluate_binary_classification(\n",
    "        gold_test_bin, test_pred_json_feature, y_test, y_pred_feature,\n",
    "        gold_test_txt, test_pred_txt_feature, label_names, model_name=model_name\n",
    "    )\n",
    "\n",
    "    # Calculate additional metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred_feature)\n",
    "    precision_macro = precision_score(y_test, y_pred_feature, average='macro')\n",
    "    recall_macro = recall_score(y_test, y_pred_feature, average='macro')\n",
    "    f1_macro = f1_score(y_test, y_pred_feature, average='macro')\n",
    "\n",
    "    # Get classification report as dictionary\n",
    "    class_report_dict = classification_report(y_test, y_pred_feature,\n",
    "                                            target_names=label_names,\n",
    "                                            zero_division=0, digits=3,\n",
    "                                            output_dict=True)\n",
    "\n",
    "    # Calculate binary F1 score (MAMI evaluation metric)\n",
    "    binary_f1 = evaluate_f1_scores(gold_test_txt, test_pred_txt_feature, 2)\n",
    "\n",
    "    # Create structured results dictionary\n",
    "    results = {\n",
    "        'feature_type': feature_type,\n",
    "        'ablation_method': ablation_method,\n",
    "        'dataset_name': dataset_name,\n",
    "        'binary_label': binary_label,\n",
    "        'model_type': 'BERT',\n",
    "        'feature_ablation': {\n",
    "            'model_name': model_name,\n",
    "            'binary_f1': binary_f1,\n",
    "            'macro_f1': f1_macro,\n",
    "            'accuracy': accuracy,\n",
    "            'precision_macro': precision_macro,\n",
    "            'recall_macro': recall_macro,\n",
    "            'per_label_metrics': class_report_dict,\n",
    "            'prediction_files': {\n",
    "                'json': test_pred_json_feature,\n",
    "                'txt': test_pred_txt_feature\n",
    "            }\n",
    "        },\n",
    "        'train_statistics': {\n",
    "            'total_documents': len(X_train_feature_ablated),\n",
    "            'documents_with_features': train_documents_with_features,\n",
    "            'total_feature_words_processed': train_total_feature_words_processed,\n",
    "            'avg_feature_words_per_doc': np.mean(train_feature_words_per_doc),\n",
    "            'max_feature_words_per_doc': max(train_feature_words_per_doc) if train_feature_words_per_doc else 0,\n",
    "            'feature_coverage_percentage': train_documents_with_features/len(X_train_feature_ablated)*100\n",
    "        },\n",
    "        'test_statistics': {\n",
    "            'total_documents': len(X_test_feature_ablated),\n",
    "            'documents_with_features': test_documents_with_features,\n",
    "            'total_feature_words_processed': test_total_feature_words_processed,\n",
    "            'avg_feature_words_per_doc': np.mean(test_feature_words_per_doc),\n",
    "            'max_feature_words_per_doc': max(test_feature_words_per_doc) if test_feature_words_per_doc else 0,\n",
    "            'feature_coverage_percentage': test_documents_with_features/len(X_test_feature_ablated)*100\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Save results to JSON file for later analysis\n",
    "    os.makedirs(\"evaluation/results/binary/BERT\", exist_ok=True)\n",
    "    results_file = f\"evaluation/results/binary/BERT/{model_name}_bert_results.json\"\n",
    "\n",
    "    # Create the format for saving results\n",
    "    save_results = {\n",
    "        'binary_f1': binary_f1,\n",
    "        'macro_f1': f1_macro,\n",
    "        'accuracy': accuracy,\n",
    "        'precision_macro': precision_macro,\n",
    "        'recall_macro': recall_macro,\n",
    "        'per_label_metrics': class_report_dict,\n",
    "        'predictions': y_pred_feature.tolist(),  # Save actual predictions\n",
    "        'true_labels': y_test,  # Save true labels for reference\n",
    "        'processing_stats': results['train_statistics'],\n",
    "        'test_stats': results['test_statistics']\n",
    "    }\n",
    "\n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(save_results, f, indent=2, cls=NumpyEncoder)\n",
    "\n",
    "    print(f\"✅ BERT ablation results saved to: {results_file}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_bert_coarse_grained_experiments():\n",
    "    \"\"\"\n",
    "    Run all coarse-grained BERT ablation experiments for both datasets (MAMI and EXIST2024).\n",
    "    This focuses only on binary classification experiments with BERT.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"RUNNING COARSE-GRAINED BERT ABLATION EXPERIMENTS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Define feature types and ablation methods\n",
    "    feature_types = ['sentiment_pos', 'sentiment_neg', 'hate', 'function']\n",
    "    ablation_methods = ['mask', 'remove']\n",
    "\n",
    "    # Storage for results\n",
    "    all_results = {\n",
    "        'MAMI': {},\n",
    "        'EXIST2024': {}\n",
    "    }\n",
    "\n",
    "    # MAMI Dataset Experiments\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"MAMI DATASET BERT EXPERIMENTS\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    for feature_type in feature_types:\n",
    "        all_results['MAMI'][feature_type] = {}\n",
    "        for ablation_method in ablation_methods:\n",
    "            print(f\"\\n{'='*20} MAMI BERT: {feature_type.capitalize()} with {ablation_method} {'='*20}\")\n",
    "            results = run_coarse_binary_bert_ablation_experiment(\n",
    "                mami_training_df, mami_test_df, feature_type, ablation_method, \"MAMI\"\n",
    "            )\n",
    "            all_results['MAMI'][feature_type][ablation_method] = results\n",
    "\n",
    "    # EXIST2024 Dataset Experiments\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"EXIST2024 DATASET BERT EXPERIMENTS\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    for feature_type in feature_types:\n",
    "        all_results['EXIST2024'][feature_type] = {}\n",
    "        for ablation_method in ablation_methods:\n",
    "            print(f\"\\n{'='*20} EXIST2024 BERT: {feature_type.capitalize()} with {ablation_method} {'='*20}\")\n",
    "            results = run_coarse_binary_bert_ablation_experiment(\n",
    "                exist_training_df, exist_test_df, feature_type, ablation_method,\n",
    "                \"EXIST2024\", binary_label='sexist'\n",
    "            )\n",
    "            all_results['EXIST2024'][feature_type][ablation_method] = results\n",
    "\n",
    "    # Save overall results summary\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"SAVING OVERALL RESULTS SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    os.makedirs(\"evaluation/results/binary/BERT\", exist_ok=True)\n",
    "    summary_file = \"evaluation/results/binary/BERT/bert_coarse_ablation_summary.json\"\n",
    "\n",
    "    # Create summary with key metrics\n",
    "    summary = {}\n",
    "    for dataset in all_results:\n",
    "        summary[dataset] = {}\n",
    "        for feature_type in all_results[dataset]:\n",
    "            summary[dataset][feature_type] = {}\n",
    "            for ablation_method in all_results[dataset][feature_type]:\n",
    "                result = all_results[dataset][feature_type][ablation_method]\n",
    "                summary[dataset][feature_type][ablation_method] = {\n",
    "                    'binary_f1': result['feature_ablation']['binary_f1'],\n",
    "                    'macro_f1': result['feature_ablation']['macro_f1'],\n",
    "                    'accuracy': result['feature_ablation']['accuracy'],\n",
    "                    'feature_coverage_percentage': result['train_statistics']['feature_coverage_percentage'],\n",
    "                    'avg_feature_words_per_doc': result['train_statistics']['avg_feature_words_per_doc']\n",
    "                }\n",
    "\n",
    "    with open(summary_file, 'w') as f:\n",
    "        json.dump(summary, f, indent=2, cls=NumpyEncoder)\n",
    "\n",
    "    print(f\"✅ Overall BERT ablation summary saved to: {summary_file}\")\n",
    "\n",
    "    return all_results\n",
    "\n",
    "\n",
    "# Function to run individual experiments\n",
    "def run_mami_bert_ablation():\n",
    "    \"\"\"\n",
    "    Run coarse-grained BERT ablation experiments for MAMI dataset only.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    feature_types = ['sentiment_pos', 'sentiment_neg', 'hate', 'function']\n",
    "    ablation_methods = ['mask', 'remove']\n",
    "\n",
    "    for feature_type in feature_types:\n",
    "        results[feature_type] = {}\n",
    "        for ablation_method in ablation_methods:\n",
    "            print(f\"\\nRunning BERT ablation for {feature_type} with {ablation_method} on MAMI...\")\n",
    "            result = run_coarse_binary_bert_ablation_experiment(\n",
    "                mami_training_df, mami_test_df, feature_type, ablation_method, 'MAMI', 'misogynous'\n",
    "            )\n",
    "            results[feature_type][ablation_method] = result\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_exist_bert_ablation():\n",
    "    \"\"\"\n",
    "    Run coarse-grained BERT ablation experiments for EXIST2024 dataset only.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    feature_types = ['sentiment_pos', 'sentiment_neg', 'hate', 'function']\n",
    "    ablation_methods = ['mask', 'remove']\n",
    "\n",
    "    for feature_type in feature_types:\n",
    "        results[feature_type] = {}\n",
    "        for ablation_method in ablation_methods:\n",
    "            print(f\"\\nRunning BERT ablation for {feature_type} with {ablation_method} on EXIST2024...\")\n",
    "            result = run_coarse_binary_bert_ablation_experiment(\n",
    "                exist_training_df, exist_test_df, feature_type, ablation_method, 'EXIST2024', 'sexist'\n",
    "            )\n",
    "            results[feature_type][ablation_method] = result\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "brzQkdJLzJu-",
    "outputId": "55de0fe3-b8d7-4160-e56a-086e2ba00436"
   },
   "outputs": [],
   "source": [
    "# Run all BERT ablation experiments\n",
    "all_results = run_bert_coarse_grained_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWJ3fJx-kaFE"
   },
   "source": [
    "### Multi-label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ry_9jaHokfEE",
    "outputId": "8664b2d3-f901-4c31-be1e-9e202477bcbc"
   },
   "outputs": [],
   "source": [
    "def run_bert_coarse_grained_multilabel_ablation_experiments(dataset_df_train, dataset_df_test, feature_type,\n",
    "                                                         ablation_method, dataset_name, binary_label, fine_grained_labels):\n",
    "    \"\"\"\n",
    "    Run BERT multilabel ablation experiment for coarse-grained feature categories.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset_df_train : DataFrame\n",
    "        Training dataset\n",
    "    dataset_df_test : DataFrame\n",
    "        Test dataset\n",
    "    feature_type : str\n",
    "        Type of feature to ablate:\n",
    "        - 'sentiment_pos': Positive sentiment words\n",
    "        - 'sentiment_neg': Negative sentiment words\n",
    "        - 'hate': All hate speech terms\n",
    "        - 'function': All function words\n",
    "    ablation_method : str\n",
    "        Method for ablation ('mask' or 'remove')\n",
    "    dataset_name : str\n",
    "        Name of the dataset ('MAMI' or 'EXIST2024')\n",
    "    binary_label : str\n",
    "        Name of the binary label column\n",
    "    fine_grained_labels : list\n",
    "        List of fine-grained category label names\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"\\n🤖 RUNNING BERT COARSE-GRAINED MULTILABEL ABLATION\")\n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    print(f\"Feature: {feature_type}\")\n",
    "    print(f\"Method: {ablation_method}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Download required NLTK data if needed\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt_tab')\n",
    "    except LookupError:\n",
    "        print(\"Downloading required NLTK data...\")\n",
    "        nltk.download('punkt_tab')\n",
    "        nltk.download('punkt')\n",
    "\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt')\n",
    "\n",
    "    # Load necessary lexicons\n",
    "    print(\"Loading lexicons...\")\n",
    "    nrc_lexicon = load_nrc_lexicon(route + \"NRC-Emotion-Lexicon/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\")\n",
    "    function_words_dict = load_function_words()\n",
    "    hurtlex_dict = load_hurtlex(route + \"hurtlex-master/lexica/EN/1.2/hurtlex_EN.tsv\")\n",
    "\n",
    "    # Get original texts and labels (with text cleaning)\n",
    "    X_train_orig_raw = dataset_df_train[\"bert representation\"].tolist()\n",
    "    X_test_orig_raw = dataset_df_test[\"bert representation\"].tolist()\n",
    "\n",
    "    # Clean texts for consistent feature extraction\n",
    "    print(\"🧹 Cleaning texts for feature extraction (removing BERT tokens)...\")\n",
    "    X_train_orig = clean_text_sep_token(X_train_orig_raw)\n",
    "    X_test_orig = clean_text_sep_token(X_test_orig_raw)\n",
    "\n",
    "    # Get labels\n",
    "    y_train_binary = dataset_df_train[binary_label].tolist()\n",
    "    train_df_bin_positive = dataset_df_train.loc[dataset_df_train[binary_label] == 1]\n",
    "    X_train_bin_positive_raw = train_df_bin_positive[\"bert representation\"].tolist()\n",
    "    X_train_bin_positive = clean_text_sep_token(X_train_bin_positive_raw)\n",
    "    y_train_categories = train_df_bin_positive[fine_grained_labels].values.tolist()\n",
    "\n",
    "    # Prepare complete label list (binary + fine-grained)\n",
    "    all_labels = [binary_label] + fine_grained_labels\n",
    "    y_test_all = dataset_df_test[all_labels]\n",
    "\n",
    "    # Create display name and model name for feature type\n",
    "    display_name = f\"Coarse-Grained: {feature_type.replace('_', ' ').title()}\"\n",
    "    feature_name_for_model = feature_type\n",
    "\n",
    "    print(f\"Running BERT multilabel coarse-grained ablation for {display_name} with {ablation_method}...\")\n",
    "\n",
    "    # Statistics tracking\n",
    "    total_feature_words_processed = 0\n",
    "    documents_with_features = 0\n",
    "    feature_words_per_doc = []\n",
    "\n",
    "    # Define function to extract coarse-grained features\n",
    "    def extract_coarse_grained_features(text):\n",
    "        \"\"\"Helper function to extract coarse-grained feature words\"\"\"\n",
    "        if feature_type == 'sentiment_pos':\n",
    "            return extract_sentiment_words(text, nrc_lexicon, 'positive')\n",
    "        elif feature_type == 'sentiment_neg':\n",
    "            return extract_sentiment_words(text, nrc_lexicon, 'negative')\n",
    "        elif feature_type == 'hate':\n",
    "            hate_words = []\n",
    "            for word in text.lower().split():\n",
    "                if word in hurtlex_dict:\n",
    "                    hate_words.append(word)\n",
    "            return list(set(hate_words))\n",
    "        elif feature_type == 'function':\n",
    "            function_words = []\n",
    "            for category in function_words_dict.keys():\n",
    "                category_words = extract_function_words(text, function_words_dict, category)\n",
    "                function_words.extend(category_words)\n",
    "            return list(set(function_words))\n",
    "        else:\n",
    "            print(f\"Warning: Unknown coarse-grained feature type '{feature_type}'. No features will be ablated.\")\n",
    "            return []\n",
    "\n",
    "    # Process training data\n",
    "    X_train_ablated = []\n",
    "    X_train_bin_pos_ablated = []\n",
    "    X_test_ablated = []\n",
    "\n",
    "    print(\"Processing training data...\")\n",
    "    for text in X_train_orig:\n",
    "        feature_words = extract_coarse_grained_features(text)\n",
    "        feature_words_set = set(feature_words)\n",
    "\n",
    "        # Count feature words in this text\n",
    "        num_feature_words = len([word for word in text.lower().split() if word in feature_words_set])\n",
    "        feature_words_per_doc.append(num_feature_words)\n",
    "\n",
    "        if num_feature_words > 0:\n",
    "            documents_with_features += 1\n",
    "\n",
    "        # Apply feature ablation\n",
    "        if ablation_method == 'mask':\n",
    "            ablated_text, count = replace_mask_features(text, feature_words, mask_token='[MASK]')\n",
    "        elif ablation_method == 'remove':\n",
    "            ablated_text, count = remove_features(text, feature_words)\n",
    "\n",
    "        X_train_ablated.append(ablated_text)\n",
    "        total_feature_words_processed += count\n",
    "\n",
    "    # Process positive-only training data\n",
    "    print(\"Processing positive training data...\")\n",
    "    for text in X_train_bin_positive:\n",
    "        feature_words = extract_coarse_grained_features(text)\n",
    "\n",
    "        if ablation_method == 'mask':\n",
    "            ablated_text, _ = replace_mask_features(text, feature_words, mask_token='[MASK]')\n",
    "        elif ablation_method == 'remove':\n",
    "            ablated_text, _ = remove_features(text, feature_words)\n",
    "\n",
    "        X_train_bin_pos_ablated.append(ablated_text)\n",
    "\n",
    "    # Process test data\n",
    "    print(\"Processing test data...\")\n",
    "    for text in X_test_orig:\n",
    "        feature_words = extract_coarse_grained_features(text)\n",
    "\n",
    "        if ablation_method == 'mask':\n",
    "            ablated_text, _ = replace_mask_features(text, feature_words, mask_token='[MASK]')\n",
    "        elif ablation_method == 'remove':\n",
    "            ablated_text, _ = remove_features(text, feature_words)\n",
    "\n",
    "        X_test_ablated.append(ablated_text)\n",
    "\n",
    "    # Summary statistics\n",
    "    print(f\"\\n{'=' * 50}\")\n",
    "    print(f\"Processing Summary:\")\n",
    "    print(f\"Total documents: {len(X_train_ablated)}\")\n",
    "    print(f\"Documents with features: {documents_with_features} ({documents_with_features/len(X_train_ablated)*100:.2f}%)\")\n",
    "    print(f\"Total feature words processed: {total_feature_words_processed}\")\n",
    "    print(f\"Average feature words per document: {np.mean(feature_words_per_doc):.2f}\")\n",
    "    print(f\"Max words per document: {max(feature_words_per_doc) if feature_words_per_doc else 0}\")\n",
    "    print(f\"{'=' * 50}\")\n",
    "\n",
    "    # Create model name\n",
    "    model_name = f\"{dataset_name}_bert_ablation_{feature_name_for_model}_{ablation_method}_multilabel\"\n",
    "\n",
    "    try:\n",
    "        # Initialize BERT tokenizer and device\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {device}\")\n",
    "\n",
    "        # STEP 1: Train binary classifier on ablated data\n",
    "        print(f\"\\n🚀 Training BERT binary classifier on ablated data...\")\n",
    "\n",
    "        # Create binary datasets\n",
    "        train_dataset_bin = MemeDataset(\n",
    "            texts=X_train_ablated,\n",
    "            labels=y_train_binary,\n",
    "            tokenizer=tokenizer,\n",
    "            max_len=128\n",
    "        )\n",
    "\n",
    "        test_dataset_bin = MemeDataset(\n",
    "            texts=X_test_ablated,\n",
    "            labels=[0] * len(X_test_ablated),  # Dummy labels for test\n",
    "            tokenizer=tokenizer,\n",
    "            max_len=128\n",
    "        )\n",
    "\n",
    "        # Create data loaders\n",
    "        batch_size = 16\n",
    "        train_loader_bin = DataLoader(\n",
    "            train_dataset_bin,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        test_loader_bin = DataLoader(\n",
    "            test_dataset_bin,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "\n",
    "        # Train binary model\n",
    "        binary_model = BertClassifier(n_classes=2).to(device)\n",
    "        optimizer_bin = AdamW(binary_model.parameters(), lr=2e-5)\n",
    "\n",
    "        epochs = 3\n",
    "        total_steps = len(train_loader_bin) * epochs\n",
    "        scheduler_bin = get_linear_schedule_with_warmup(\n",
    "            optimizer_bin,\n",
    "            num_warmup_steps=0,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "\n",
    "        best_accuracy = 0\n",
    "        for epoch in range(epochs):\n",
    "            print(f'Binary Epoch {epoch + 1}/{epochs}')\n",
    "\n",
    "            train_acc, train_loss = train_epoch_bin(\n",
    "                binary_model,\n",
    "                train_loader_bin,\n",
    "                optimizer_bin,\n",
    "                scheduler_bin,\n",
    "                device\n",
    "            )\n",
    "\n",
    "            print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "\n",
    "        # STEP 2: Train multi-label classifier on positive samples\n",
    "        print(f\"\\n🏷️ Training BERT multi-label classifier...\")\n",
    "\n",
    "        # Create multi-label datasets\n",
    "        ml_train_dataset = MemeMultiLabelDataset(\n",
    "            texts=X_train_bin_pos_ablated,\n",
    "            labels=y_train_categories,\n",
    "            tokenizer=tokenizer,\n",
    "            max_len=128\n",
    "        )\n",
    "\n",
    "        ml_train_loader = DataLoader(\n",
    "            ml_train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        # Train multi-label model\n",
    "        ml_model = BertMultiLabelClassifier(n_classes=len(fine_grained_labels)).to(device)\n",
    "        ml_optimizer = AdamW(ml_model.parameters(), lr=2e-5)\n",
    "\n",
    "        ml_total_steps = len(ml_train_loader) * epochs\n",
    "        ml_scheduler = get_linear_schedule_with_warmup(\n",
    "            ml_optimizer,\n",
    "            num_warmup_steps=0,\n",
    "            num_training_steps=ml_total_steps\n",
    "        )\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print(f'Multi-label Epoch {epoch + 1}/{epochs}')\n",
    "\n",
    "            train_loss = train_epoch_multilabel(\n",
    "                ml_model,\n",
    "                ml_train_loader,\n",
    "                ml_optimizer,\n",
    "                ml_scheduler,\n",
    "                device\n",
    "            )\n",
    "\n",
    "            print(f'Multi-label Train Loss: {train_loss:.4f}')\n",
    "\n",
    "        # STEP 3: Make hierarchical predictions\n",
    "        print(f\"\\n🔮 Making hierarchical predictions...\")\n",
    "        hierarchical_predictions = predict_hierarchical(\n",
    "            binary_model,\n",
    "            ml_model,\n",
    "            test_loader_bin,\n",
    "            tokenizer,\n",
    "            device,\n",
    "            fine_grained_labels,\n",
    "            test_texts=X_test_ablated,\n",
    "            threshold=0.5,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "\n",
    "        # Convert predictions to DataFrame\n",
    "        test_pred_df = pd.DataFrame(\n",
    "            hierarchical_predictions,\n",
    "            columns=all_labels\n",
    "        )\n",
    "\n",
    "        print(f\"Hierarchical predictions shape: {hierarchical_predictions.shape}\")\n",
    "        print(f\"Test predictions DataFrame shape: {test_pred_df.shape}\")\n",
    "\n",
    "        # Get gold file paths based on dataset\n",
    "        if dataset_name == \"MAMI\":\n",
    "            gold_test_ml = route + '' # Path to MAMI multi-label classification gold labels (json)  \n",
    "            gold_test_txt = route + '' # Path to MAMI text gold label (txt)\n",
    "        else:  # EXIST2024\n",
    "            gold_test_ml = route + '' # Path to EXIST2024 multi-label classification gold labels (json)  \n",
    "            gold_test_txt = route + ''# Path to EXIST2024 text gold label (txt)\n",
    "\n",
    "        evaluation_type = \"hierarchical\"\n",
    "\n",
    "        # Save evaluation results\n",
    "        print(f\"\\nSaving evaluation results for: {model_name}\")\n",
    "        test_pred_json_ml, test_pred_txt_ml = save_evaluation(\n",
    "            dataset_df_test, \"evaluation/predictions\", dataset_name, \"test\",\n",
    "            evaluation_type, model_name, test_pred_df, binary_label, all_labels\n",
    "        )\n",
    "\n",
    "        # Evaluate the model\n",
    "        print(f\"\\n{'='*20} Evaluation Results for {display_name} {'='*20}\")\n",
    "        evaluate_multilabel_classification(\n",
    "            gold_test_ml, test_pred_json_ml,\n",
    "            y_test_all, hierarchical_predictions,\n",
    "            gold_test_txt, test_pred_txt_ml,\n",
    "            all_labels, hierarchy=True\n",
    "        )\n",
    "\n",
    "        # Calculate detailed metrics for saving\n",
    "        y_true_df = dataset_df_test[all_labels]\n",
    "\n",
    "        # Calculate macro F1 score for hierarchical evaluation\n",
    "        hierarchical_scores = evaluate_f1_scores(gold_test_txt, test_pred_txt_ml, len(all_labels))\n",
    "        if isinstance(hierarchical_scores, tuple):\n",
    "            binary_f1, multilabel_f1 = hierarchical_scores\n",
    "        else:\n",
    "            binary_f1 = hierarchical_scores\n",
    "            multilabel_f1 = hierarchical_scores\n",
    "\n",
    "        # Calculate individual label metrics\n",
    "        binary_true = y_true_df[binary_label].values\n",
    "        binary_pred = hierarchical_predictions[:, 0]\n",
    "\n",
    "        # Create negative class representation\n",
    "        negative_true = np.zeros((len(hierarchical_predictions), 1))\n",
    "        negative_pred = np.zeros((len(hierarchical_predictions), 1))\n",
    "        negative_true[:, 0] = (binary_true == 0)\n",
    "        negative_pred[:, 0] = (binary_pred == 0)\n",
    "\n",
    "        # Get multi-label data\n",
    "        multilabel_true = y_true_df[fine_grained_labels].values\n",
    "        multilabel_pred = hierarchical_predictions[:, 1:]\n",
    "\n",
    "        # Create combined representation\n",
    "        combined_true = np.hstack((negative_true, multilabel_true))\n",
    "        combined_pred = np.hstack((negative_pred, multilabel_pred))\n",
    "\n",
    "        # Updated label names\n",
    "        updated_label_names = [f\"non-{binary_label}\"] + fine_grained_labels\n",
    "\n",
    "        # Calculate individual metrics\n",
    "        individual_metrics = {}\n",
    "        for i, label in enumerate(updated_label_names):\n",
    "            y_true_label = combined_true[:, i]\n",
    "            y_pred_label = combined_pred[:, i]\n",
    "\n",
    "            accuracy = accuracy_score(y_true_label, y_pred_label)\n",
    "            precision = precision_score(y_true_label, y_pred_label, zero_division=0)\n",
    "            recall = recall_score(y_true_label, y_pred_label, zero_division=0)\n",
    "            f1 = f1_score(y_true_label, y_pred_label, zero_division=0)\n",
    "\n",
    "            individual_metrics[label] = {\n",
    "                'accuracy': accuracy,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1\n",
    "            }\n",
    "\n",
    "        # Calculate overall macro metrics\n",
    "        all_f1_scores = [individual_metrics[label]['f1'] for label in updated_label_names]\n",
    "        all_precision_scores = [individual_metrics[label]['precision'] for label in updated_label_names]\n",
    "        all_recall_scores = [individual_metrics[label]['recall'] for label in updated_label_names]\n",
    "\n",
    "        macro_f1 = np.mean(all_f1_scores)\n",
    "        macro_precision = np.mean(all_precision_scores)\n",
    "        macro_recall = np.mean(all_recall_scores)\n",
    "\n",
    "        # Create comprehensive metrics dictionary\n",
    "        metrics = {\n",
    "            'binary_f1': binary_f1,\n",
    "            'hierarchical_f1': multilabel_f1,\n",
    "            'macro_f1': macro_f1,\n",
    "            'macro_precision': macro_precision,\n",
    "            'macro_recall': macro_recall,\n",
    "            'individual_label_metrics': individual_metrics,\n",
    "            'predictions': hierarchical_predictions.tolist(),\n",
    "            'true_labels': y_test_all.values.tolist(),\n",
    "            'prediction_files': {\n",
    "                'json': test_pred_json_ml,\n",
    "                'txt': test_pred_txt_ml\n",
    "            },\n",
    "            'all_labels': all_labels,\n",
    "            'feature_type': feature_type,\n",
    "            'ablation_method': ablation_method,\n",
    "            'model_config': {\n",
    "                'model_type': 'BERT',\n",
    "                'model_name': 'bert-base-uncased',\n",
    "                'epochs': epochs,\n",
    "                'batch_size': batch_size,\n",
    "                'learning_rate': 2e-5,\n",
    "                'max_length': 128,\n",
    "                'num_classes': len(fine_grained_labels),\n",
    "                'hierarchical': True\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Save results\n",
    "        os.makedirs(\"evaluation/results/multi-label/BERT/coarse-grained\", exist_ok=True)\n",
    "        results_file = f\"evaluation/results/multi-label/BERT/coarse-grained/{model_name}_results.json\"\n",
    "\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(metrics, f, indent=2, cls=NumpyEncoder)\n",
    "\n",
    "        print(f\"✅ BERT multilabel results saved to: {results_file}\")\n",
    "\n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'metrics': metrics,\n",
    "            'ablation_stats': {\n",
    "                'total_documents': len(X_train_ablated),\n",
    "                'documents_with_features': documents_with_features,\n",
    "                'percentage_with_features': documents_with_features/len(X_train_ablated)*100,\n",
    "                'total_feature_words': total_feature_words_processed,\n",
    "                'avg_feature_words': np.mean(feature_words_per_doc),\n",
    "                'max_feature_words': max(feature_words_per_doc) if feature_words_per_doc else 0\n",
    "            }\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in BERT multilabel experiment: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print(f\"Skipping BERT multilabel coarse-grained ablation for {feature_type} with {ablation_method}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def run_all_bert_coarse_grained_multilabel_experiments():\n",
    "    \"\"\"\n",
    "    Run all BERT coarse-grained multilabel ablation experiments for both datasets.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"RUNNING ALL BERT COARSE-GRAINED MULTILABEL ABLATION EXPERIMENTS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Define feature types and ablation methods\n",
    "    feature_types = ['sentiment_pos', 'sentiment_neg', 'hate', 'function']\n",
    "    ablation_methods = ['remove']\n",
    "\n",
    "    # Storage for results\n",
    "    all_results = {\n",
    "        'MAMI': {},\n",
    "        'EXIST2024': {}\n",
    "    }\n",
    "\n",
    "    # MAMI Dataset Experiments\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"MAMI DATASET BERT MULTILABEL EXPERIMENTS\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    mami_fine_grained_labels = [\"shaming\", \"stereotype\", \"objectification\", \"violence\"]\n",
    "\n",
    "    for feature_type in feature_types:\n",
    "        all_results['MAMI'][feature_type] = {}\n",
    "        for ablation_method in ablation_methods:\n",
    "            print(f\"\\n{'='*20} MAMI BERT Multilabel: {feature_type.capitalize()} with {ablation_method} {'='*20}\")\n",
    "            results = run_bert_coarse_grained_multilabel_ablation_experiments(\n",
    "                mami_training_df, mami_test_df, feature_type, ablation_method,\n",
    "                \"MAMI\", \"misogynous\", mami_fine_grained_labels\n",
    "            )\n",
    "            all_results['MAMI'][feature_type][ablation_method] = results\n",
    "\n",
    "    # EXIST2024 Dataset Experiments\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"EXIST2024 DATASET BERT MULTILABEL EXPERIMENTS\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    exist_fine_grained_labels = [\"ideological-inequality\", \"stereotyping-dominance\",\n",
    "                                \"objectification\", \"sexual-violence\", \"misogyny-non-sexual-violence\"]\n",
    "\n",
    "    for feature_type in feature_types:\n",
    "        all_results['EXIST2024'][feature_type] = {}\n",
    "        for ablation_method in ablation_methods:\n",
    "            print(f\"\\n{'='*20} EXIST2024 BERT Multilabel: {feature_type.capitalize()} with {ablation_method} {'='*20}\")\n",
    "            results = run_bert_coarse_grained_multilabel_ablation_experiments(\n",
    "                exist_training_df, exist_test_df, feature_type, ablation_method,\n",
    "                \"EXIST2024\", \"sexist\", exist_fine_grained_labels\n",
    "            )\n",
    "            all_results['EXIST2024'][feature_type][ablation_method] = results\n",
    "\n",
    "    # Save overall results summary\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"SAVING OVERALL BERT MULTILABEL RESULTS SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    os.makedirs(\"evaluation/results/multi-label/BERT/coarse-grained\", exist_ok=True)\n",
    "    summary_file = \"evaluation/results/multi-label/BERT/coarse-grained/bert_coarse_multilabel_summary.json\"\n",
    "\n",
    "    # Create summary with key metrics\n",
    "    summary = {}\n",
    "    for dataset in all_results:\n",
    "        summary[dataset] = {}\n",
    "        for feature_type in all_results[dataset]:\n",
    "            summary[dataset][feature_type] = {}\n",
    "            for ablation_method in all_results[dataset][feature_type]:\n",
    "                result = all_results[dataset][feature_type][ablation_method]\n",
    "                if result is not None:\n",
    "                    summary[dataset][feature_type][ablation_method] = {\n",
    "                        'hierarchical_f1': result['metrics']['hierarchical_f1'],\n",
    "                        'macro_f1': result['metrics']['macro_f1'],\n",
    "                        'binary_f1': result['metrics']['binary_f1'],\n",
    "                        'feature_coverage_percentage': result['ablation_stats']['percentage_with_features'],\n",
    "                        'avg_feature_words_per_doc': result['ablation_stats']['avg_feature_words']\n",
    "                    }\n",
    "                else:\n",
    "                    summary[dataset][feature_type][ablation_method] = None\n",
    "\n",
    "    with open(summary_file, 'w') as f:\n",
    "        json.dump(summary, f, indent=2, cls=NumpyEncoder)\n",
    "\n",
    "    print(f\"✅ Overall BERT multilabel summary saved to: {summary_file}\")\n",
    "\n",
    "    return all_results\n",
    "\n",
    "\n",
    "def run_mami_bert_multilabel_ablation():\n",
    "    \"\"\"\n",
    "    Run BERT coarse-grained multilabel ablation experiments for MAMI dataset only.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    feature_types = ['sentiment_pos', 'sentiment_neg', 'hate', 'function']\n",
    "    ablation_methods = ['remove']\n",
    "    mami_fine_grained_labels = [\"shaming\", \"stereotype\", \"objectification\", \"violence\"]\n",
    "\n",
    "    for feature_type in feature_types:\n",
    "        results[feature_type] = {}\n",
    "        for ablation_method in ablation_methods:\n",
    "            print(f\"\\nRunning BERT multilabel ablation for {feature_type} with {ablation_method} on MAMI...\")\n",
    "            result = run_bert_coarse_grained_multilabel_ablation_experiments(\n",
    "                mami_training_df, mami_test_df, feature_type, ablation_method,\n",
    "                'MAMI', 'misogynous', mami_fine_grained_labels\n",
    "            )\n",
    "            results[feature_type][ablation_method] = result\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_exist_bert_multilabel_ablation():\n",
    "    \"\"\"\n",
    "    Run BERT coarse-grained multilabel ablation experiments for EXIST2024 dataset only.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    feature_types = ['sentiment_pos', 'sentiment_neg', 'hate', 'function']\n",
    "    ablation_methods = ['remove']\n",
    "    exist_fine_grained_labels = [\"ideological-inequality\", \"stereotyping-dominance\",\n",
    "                                \"objectification\", \"sexual-violence\", \"misogyny-non-sexual-violence\"]\n",
    "\n",
    "    for feature_type in feature_types:\n",
    "        results[feature_type] = {}\n",
    "        for ablation_method in ablation_methods:\n",
    "            print(f\"\\nRunning BERT multilabel ablation for {feature_type} with {ablation_method} on EXIST2024...\")\n",
    "            result = run_bert_coarse_grained_multilabel_ablation_experiments(\n",
    "                exist_training_df, exist_test_df, feature_type, ablation_method,\n",
    "                'EXIST2024', 'sexist', exist_fine_grained_labels\n",
    "            )\n",
    "            results[feature_type][ablation_method] = result\n",
    "\n",
    "    return results\n",
    "\n",
    "run_mami_bert_multilabel_ablation()\n",
    "run_exist_bert_multilabel_ablation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jxwaVXSG8Fq"
   },
   "source": [
    "## POS tag ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gsYqnXdz2Q7z"
   },
   "outputs": [],
   "source": [
    "def run_bert_pos_ablation_experiment(dataset_df_train, dataset_df_test, pos_category,\n",
    "                                      dataset_name, binary_label='misogynous',\n",
    "                                      train_pos_tags=None, test_pos_tags=None):\n",
    "    \"\"\"\n",
    "    Run BERT POS ablation experiment for a specific POS category.\n",
    "    NOW WITH PROPER TEXT CLEANING!\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"BERT POS Ablation Experiment: {pos_category}\")\n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Get original texts and labels\n",
    "    X_train_orig_raw = dataset_df_train[\"bert representation\"].tolist()\n",
    "    X_test_orig_raw = dataset_df_test[\"bert representation\"].tolist()\n",
    "    y_train = dataset_df_train[binary_label].tolist()\n",
    "    y_test = dataset_df_test[binary_label].tolist()\n",
    "\n",
    "    # 🔥 KEY CHANGE: Clean texts for POS analysis\n",
    "    print(\"🧹 Cleaning texts for POS analysis (removing [SEP] tokens)...\")\n",
    "    X_train_orig = clean_text_sep_token(X_train_orig_raw)\n",
    "    X_test_orig = clean_text_sep_token(X_test_orig_raw)\n",
    "\n",
    "    # Show cleaning effect\n",
    "    print(f\"Sample cleaning:\")\n",
    "    print(f\"  Before: '{X_train_orig_raw[0]}'\")\n",
    "    print(f\"  After:  '{X_train_orig[0]}'\")\n",
    "\n",
    "    # Create ablated datasets using precomputed POS tags\n",
    "    print(\"\\nCreating ablated training set...\")\n",
    "    X_train_ablated, train_stats = create_pos_ablated_dataset(\n",
    "        X_train_orig, pos_category, train_pos_tags)\n",
    "\n",
    "    print(\"\\nCreating ablated test set...\")\n",
    "    X_test_ablated, test_stats = create_pos_ablated_dataset(\n",
    "        X_test_orig, pos_category, test_pos_tags)\n",
    "\n",
    "    # Print statistics\n",
    "    print(f\"\\n{'-'*50}\")\n",
    "    print(\"ABLATION STATISTICS:\")\n",
    "    print(f\"{'-'*50}\")\n",
    "    print(f\"Training set:\")\n",
    "    print(f\"  - Documents affected: {train_stats['documents_affected']}/{train_stats['total_documents']} ({train_stats['percentage_docs_affected']:.2f}%)\")\n",
    "    print(f\"  - Total words removed: {train_stats['total_words_removed']}\")\n",
    "    print(f\"  - Max words removed: {train_stats['max_words_removed']}\")\n",
    "\n",
    "    print(f\"\\nTest set:\")\n",
    "    print(f\"  - Documents affected: {test_stats['documents_affected']}/{test_stats['total_documents']} ({test_stats['percentage_docs_affected']:.2f}%)\")\n",
    "    print(f\"  - Total words removed: {test_stats['total_words_removed']}\")\n",
    "    print(f\"  - Max words removed: {test_stats['max_words_removed']}\")\n",
    "\n",
    "    # Train BERT model on ablated data\n",
    "    print(f\"\\n{'-'*50}\")\n",
    "    print(\"TRAINING BERT MODEL ON ABLATED DATA:\")\n",
    "    print(f\"{'-'*50}\")\n",
    "\n",
    "    try:\n",
    "        # Initialize BERT components\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {device}\")\n",
    "\n",
    "        # Create BERT datasets with cleaned ablated texts\n",
    "        train_dataset = MemeDataset(\n",
    "            texts=X_train_ablated,\n",
    "            labels=y_train,\n",
    "            tokenizer=tokenizer,\n",
    "            max_len=128\n",
    "        )\n",
    "\n",
    "        test_dataset = MemeDataset(\n",
    "            texts=X_test_ablated,\n",
    "            labels=y_test,\n",
    "            tokenizer=tokenizer,\n",
    "            max_len=128\n",
    "        )\n",
    "\n",
    "        # Create data loaders\n",
    "        batch_size = 16\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "\n",
    "        # Initialize BERT model\n",
    "        bert_model = BertClassifier(n_classes=2).to(device)\n",
    "        optimizer = AdamW(bert_model.parameters(), lr=2e-5)\n",
    "\n",
    "        epochs = 3\n",
    "        total_steps = len(train_loader) * epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=0,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        print(f\"Training BERT model for {epochs} epochs...\")\n",
    "        best_accuracy = 0\n",
    "        model_name = f\"bert_pos_ablation_clean_{pos_category.lower()}_{dataset_name.lower()}\"\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print(f'Epoch {epoch + 1}/{epochs}')\n",
    "\n",
    "            train_acc, train_loss = train_epoch_bin(\n",
    "                bert_model,\n",
    "                train_loader,\n",
    "                optimizer,\n",
    "                scheduler,\n",
    "                device\n",
    "            )\n",
    "\n",
    "            print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "\n",
    "            val_acc, val_loss, _, _ = eval_model_bin(\n",
    "                bert_model,\n",
    "                test_loader,\n",
    "                device\n",
    "            )\n",
    "\n",
    "            print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "            if val_acc > best_accuracy:\n",
    "                best_accuracy = val_acc\n",
    "                torch.save(bert_model.state_dict(), f'best_model_{model_name}.pt')\n",
    "                print('Best model saved!')\n",
    "\n",
    "        # Load best model and make final predictions\n",
    "        bert_model.load_state_dict(torch.load(f'best_model_{model_name}.pt'))\n",
    "        _, _, y_pred, _ = eval_model_bin(bert_model, test_loader, device)\n",
    "        y_pred = np.array(y_pred)\n",
    "\n",
    "        # Set up evaluation parameters\n",
    "        evaluation_type = \"binary\"\n",
    "\n",
    "        # Set up gold standard file paths\n",
    "        if dataset_name == \"MAMI\":\n",
    "            gold_test_bin = route + '' # Path to MAMI binary classification gold labels (json) \n",
    "            gold_test_txt = route +'' # Path to MAMI text gold label (txt)\n",
    "            label_names = [\"non-misogynous\", \"misogynous\"]\n",
    "        else:  # EXIST2024\n",
    "            gold_test_bin = route + '' # Path to EXIST2024 binary classification gold labels (json) \n",
    "            gold_test_txt = route + '' # Path to EXIST2024 text gold label (txt)\n",
    "            label_names = [\"non-sexist\", \"sexist\"]\n",
    "\n",
    "        # Save predictions and evaluate\n",
    "        test_pred_json, test_pred_txt = save_evaluation(\n",
    "            dataset_df_test, \"evaluation/predictions\", dataset_name, \"test\",\n",
    "            evaluation_type, model_name, y_pred, binary_label, []\n",
    "        )\n",
    "\n",
    "        print(f\"\\n{'-'*50}\")\n",
    "        print(\"EVALUATION RESULTS:\")\n",
    "        print(f\"{'-'*50}\")\n",
    "\n",
    "        # Use existing evaluation function\n",
    "        evaluate_binary_classification(\n",
    "            gold_test_bin, test_pred_json, y_test, y_pred,\n",
    "            gold_test_txt, test_pred_txt, label_names,\n",
    "            model_name=f\"BERT POS Ablation ({pos_category}) - CLEANED\"\n",
    "        )\n",
    "\n",
    "        # Calculate all required metrics\n",
    "        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision_macro = precision_score(y_test, y_pred, average='macro')\n",
    "        recall_macro = recall_score(y_test, y_pred, average='macro')\n",
    "        f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "        # Get classification report as dictionary\n",
    "        class_report_dict = classification_report(y_test, y_pred,\n",
    "                                                target_names=label_names,\n",
    "                                                zero_division=0, digits=3,\n",
    "                                                output_dict=True)\n",
    "\n",
    "        # Calculate binary F1 score (MAMI evaluation metric)\n",
    "        binary_f1 = evaluate_f1_scores(gold_test_txt, test_pred_txt, 2)\n",
    "\n",
    "        # Create structured results dictionary\n",
    "        pos_results = {\n",
    "            'model_type': 'BERT_CLEANED',\n",
    "            'pos_category': pos_category,\n",
    "            'binary_f1': binary_f1,\n",
    "            'macro_f1': f1_macro,\n",
    "            'accuracy': accuracy,\n",
    "            'precision_macro': precision_macro,\n",
    "            'recall_macro': recall_macro,\n",
    "            'per_label_metrics': class_report_dict,\n",
    "            'confusion_matrix': confusion_matrix(y_test, y_pred).tolist(),\n",
    "            'label_names': label_names,\n",
    "            'train_stats': train_stats,\n",
    "            'test_stats': test_stats,\n",
    "            'text_cleaned': True\n",
    "        }\n",
    "\n",
    "        # Save results to JSON file\n",
    "        import os, json\n",
    "        os.makedirs(\"evaluation/results/POS/BERT\", exist_ok=True)\n",
    "        results_file = f\"evaluation/results/POS/BERT/{model_name}_results.json\"\n",
    "\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(pos_results, f, indent=2, cls=NumpyEncoder)\n",
    "\n",
    "        print(f\"✅ BERT POS results (CLEANED) saved to: {results_file}\")\n",
    "\n",
    "        results = {\n",
    "            'pos_category': pos_category,\n",
    "            'model_name': model_name,\n",
    "            'model_type': 'BERT_CLEANED',\n",
    "            'accuracy': accuracy,\n",
    "            'precision_macro': precision_macro,\n",
    "            'recall_macro': recall_macro,\n",
    "            'f1_macro': f1_macro,\n",
    "            'binary_f1': binary_f1,\n",
    "            'train_stats': train_stats,\n",
    "            'test_stats': test_stats,\n",
    "            'text_cleaned': True,\n",
    "            'prediction_files': {\n",
    "                'json': test_pred_json,\n",
    "                'txt': test_pred_txt\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in BERT POS ablation for {pos_category}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "def run_complete_bert_pos_ablation_study(dataset_df_train, dataset_df_test, dataset_name,\n",
    "                                         binary_label='misogynous', pos_tags=None):\n",
    "    \"\"\"\n",
    "    Run complete BERT POS ablation study for all POS categories.\n",
    "    NOW WITH PROPER TEXT CLEANING!\n",
    "    \"\"\"\n",
    "\n",
    "    if pos_tags is None:\n",
    "        pos_tags = UNIVERSAL_POS_TAGS.copy()\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"BERT POS ABLATION STUDY FOR {dataset_name} (WITH TEXT CLEANING)\")\n",
    "    print(f\"Testing {len(pos_tags)} POS categories: {', '.join(pos_tags)}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    # Get original texts and labels\n",
    "    X_train_raw = dataset_df_train[\"bert representation\"].tolist()\n",
    "    X_test_raw = dataset_df_test[\"bert representation\"].tolist()\n",
    "    y_train = dataset_df_train[binary_label].tolist()\n",
    "    y_test = dataset_df_test[binary_label].tolist()\n",
    "\n",
    "    # 🔥 KEY CHANGE: Clean texts for consistent POS analysis\n",
    "    print(\"🧹 Cleaning texts for POS analysis...\")\n",
    "    X_train = clean_text_sep_token(X_train_raw)\n",
    "    X_test = clean_text_sep_token(X_test_raw)\n",
    "\n",
    "    print(f\"Text cleaning summary:\")\n",
    "    print(f\"  Original sample: '{X_train_raw[0][:60]}...'\")\n",
    "    print(f\"  Cleaned sample:  '{X_train[0][:60]}...'\")\n",
    "\n",
    "    # OPTIMIZATION: Precompute POS tags for all CLEANED texts once\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"PRECOMPUTING POS TAGS FOR CLEANED TEXTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    print(\"Computing POS tags for cleaned training data...\")\n",
    "    train_pos_tags = extract_pos_tags_batch(X_train, batch_size=1000)\n",
    "\n",
    "    print(\"Computing POS tags for cleaned test data...\")\n",
    "    test_pos_tags = extract_pos_tags_batch(X_test, batch_size=1000)\n",
    "\n",
    "    print(\"POS tag computation complete! Now running experiments...\")\n",
    "\n",
    "    # First run baseline (no ablation) with BERT on CLEANED texts\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"BERT BASELINE EXPERIMENT (NO ABLATION, CLEANED TEXTS)\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Initialize BERT components for baseline\n",
    "    from transformers import BertTokenizer, get_linear_schedule_with_warmup\n",
    "    import torch\n",
    "    from torch.optim import AdamW\n",
    "    from torch.utils.data import DataLoader\n",
    "    from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Create baseline BERT datasets with CLEANED texts\n",
    "    baseline_train_dataset = MemeDataset(\n",
    "        texts=X_train,  # Using cleaned texts\n",
    "        labels=y_train,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=128\n",
    "    )\n",
    "\n",
    "    baseline_test_dataset = MemeDataset(\n",
    "        texts=X_test,   # Using cleaned texts\n",
    "        labels=y_test,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=128\n",
    "    )\n",
    "\n",
    "    # Create baseline data loaders\n",
    "    batch_size = 16\n",
    "    baseline_train_loader = DataLoader(\n",
    "        baseline_train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    baseline_test_loader = DataLoader(\n",
    "        baseline_test_dataset,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Train baseline BERT model\n",
    "    baseline_bert_model = BertClassifier(n_classes=2).to(device)\n",
    "    baseline_optimizer = AdamW(baseline_bert_model.parameters(), lr=2e-5)\n",
    "\n",
    "    epochs = 3\n",
    "    total_steps = len(baseline_train_loader) * epochs\n",
    "    baseline_scheduler = get_linear_schedule_with_warmup(\n",
    "        baseline_optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    # Train baseline model\n",
    "    best_baseline_accuracy = 0\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Baseline Epoch {epoch + 1}/{epochs}')\n",
    "\n",
    "        train_acc, train_loss = train_epoch_bin(\n",
    "            baseline_bert_model,\n",
    "            baseline_train_loader,\n",
    "            baseline_optimizer,\n",
    "            baseline_scheduler,\n",
    "            device\n",
    "        )\n",
    "\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "\n",
    "        val_acc, val_loss, _, _ = eval_model_bin(\n",
    "            baseline_bert_model,\n",
    "            baseline_test_loader,\n",
    "            device\n",
    "        )\n",
    "\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "        if val_acc > best_baseline_accuracy:\n",
    "            best_baseline_accuracy = val_acc\n",
    "            torch.save(baseline_bert_model.state_dict(), f'best_baseline_bert_cleaned_{dataset_name.lower()}.pt')\n",
    "\n",
    "    # Load best baseline model and get predictions\n",
    "    baseline_bert_model.load_state_dict(torch.load(f'best_baseline_bert_cleaned_{dataset_name.lower()}.pt'))\n",
    "    _, _, baseline_pred, _ = eval_model_bin(baseline_bert_model, baseline_test_loader, device)\n",
    "    baseline_pred = np.array(baseline_pred)\n",
    "\n",
    "    # Calculate baseline metrics\n",
    "    baseline_accuracy = accuracy_score(y_test, baseline_pred)\n",
    "    baseline_f1 = f1_score(y_test, baseline_pred, average='macro')\n",
    "    baseline_precision = precision_score(y_test, baseline_pred, average='macro')\n",
    "    baseline_recall = recall_score(y_test, baseline_pred, average='macro')\n",
    "\n",
    "    print(f\"BERT Baseline Results (CLEANED TEXTS):\")\n",
    "    print(f\"  - Accuracy: {baseline_accuracy:.3f}\")\n",
    "    print(f\"  - F1-macro: {baseline_f1:.3f}\")\n",
    "    print(f\"  - Precision-macro: {baseline_precision:.3f}\")\n",
    "    print(f\"  - Recall-macro: {baseline_recall:.3f}\")\n",
    "\n",
    "    # Store all results\n",
    "    all_results = {\n",
    "        'model_type': 'BERT_CLEANED',\n",
    "        'dataset': dataset_name,\n",
    "        'text_cleaned': True,\n",
    "        'baseline': {\n",
    "            'accuracy': baseline_accuracy,\n",
    "            'f1_macro': baseline_f1,\n",
    "            'precision_macro': baseline_precision,\n",
    "            'recall_macro': baseline_recall\n",
    "        },\n",
    "        'ablation_results': {}\n",
    "    }\n",
    "\n",
    "    # Run ablation for each POS category using precomputed tags for CLEANED texts\n",
    "    for pos_category in pos_tags:\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(f\"Running BERT ablation for {pos_category} (CLEANED)\")\n",
    "        print(f\"{'='*40}\")\n",
    "\n",
    "        results = run_bert_pos_ablation_experiment(\n",
    "            dataset_df_train, dataset_df_test, pos_category, dataset_name, binary_label,\n",
    "            train_pos_tags, test_pos_tags\n",
    "        )\n",
    "\n",
    "        if results is not None:\n",
    "            all_results['ablation_results'][pos_category] = results\n",
    "\n",
    "            # Print performance drop\n",
    "            accuracy_drop = baseline_accuracy - results['accuracy']\n",
    "            f1_drop = baseline_f1 - results['f1_macro']\n",
    "\n",
    "            print(f\"\\n📊 Performance Impact for {pos_category} (CLEANED):\")\n",
    "            print(f\"   Accuracy drop: {accuracy_drop:.3f} ({accuracy_drop/baseline_accuracy*100:.1f}%)\")\n",
    "            print(f\"   F1-macro drop: {f1_drop:.3f} ({f1_drop/baseline_f1*100:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"❌ Failed to process {pos_category}\")\n",
    "\n",
    "    # Save complete results\n",
    "    import os\n",
    "    import json\n",
    "    os.makedirs(\"evaluation/results/POS/BERT\", exist_ok=True)\n",
    "    summary_file = f\"evaluation/results/POS/BERT/bert_pos_ablation_summary_cleaned_{dataset_name.lower()}.json\"\n",
    "\n",
    "    with open(summary_file, 'w') as f:\n",
    "        json.dump(all_results, f, indent=2, cls=NumpyEncoder)\n",
    "\n",
    "    print(f\"\\n✅ Complete BERT POS ablation results (CLEANED) saved to: {summary_file}\")\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"BERT POS ABLATION SUMMARY (CLEANED TEXTS)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Baseline BERT (CLEANED) - Accuracy: {baseline_accuracy:.3f}, F1: {baseline_f1:.3f}\")\n",
    "    print(\"\\nPOS Category Performance Drops:\")\n",
    "\n",
    "    for pos_category, result in all_results['ablation_results'].items():\n",
    "        if result is not None:\n",
    "            acc_drop = baseline_accuracy - result['accuracy']\n",
    "            f1_drop = baseline_f1 - result['f1_macro']\n",
    "            print(f\"  {pos_category:6s}: Acc drop {acc_drop:.3f} ({acc_drop/baseline_accuracy*100:+5.1f}%), F1 drop {f1_drop:.3f} ({f1_drop/baseline_f1*100:+5.1f}%)\")\n",
    "\n",
    "    return all_results\n",
    "\n",
    "\n",
    "# Universal POS tags definition for reference\n",
    "UNIVERSAL_POS_TAGS = [\n",
    "    'ADJ',      # adjective\n",
    "    'ADV',      # adverb\n",
    "    'INTJ',     # interjection\n",
    "    'NOUN',     # noun\n",
    "    'PROPN',    # proper noun\n",
    "    'VERB'      # verb\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "r5MJq_6M2jIM",
    "outputId": "37835a57-f530-4006-97fc-a19acfb7607a"
   },
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "def run_mami_bert_pos_ablation():\n",
    "    \"\"\"Run BERT POS ablation study on MAMI dataset.\"\"\"\n",
    "    print(\"Starting MAMI BERT POS Ablation Study...\")\n",
    "\n",
    "    results = run_complete_bert_pos_ablation_study(\n",
    "        mami_training_df, mami_test_df, \"MAMI\", binary_label='misogynous'\n",
    "    )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_exist2024_bert_pos_ablation():\n",
    "    \"\"\"Run BERT POS ablation study on EXIST2024 dataset.\"\"\"\n",
    "    print(\"Starting EXIST2024 BERT POS Ablation Study...\")\n",
    "\n",
    "    results = run_complete_bert_pos_ablation_study(\n",
    "        exist_training_df, exist_test_df, \"EXIST2024\", binary_label='sexist'\n",
    "    )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_all_bert_pos_ablation_studies():\n",
    "    \"\"\"\n",
    "    Run BERT POS ablation studies on both MAMI and EXIST2024 datasets.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 100)\n",
    "    print(\"RUNNING ALL BERT POS ABLATION STUDIES\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "    all_results = {}\n",
    "\n",
    "    # Run MAMI BERT POS ablation\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"MAMI DATASET BERT POS ABLATION\")\n",
    "    print(\"=\" * 50)\n",
    "    mami_results = run_mami_bert_pos_ablation()\n",
    "    all_results['MAMI'] = mami_results\n",
    "\n",
    "    # Run EXIST2024 BERT POS ablation\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"EXIST2024 DATASET BERT POS ABLATION\")\n",
    "    print(\"=\" * 50)\n",
    "    exist_results = run_exist2024_bert_pos_ablation()\n",
    "    all_results['EXIST2024'] = exist_results\n",
    "\n",
    "    # Save combined results\n",
    "    import os\n",
    "    import json\n",
    "    os.makedirs(\"evaluation/results/bert_pos\", exist_ok=True)\n",
    "    combined_file = \"evaluation/results/bert_pos/bert_pos_ablation_all_datasets.json\"\n",
    "\n",
    "    with open(combined_file, 'w') as f:\n",
    "        json.dump(all_results, f, indent=2, cls=NumpyEncoder)\n",
    "\n",
    "    print(f\"\\n✅ Combined BERT POS ablation results saved to: {combined_file}\")\n",
    "\n",
    "    # Print comparison summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"CROSS-DATASET BERT POS ABLATION COMPARISON\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    for pos_tag in UNIVERSAL_POS_TAGS:\n",
    "        print(f\"\\n{pos_tag} Impact:\")\n",
    "        for dataset in ['MAMI', 'EXIST2024']:\n",
    "            if pos_tag in all_results[dataset]['ablation_results']:\n",
    "                result = all_results[dataset]['ablation_results'][pos_tag]\n",
    "                baseline = all_results[dataset]['baseline']\n",
    "                acc_drop = baseline['accuracy'] - result['accuracy']\n",
    "                f1_drop = baseline['f1_macro'] - result['f1_macro']\n",
    "                print(f\"  {dataset:8s}: Acc drop {acc_drop:.3f} ({acc_drop/baseline['accuracy']*100:+5.1f}%), F1 drop {f1_drop:.3f} ({f1_drop/baseline['f1_macro']*100:+5.1f}%)\")\n",
    "\n",
    "    return all_results\n",
    "\n",
    "\n",
    "# Run all BERT POS ablation experiments\n",
    "all_results = run_all_bert_pos_ablation_studies()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
